{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BdQ-7EbqC1hR"
   },
   "source": [
    "# Run Notes\n",
    "* carb_ml_overview_xx.ipynb\n",
    "  - This file which can be run to see TF examples\n",
    "* tf_util.py\n",
    "  - utility functions/classes that can be imported for TF analysis.\n",
    "* When using TF 2.13 and above, model names can't end with .keras extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyFquAWfu52g"
   },
   "source": [
    "## Select maximum epochs and which sections of this notebook to run\n",
    " \n",
    "\n",
    "Flag time-consuming sections as skip-able"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGmLv5wSvLTj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276579942,
     "user_tz": 420,
     "elapsed": 276,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# MAX_EPOCHS is user variable to specify the maximum amount of training simulations\n",
    "# for each model training.  Set to a very low number (e.g. 3) for very short training just\n",
    "# to see if the models are set up correctly.  set to 15 to 25 for reasonably long training and 100 for extra long training (may take too long at this number of epochs for slow machines)\n",
    "MAX_EPOCHS = 3 # 100\n",
    "\n",
    "# Set flag values to 'True' for each section of this notebook you wish to evaluate\n",
    "flags = {}\n",
    "flags['download_images'] = True\n",
    "flags['pandas'] = True\n",
    "flags['pipelines'] = True\n",
    "flags['pre_processing'] = True\n",
    "flags['plot_example_images'] = True\n",
    "flags['functional_api_example'] = True\n",
    "flags['regression_example'] = True\n",
    "flags['plot_regression_example'] = True\n",
    "flags['datasets'] = False # needs to be updated for new file structure\n",
    "flags['augmentation'] = True\n",
    "flags['binary_example'] = True\n",
    "flags['multiclass_example'] = True\n",
    "flags['tiny_vgg'] = True\n",
    "flags['transfer'] = True\n",
    "# tensorflow_hub requires user interaction at prmpt, set to False unless you want to monitor\n",
    "flags['tensorflow_hub'] = False  # requires flags['transfer'] \n",
    "flags['feature_extraction'] = False # requires flags['transfer']\n",
    "flags['fine_tuned'] = True     # requires flags['transfer']\n",
    "flags['efficient_net'] = True\n",
    "flags['retrain'] = True\n",
    "flags['augmentation_visualization'] = True\n",
    "flags['time_series'] = True\n",
    "flags['nlp'] = True\n",
    "flags['nlp_sequence'] = True  # These can be slow, so they are treated separately ...\n",
    "\n",
    "# Tensorboard and hub can't be run unattended, so safest to have them be False\n",
    "flags['tensorboard'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NmHmlu3IV9ft"
   },
   "source": [
    "# ML Related Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx_r8F2tV9ft",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276581052,
     "user_tz": 420,
     "elapsed": 2,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import IPython.display\n",
    "import collections\n",
    "import copy\n",
    "# import datetime\n",
    "import importlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "from packaging import version\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import Sequential, layers, models, regularizers\n",
    "from keras.layers import Activation, Conv2D, Dense, Flatten, MaxPool2D,  TextVectorization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import image_dataset_from_directory, text_dataset_from_directory, img_to_array, load_img, plot_model\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from tensorflow import keras  # note - maybe this should be imported differently\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# If you can import drive, you are on colab\n",
    "try:\n",
    "  # noinspection PyUnresolvedReferences\n",
    "  from google.colab import drive\n",
    "  run_mode = 'colab'\n",
    "except ModuleNotFoundError:\n",
    "  run_mode = 'pycharm'\n",
    "\n",
    "# tensorflow_hub is not a required package on the Google cert exam interpreter\n",
    "# it may not be load if strict compliance with minimal exam requirements are used\n",
    "try:\n",
    "  import tensorflow_hub as hub\n",
    "except ModuleNotFoundError:\n",
    "  print(f\"tensorflow_hub package not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optional, set system-wide matplotlib options\n",
    "# mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "# mpl.rcParams['axes.grid'] = False"
   ],
   "metadata": {
    "id": "9ZGREki9CT6M",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276581052,
     "user_tz": 420,
     "elapsed": 2,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "x8Xe6DiVdUPR"
   },
   "source": [
    "# Import TensorFlow Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyGgqmVndUPR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276582460,
     "user_tz": 420,
     "elapsed": 1410,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "9dd8f67c-7150-4cef-cb4e-36bd5ed5c7d8"
   },
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# colab uses unix style file system, windows requires a drive letter\n",
    "if run_mode == \"colab\":\n",
    "  prefix = \"\"\n",
    "else:\n",
    "  prefix = \"c:\"\n",
    "  \n",
    "dir_ml = prefix + \"/content/ml_rsdas/\"\n",
    "dir_ml_util = prefix + \"/content/ml_rsdas/ml_util/\"\n",
    "dir_working = prefix + \"/content/working/\"\n",
    "\n",
    "# Change working directory to location that is not backed-up to avoid gdrive lag\n",
    "Path(dir_ml).mkdir(parents=True, exist_ok=True)\n",
    "Path(dir_ml_util).mkdir(parents=True, exist_ok=True)\n",
    "Path(dir_working).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy tensorflow utilities file from tony's archive\n",
    "\n",
    "if Path(dir_ml_util + '/tf_util.py').exists():\n",
    "  print('Tensor Flow Utility File already exists, delete if you wish to download it again')\n",
    "else:\n",
    "  print('Downloading tf_util.py')\n",
    "  url = \"https://raw.githubusercontent.com/tony-held-carb/ml_rsdas/main/tf_util.py\"\n",
    "  os.chdir(dir_ml_util)\n",
    "  if run_mode == 'colab':\n",
    "    !wget \"https://raw.githubusercontent.com/tony-held-carb/ml_rsdas/main/tf_util.py\"\n",
    "  else:\n",
    "    import wget\n",
    "    wget.download(url)\n",
    "\n",
    "os.chdir(dir_working)\n",
    "print(f'Running notebook on platform: {run_mode}')\n",
    "print(f'Present Working Directory is: {dir_working}')\n",
    "print(f'Detected physical devices: {dir_working}')\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "# Next line will provide GPU diagnostics if your card is nvidia\n",
    "!nvidia-smi\n",
    "\n",
    "# update python import path to allow for utility function import\n",
    "sys.path.insert(0, dir_ml)\n",
    "\n",
    "# Load utility functions, directory structure, and standard ml imports\n",
    "import ml_util.tf_util as tfu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bdonDqfKdUPS"
   },
   "source": [
    "# Reload TFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqtjboCgdUPS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584001,
     "user_tz": 420,
     "elapsed": 1543,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "bc53b67a-1cf3-4828-d925-86e3b80e7216"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if True:\n",
    "  importlib.reload(tfu)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensure Run Flag Consistency\n",
    "Ensure that flags that depend on other flags are consistent"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zUbNrFwA3zLN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfu.ensure_flag(flags, 'transfer', 'tensorflow_hub')\n",
    "tfu.ensure_flag(flags, 'transfer', 'feature_extraction')\n",
    "tfu.ensure_flag(flags, 'transfer', 'fine_tuned')\n",
    "print(f\"Runtime flag settings\\n {flags}\")"
   ],
   "metadata": {
    "id": "UyP6V6mZ3zLN",
    "outputId": "acd35dc9-7bd5-41fd-9eb4-13ae030cfba3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584002,
     "user_tz": 420,
     "elapsed": 10,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "e6xMB5NJJRJR"
   },
   "source": [
    "# Image Datasets\n",
    "\n",
    "Additional Information on how Udemy Datasets were created:\n",
    "https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Por5AtYoJRJR"
   },
   "source": [
    "## Image File Structure\n",
    "\n",
    "When using CNN, you usually don't use separate x_train and y_train tensors\n",
    "that you create from scratch.  Instead, you use a dataset that is\n",
    "created from a directory of a certain structure.\n",
    "\n",
    "The standard structure is that each dataset directory has the same subdirectories and that the subdirectories are effectively the y_label\n",
    "\n",
    "An example train, test, val structure could be:\n",
    "```\n",
    "base_dir/train\n",
    "  /cats\n",
    "  /dogs\n",
    "  /fish\n",
    "base_dir/test\n",
    "  /cats\n",
    "  /dogs\n",
    "  /fish\n",
    "base_dir/val\n",
    "  /cats\n",
    "  /dogs\n",
    "  /fish\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoD0rXb-JRJR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584002,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# local drive location for image downloads\n",
    "dir_images_local = tfu.paths['dir_images_local']\n",
    "\n",
    "# 2 class dataset with only pizza and steak used for binary training\n",
    "url_pizza_steak = \"https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\"\n",
    "dir_pizza_steak = dir_images_local + 'pizza_steak'\n",
    "dir_pizza_steak_train = pathlib.Path(dir_pizza_steak).joinpath('train')\n",
    "dir_pizza_steak_test = pathlib.Path(dir_pizza_steak).joinpath('test')\n",
    "dir_pizza_steak_steak_only = dir_pizza_steak_test.joinpath('steak')\n",
    "dir_pizza_steak_pizza_only = dir_pizza_steak_test.joinpath('pizza')\n",
    "\n",
    "# 10 class dataset for Multiclass modeling\n",
    "url_food_10_class_all = \"https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\"\n",
    "dir_food_10_class_all = dir_images_local + '10_food_classes_all_data'\n",
    "dir_food_10_class_all_train = pathlib.Path(dir_food_10_class_all).joinpath('train')\n",
    "dir_food_10_class_all_test = pathlib.Path(dir_food_10_class_all).joinpath('test')\n",
    "\n",
    "# 10 food classes with only 10% of the training data\n",
    "url_food_10_class_10 = \"https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\"\n",
    "dir_food_10_class_10 = dir_images_local + '10_food_classes_10_percent/'\n",
    "dir_food_10_class_10_train = dir_food_10_class_10 + 'train/'\n",
    "dir_food_10_class_10_test = dir_food_10_class_10 + 'test/'\n",
    "\n",
    "# 10 food classes with only 1% of the training data\n",
    "url_food_10_class_1 = \"https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\"\n",
    "dir_food_10_class_1 = dir_images_local + '10_food_classes_1_percent/'\n",
    "dir_food_10_class_1_train = dir_food_10_class_1 + 'train/'\n",
    "dir_food_10_class_1_test = dir_food_10_class_1 + 'test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzZXtHzK5yvi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584002,
     "user_tz": 420,
     "elapsed": 5,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Individual image files for diagnostics/testing\n",
    "url_pizza_dad = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg\"\n",
    "url_steak_03 = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\"\n",
    "url_hamburger_03 = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg\"\n",
    "url_sushi_03 = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NcBRlo2A5yvi"
   },
   "source": [
    "## Download image databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG4u37PR5yvi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584002,
     "user_tz": 420,
     "elapsed": 5,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "1d705cbf-6f09-43f4-8f06-53d2bd134a57",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# These are fast enough to always be loaded\n",
    "pizza_dad_03 = tfu.url_to_local_dir(url_pizza_dad, dir_images_local, decompress=False)\n",
    "steak_03 = tfu.url_to_local_dir(url_steak_03, dir_images_local, decompress=False)\n",
    "hamburger_03 = tfu.url_to_local_dir(url_hamburger_03, dir_images_local, decompress=False)\n",
    "sushi_03 = tfu.url_to_local_dir(url_sushi_03, dir_images_local, decompress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['download_images']:\n",
    "  tfu.url_to_local_dir(url_pizza_steak, dir_images_local)\n",
    "  tfu.url_to_local_dir(url_food_10_class_all, dir_images_local)\n",
    "  tfu.url_to_local_dir(url_food_10_class_10, dir_images_local)\n",
    "  tfu.url_to_local_dir(url_food_10_class_1, dir_images_local)"
   ],
   "metadata": {
    "id": "t0SPzwxr3zLQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584002,
     "user_tz": 420,
     "elapsed": 3,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Additional images for letter manipulation and augmentation examples\n",
    "url_example_images = \"https://tonyserver.github.io/images/ml_images.zip\"\n",
    "tfu.url_to_local_dir(url_example_images, tfu.paths['dir_local'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pandas Fundamentals"
   ],
   "metadata": {
    "collapsed": false,
    "id": "sCYTY00_t8p5"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "p_0saHJRt8p5"
   },
   "source": [
    "## Read in CSV into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoYHw26ot8p5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584395,
     "user_tz": 420,
     "elapsed": 395,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Read in the insurance dataset\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wiInLjL1t8p5"
   },
   "source": [
    "## DataFrame Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIiSNIoet8p5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584395,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  display(insurance)\n",
    "  print(insurance.head())\n",
    "  print(insurance.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Td_Cuiyrt8p5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584396,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  insurance.info()\n",
    "  print(f\"\\ncolumns: {insurance.columns}\")\n",
    "  print(f\"\\nshape: {insurance.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "i4RR3Rllt8p5"
   },
   "source": [
    "## get_dummies\n",
    "get_dummy's is a fast and easy way to work with a single dataframe,\n",
    "but if you are going to working with different dataframes for training/learning\n",
    "you should use sklearn's OneHot to avoid errors associated with missing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6icpzmeIt8p6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584396,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  insurance_dummy = pd.get_dummies(insurance)\n",
    "  display(insurance_dummy)\n",
    "  print(f\"\\ncolumns: {insurance_dummy.columns}\")\n",
    "  print(f\"\\nshape: {insurance_dummy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "iWji73XTt8p6"
   },
   "source": [
    "## Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wh-ohyJt8p6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584396,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  value_counts_smoker = insurance.value_counts('smoker')\n",
    "  value_counts_smoker_yes = insurance_dummy.value_counts('smoker_yes')\n",
    "\n",
    "  print(f\"{value_counts_smoker}\")\n",
    "  print(f\"\\n{value_counts_smoker_yes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yrVYXgG4t8p6"
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_mUI835jt8p6"
   },
   "source": [
    "### Default DataFrame Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWbdQ7AUt8p6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584396,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  # Plot multiple columns\n",
    "  insurance.plot(y=['age', 'bmi'])\n",
    "  # Plot x, y scatter plots\n",
    "  insurance.plot(x='age', y='bmi', marker='.', linestyle='None')\n",
    "  # Plot all columns of dataframe (can be slow, uncomment to see)\n",
    "  # insurance.plot(subplots=True, figsize=(12, 20), marker='.', linestyle='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "alDxRnDkt8p6"
   },
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBdlCinut8p6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584396,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  insurance.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHDOcyEKt8p6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584396,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  insurance_dummy.hist(figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QYfFZQH55yvj"
   },
   "source": [
    "## Timeseries and Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsay069Y5yvk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584397,
     "user_tz": 420,
     "elapsed": 26,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  # Parse dates and set date column to index\n",
    "  # parse the date column (tell pandas column 1 is a datetime)\n",
    "  df = pd.read_csv(data_file_name,\n",
    "                   parse_dates=[\"Date\"],\n",
    "                   index_col=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wV6-1UdD5yvk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584397,
     "user_tz": 420,
     "elapsed": 26,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  fn_climate_data = \"jena_climate_2009_2016.csv\"\n",
    "  url_climate_data = \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "  tfu.url_to_local_dir(url_climate_data, decompress=True)\n",
    "  climate_df = pd.read_csv(fn_climate_data)\n",
    "\n",
    "  val = pd.to_datetime(climate_df['Date Time'], format=\"%d.%m.%Y %H:%M:%S\")\n",
    "\n",
    "  climate_df.insert(loc=1, column='timestamp', value=val)\n",
    "  climate_df.insert(loc=2, column='Year', value=climate_df[\"timestamp\"].dt.year)\n",
    "  climate_df.insert(loc=3, column='Month', value=climate_df[\"timestamp\"].dt.month)\n",
    "  climate_df.insert(loc=4, column='Day', value=climate_df[\"timestamp\"].dt.day)\n",
    "  climate_df.insert(loc=5, column='Hour', value=climate_df[\"timestamp\"].dt.hour)\n",
    "  climate_df.insert(loc=6, column='Minute', value=climate_df[\"timestamp\"].dt.minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OovX2Bfn5yvk"
   },
   "source": [
    "## Create features and target dataframes and arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhJeCMRT5yvk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584397,
     "user_tz": 420,
     "elapsed": 26,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pandas']:\n",
    "  temperature_df = climate_df[['timestamp', 'T (degC)']]\n",
    "  features_df = climate_df.drop(['Date Time', 'Year', 'Month', 'Day', 'Hour', 'Minute'], axis=1)\n",
    "  # squeeze required to get rid of singleton dimension\n",
    "  temperature = np.squeeze(temperature_df.drop(['timestamp'], axis=1).to_numpy())\n",
    "  features = features_df.drop(['timestamp'], axis=1).to_numpy()\n",
    "  print(temperature.shape, features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ITbA3cElt8p6"
   },
   "source": [
    "# Column Transformers & Pipelines\n",
    "Overall Pipeline Strategy:\n",
    "* Use Pipeline to create a sequence of transformations\n",
    "  * likely you will want different transformations for numeric/categorical\n",
    "* Select columns for each transformation pipeline\n",
    "  * You can explicitly name columns, or\n",
    "  * make_column_selector to select numeric vs. categorical data\n",
    "* Use ColumnTransformer to fit_transform the data\n",
    "Notes:\n",
    "  * There are anonymous options for the pipelines that don't require naming:\n",
    "    * Pipeline - you name each transformation\n",
    "    * make_pipeline - no naming required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tfN8q2JBt8p7"
   },
   "source": [
    "## OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeYve9Xmt8p7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584398,
     "user_tz": 420,
     "elapsed": 27,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Display pipelines interactively\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBCoOFqEt8p7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584398,
     "user_tz": 420,
     "elapsed": 27,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pipelines']:\n",
    "  encoder = OneHotEncoder()\n",
    "  insurance_categories = insurance[['region', 'sex']]\n",
    "  insurance_one_hot = encoder.fit_transform(insurance_categories).todense()\n",
    "  display(insurance_one_hot)\n",
    "  print(f\"\\nshape: {insurance_one_hot.shape}\")\n",
    "  print(f\"\\nfeature_names_in_: {encoder.feature_names_in_}\")\n",
    "  print(f\"\\nget_feature_names_out(): {encoder.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Fcqslgrmt8p7"
   },
   "source": [
    "## Pipelines for Numeric and Categorical Type\n",
    "Create separate pipelines for numerical and categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "204TBxkYt8p7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584398,
     "user_tz": 420,
     "elapsed": 27,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "  (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "  (\"standardize\", StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "  (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "  (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2no_UoHlt8p7"
   },
   "source": [
    "## Find numeric/categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHCZsXhjt8p7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584398,
     "user_tz": 420,
     "elapsed": 27,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "871cff95-5118-4a47-9cba-10cb8edd0da7"
   },
   "outputs": [],
   "source": [
    "num_cols = make_column_selector(dtype_include=np.number)\n",
    "cat_cols = make_column_selector(dtype_include=object)\n",
    "print(f\"numeric data types: {num_cols(insurance)}\")\n",
    "print(f\"categorical data types: {cat_cols(insurance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "HToV77TJt8p7"
   },
   "source": [
    "## Create column transformer based on datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNkiPBg4t8p7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584399,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer(\n",
    "  (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "  (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tm5VUWSt8p7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584399,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "725971f7-32f7-4993-a98d-242ad998e641"
   },
   "outputs": [],
   "source": [
    "input_1 = preprocessing.fit_transform(insurance)\n",
    "input_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fxDuKkpvt8p7"
   },
   "source": [
    "## Explicit selection of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSgSCUIst8p7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584399,
     "user_tz": 420,
     "elapsed": 21,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "num_cols_2 = ['age', 'bmi']\n",
    "cat_cols_2 = ['sex', 'smoker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "AFcoFMxwt8p8"
   },
   "source": [
    "### Create column transformer with explicitly named columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EXBB_Kbt8p8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584399,
     "user_tz": 420,
     "elapsed": 21,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_2 = ColumnTransformer([\n",
    "  (\"num\", num_pipeline, num_cols_2),\n",
    "  (\"cat\", cat_pipeline, cat_cols_2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mP-cFl0Gt8p8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584400,
     "user_tz": 420,
     "elapsed": 22,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "535e1e79-4a5d-4159-a508-89228a14e214"
   },
   "outputs": [],
   "source": [
    "input_2 = preprocessing_2.fit_transform(insurance)\n",
    "input_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "VvU0bTKRt8p8"
   },
   "source": [
    "## Column Transformers\n",
    "Allows for a simple one-step column transformation (not a pipeline) for each datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7a-Q7zg0t8p8"
   },
   "source": [
    "### Using Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "io6H5dDot8p8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584400,
     "user_tz": 420,
     "elapsed": 20,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "8b4e3afe-a5af-4da7-ddb2-9ecc56343b5e"
   },
   "outputs": [],
   "source": [
    "ct1 = make_column_transformer(\n",
    "  (StandardScaler(), [\"age\", \"bmi\", \"children\"]),\n",
    "  (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"]))\n",
    "\n",
    "ct1_output = ct1.fit_transform(insurance)\n",
    "ct1_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Onj1aBgRt8p8"
   },
   "source": [
    "### Using Column Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXAwAF0Nt8p8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584400,
     "user_tz": 420,
     "elapsed": 19,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "4cc6add8-c3c9-463f-e5d4-4de51d496a2f"
   },
   "outputs": [],
   "source": [
    "ct2 = make_column_transformer(\n",
    "  (StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "  (OneHotEncoder(handle_unknown=\"ignore\"), make_column_selector(dtype_include=object)))\n",
    "\n",
    "ct2_output = ct2.fit_transform(insurance)\n",
    "ct2_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yndLDMC6t8p8"
   },
   "source": [
    "## Interactive Transformer Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2foAz5Tt8p8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584400,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "ed88c4d1-268e-4d36-8d0e-b1476f847f9c"
   },
   "outputs": [],
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wdYHdxHt8p8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584400,
     "user_tz": 420,
     "elapsed": 16,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "871bd13f-410f-49a1-9811-dbeee514c44d"
   },
   "outputs": [],
   "source": [
    "ct1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ibETu_MKdUPT"
   },
   "source": [
    "# Splitting Data into Training, CV, and Testing\n",
    "\n",
    "Typical variable names are: X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "Typical splitting routines:\n",
    "* sklearn's train_test_split\n",
    "* tfu.split_data\n",
    "* column transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tvwjb0rwdUPT"
   },
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zh64uHgdUPT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584401,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pre_processing']:\n",
    "  # Sample data based on y = x + 17\n",
    "  num_samples = 25\n",
    "  X = np.linspace(-5, 5, num_samples)\n",
    "  X = np.expand_dims(X, axis=-1)\n",
    "  y = X + 17\n",
    "  print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "b0ln3RmwdUPT"
   },
   "source": [
    "## Splitting using sklearn's train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooMk0DcDdUPT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584401,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pre_processing']:\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                      y,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=42)\n",
    "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzldN-Uch7Jd"
   },
   "source": [
    "## Splitting using tfu.split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v49Mwddib9FH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584401,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pre_processing']:\n",
    "  splits = tfu.split_data(X, y, test_size=0.20, val_size=0.10)\n",
    "  X_train, X_test, X_val, y_train, y_test, y_val = splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VPlM2aUGDGn"
   },
   "source": [
    "## Splitting with column transformers\n",
    "1. Transformer is fit off training data only (no test data)\n",
    "1. Transformation applied to all input (X_train, X_val, X_test)\n",
    "1. Target/Labels are not transformed\n",
    "1. typical transformations\n",
    "  - StandardScaler()\n",
    "  - MinMaxScaler()\n",
    "  - OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OawdFqKxcCFK"
   },
   "source": [
    "#### Column Transform Pandas\n",
    "\n",
    "Note: if you perform a column transformation of Pandas df, the output will be numpy (suitable for tf fitting), rather than a pandas datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGnhJR1IIj2b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584401,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pre_processing']:\n",
    "  # Sample insurance dataset 1338 entries and format of\n",
    "  #     Column    Dtype\n",
    "  #  1\tsex       object\n",
    "  #  2\tbmi       float64\n",
    "  #  3\tchildren  int64\n",
    "  #  4\tsmoker    object\n",
    "  #  5\tregion    object\n",
    "  #  6\tcharges   float64\n",
    "  csv_file = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "  insurance = pd.read_csv(csv_file)\n",
    "\n",
    "  # Create column transformer (this will help us normalize/preprocess our data)\n",
    "  ct = make_column_transformer(\n",
    "    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]),  # get all values between 0 and 1\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n",
    "  )\n",
    "\n",
    "  # Create X & y values\n",
    "  X = insurance.drop(\"charges\", axis=1)\n",
    "  y = insurance[\"charges\"]\n",
    "\n",
    "  # Build our train and test sets (use random state to ensure same split as before)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Fit column transformer on the training data only (doing so on test data would result in data leakage)\n",
    "  ct.fit(X_train)\n",
    "\n",
    "  # Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\n",
    "  # These normalized data can be used as kera input\n",
    "  X_train_normal = ct.transform(X_train)\n",
    "  X_test_normal = ct.transform(X_test)\n",
    "\n",
    "  print(type(X_train), type(X_train_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nx7qqq5WdUPU"
   },
   "source": [
    "#### Column Transformation of Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSoRk_qoO6mG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584402,
     "user_tz": 420,
     "elapsed": 18,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['pre_processing']:\n",
    "  # Create input matrix with 3 features\n",
    "  num_samples = 101\n",
    "  x1 = np.arange(num_samples)  # vector with average of 50\n",
    "  x2 = np.arange(num_samples) + 100  # vector with average of 150\n",
    "  x3 = np.random.randint(0, 10, size=num_samples)  # random value (0-9)\n",
    "  y = np.random.randint(0, 2, size=num_samples)  # random value (0 or 1)\n",
    "  X = np.column_stack((x1, x2, x3))\n",
    "  print(x1.shape, x2.shape, x3.shape, X.shape)\n",
    "\n",
    "  # Split data into\n",
    "  splits = tfu.split_data(X, y, test_size=0.15, val_size=0.15)\n",
    "  X_train, X_test, _X_val, y_train, y_test, _y_val = splits\n",
    "\n",
    "  # Column transformer is a list of tuples\n",
    "  # Each one specifies the type of transformation (MinMaxScaler, OneHotEncoder, ect)\n",
    "  # Followed by a list indicating which columns should be transformed\n",
    "  ct = make_column_transformer((MinMaxScaler(), [0, 1]),\n",
    "                               (OneHotEncoder(handle_unknown=\"ignore\"), [2])\n",
    "                               )\n",
    "\n",
    "  # Fit column transformer on the training data only\n",
    "  # Can't use test or val data to prevent data leakage\n",
    "  ct.fit(X_train)\n",
    "\n",
    "  # Transform training and test data with normalization\n",
    "  # These normalized data can be used as kera input\n",
    "  X_train_normal = ct.transform(X_train)\n",
    "  X_test_normal = ct.transform(X_test)\n",
    "\n",
    "  # These transformations may result in sparse matrices\n",
    "  # For the following diagnostics to work as expected, convert them to dense matrix\n",
    "  # You don't have to convert to dense when using keras, just for the output below\n",
    "  X_train_normal = X_train_normal.todense()\n",
    "  X_test_normal = X_test_normal.todense()\n",
    "\n",
    "  if True:\n",
    "    tfu.min_max_mean(X, 'x original')\n",
    "    tfu.min_max_mean(X_train, 'x train')\n",
    "    tfu.min_max_mean(X_train_normal, 'x train norm')\n",
    "    tfu.min_max_mean(X_test, 'x test')\n",
    "    tfu.min_max_mean(X_test_normal, 'x test norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Y62l53B5dUPU"
   },
   "source": [
    "# Image Downloading & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "c5TG7BsydUPV"
   },
   "source": [
    "## Random sampling of Image directories\n",
    "\n",
    "Routines to randomly sample image tree structures to get a feel for the datasets\n",
    "* tfu.random_sample_directory(directory, number_files)\n",
    "* tfu.random_sample_single_class(directory, class_name, number_files)\n",
    "* tfu. random_sample_all_classes(directory, number_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6SdQmtDrdUPV"
   },
   "source": [
    "## Image Visualization Routines\n",
    "\n",
    "* plot_tensor(x, title=None, include_shape=True, figsize=None)\n",
    "  - Plot image file with title and optional image shape\n",
    "* tfu.plot_image(file_name, title=None, include_shape=True)\n",
    "  - plot image files (such as xxx.jpeg)\n",
    "* tfu.plot_single_class(directory, class_name, number_files)\n",
    "  - Plot random images for a single class.\n",
    "* tfu.plot_all_classes(directory, number_files)\n",
    "  - Plot sampled files from a directory for each image class type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "m2UAvqo8dUPV"
   },
   "source": [
    "## Download URL & Visualize Single Image File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLekTSomdUPV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584402,
     "user_tz": 420,
     "elapsed": 18,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Download and view an image from url\n",
    "if flags['plot_example_images']:\n",
    "  steak_url = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\"\n",
    "  target_dir = tfu.paths['dir_images_local']\n",
    "  steak_full_path = tfu.url_to_local_dir(steak_url, target_dir, decompress=False)\n",
    "  tfu.plot_image(steak_full_path,\n",
    "                 title=steak_full_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ndhbwJOSdUPV"
   },
   "source": [
    "## Convert local files into a tensor\n",
    "tfu.image_files_to_tensor(filenames, img_shape, channels=3, rescale=255.)\n",
    "  - Read images from filenames, turns them into a tensor\n",
    "    and reshapes it to (samples, img_shape, img_shape, colour_channel).\n",
    "  - Use a list for filenames even if there is only one file considered\n",
    "  - Rescaling an image so that pixel values are between 0 and 1 will allow for\n",
    "    better TF modeling, but will make plotting the tensor impractical\n",
    "    since plotting routines expect pixel values between 0 and 255\n",
    "  - Set rescale to None to allow for plotting\n",
    "\n",
    "For the following examples the same steak image is loaded multiple times\n",
    "to simulate multiple different files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJBY9SvYdUPV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584402,
     "user_tz": 420,
     "elapsed": 18,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['plot_example_images']:\n",
    "  # Single image with rescale (pixels will be between 0 and 1)\n",
    "  tf_steak_with_rescale = tfu.image_files_to_tensor([steak_full_path],\n",
    "                                                    img_shape=224,\n",
    "                                                    channels=3,\n",
    "                                                    rescale=255.,\n",
    "                                                    )\n",
    "  print(f'resized & rescaled tensor: {tf_steak_with_rescale.shape}')\n",
    "  print(f'{tf.reduce_min(tf_steak_with_rescale) =}')\n",
    "  print(f'{tf.reduce_max(tf_steak_with_rescale) =}')\n",
    "\n",
    "  # Multiple images with no rescale (pixels will be between 0 and 255)\n",
    "  tf_steak_no_rescale = tfu.image_files_to_tensor([steak_full_path, steak_full_path],\n",
    "                                                  img_shape=224,\n",
    "                                                  channels=3,\n",
    "                                                  rescale=1.0)\n",
    "  print(f'resized (not rescaled) tensor: {tf_steak_no_rescale.shape}')\n",
    "  print(f'{tf.reduce_min(tf_steak_no_rescale) =}')\n",
    "  print(f'{tf.reduce_max(tf_steak_no_rescale) =}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4br-h0eRdUPV"
   },
   "source": [
    "### Visualize tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFkYdgeLdUPV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584402,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['plot_example_images']:\n",
    "  image_number = 0  # index of image in tensor to plot\n",
    "  # Use squeeze in-case the image was grayscale (1 channel)\n",
    "  tfu.plot_tensor(tf_steak_no_rescale[image_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CBtPWSKddUPW"
   },
   "source": [
    "## Inspect Image File Structure & Find Classifier Names\n",
    "This allows for the determination of training/testing file counts for each class\n",
    "* tfu.dir_class_names(directory)\n",
    "  * Can be used on any file standard structure with image data\n",
    "* tfu.generator_labels(directory, class_mode='binary')\n",
    "* dataset.class_names\n",
    "  * If you are using a dataset for tf model fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCJFazbvJRJV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584402,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  # Load pizza steak images if necessary\n",
    "  tfu.url_to_local_dir(url_pizza_steak, dir_images_local)\n",
    "\n",
    "  # Find the labels and class counts for the test data\n",
    "  y_labels, counts = tfu.generator_labels(dir_pizza_steak_test, class_mode='binary')\n",
    "\n",
    "  # Find baseline accuracy using test labels (faster if you already have y_labels)\n",
    "  tfu.random_guess_accuracy(y_labels)\n",
    "\n",
    "  # Find class names from file structure\n",
    "  class_names_pizza_steak = tfu.dir_class_names(dir_pizza_steak_test)\n",
    "  print(f'Binary class names: {class_names_pizza_steak}')\n",
    "\n",
    "  # List file counts in each directory\n",
    "  tfu.walk_directory(dir_pizza_steak_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vsEQipERJRJV"
   },
   "source": [
    "Save classnames to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf146KVsJRJV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584402,
     "user_tz": 420,
     "elapsed": 17,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  class_names_pizza_steak = tfu.dir_class_names(dir_pizza_steak_train)\n",
    "  print(f'The pizza/steak class names are: \\n\\t{class_names_pizza_steak}')\n",
    "\n",
    "  # Save class names to json files\n",
    "  with open(tfu.paths['dir_model_runs'] + \"class_names_2.json\", \"w\") as write_file:\n",
    "    json.dump(class_names_pizza_steak.tolist(), write_file)\n",
    "\n",
    "  class_names_food_10 = tfu.dir_class_names(dir_food_10_class_all_train)\n",
    "  print(f'The 10 category class names are: \\n\\t{class_names_food_10}')\n",
    "\n",
    "  with open(tfu.paths['dir_model_runs'] + \"class_names_10.json\", \"w\") as write_file:\n",
    "    json.dump(class_names_food_10.tolist(), write_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "HSccvqAgdUPX"
   },
   "source": [
    "# Creating TF input from directory structure\n",
    "\n",
    "* ConvNets will likely use Datasets or ImageDataGenerators for fitting rather than using\n",
    "  tensors or numpy arrays\n",
    "* Datasets may be more efficient than ImageDataGenerators since they may have better\n",
    "  CPU/GPU parallelization\n",
    "\n",
    "\n",
    "Two primary routines for image dataset tf input specification:\n",
    "  * ImageDataGenerator\n",
    "    - Used by Udemy\n",
    "    - Can directly specify image augmentation\n",
    "  * image_dataset_from_directory\n",
    "    - Used in the Deep Learning Book\n",
    "    - Likely better performance\n",
    "    - Does not perform augmentation.\n",
    "      Easy work around is to use an augmentation model layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Fk63QbMyJRJV"
   },
   "source": [
    "### ImageDataGenerator\n",
    "\n",
    "Usage Steps:\n",
    "1. Use ImageDataGenerator to create datagen with desired scaling and augmentation characteristics\n",
    "2. datagen.flow_from_directory to create dataset that can be used with model.fit()\n",
    "3. len(dataset) is the approximate number of batches for a single epoch\n",
    "\n",
    "Notes:\n",
    "1.  This was the approach used in the Udemy online class\n",
    "1.  See Binary and Multiclass Classification for additional examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYc_tOcGJRJV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584403,
     "user_tz": 420,
     "elapsed": 18,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  # Set the seed\n",
    "  tf.random.set_seed(42)\n",
    "\n",
    "  # Create ImageDataGenerator training instance without data augmentation\n",
    "  # Since we know the data are between 0 and 255, this will rescale to 0->1\n",
    "  train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "  valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "  # Create ImageDataGenerator training instance with data augmentation\n",
    "  train_datagen_augmented = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                               rotation_range=20,\n",
    "                                               # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
    "                                               shear_range=0.2,  # shear the image\n",
    "                                               zoom_range=0.2,  # zoom into the image\n",
    "                                               width_shift_range=0.2,  # shift the image width ways\n",
    "                                               height_shift_range=0.2,  # shift the image height ways\n",
    "                                               horizontal_flip=True)  # flip the image on the horizontal axis\n",
    "\n",
    "  batch_size = 32\n",
    "  # Import data from directories and turn it into batches\n",
    "  train_data = train_datagen.flow_from_directory(\n",
    "    dir_pizza_steak_train,\n",
    "    batch_size=batch_size,  # number of images to process at a time\n",
    "    target_size=(224, 224),  # convert all images to be 224 x 224\n",
    "    class_mode=\"binary\",  # type of problem we're working on\n",
    "    # If we had more than two classes, we would use 'categorical'.\n",
    "    seed=42)\n",
    "\n",
    "  valid_data = valid_datagen.flow_from_directory(\n",
    "    dir_pizza_steak_test,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(224, 224),\n",
    "    class_mode=\"binary\",\n",
    "    seed=42)\n",
    "  print(\n",
    "    f'training data will take about {len(train_data)} steps (batches) per epoch for about {len(train_data) * batch_size} files')\n",
    "  print(\n",
    "    f'validation data will take about {len(valid_data)} steps (batches) per epoch for about {len(valid_data) * batch_size} files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ZujWCbGMJRJW"
   },
   "source": [
    "ImageDataGenerator variable diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7USBCIzJRJW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276584403,
     "user_tz": 420,
     "elapsed": 18,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  print(type(train_datagen), type(train_data), len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dOavFKqzJRJW"
   },
   "source": [
    "Loop through a dataset to see its contents\n",
    "\n",
    "Note that an ImageDataGenerator will loop endlessly,\n",
    "so you have to include a break in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-8sNvVUJRJW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585124,
     "user_tz": 420,
     "elapsed": 739,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  for i, (x, y) in enumerate(train_data):\n",
    "    if i == 0:\n",
    "      print(f\"{y = }\")\n",
    "      print(f'x type: {type(x)}, y type: {type(y)}')\n",
    "      print(f'batch, x.shape, y.shape')\n",
    "    print(i, x.shape, y.shape)\n",
    "    if i >= 5:\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "-NFEHRJRJRJW"
   },
   "source": [
    "#### ImageDataGenerator example fitting\n",
    "\n",
    "Since the Image Processor is an infinite loop generator, you may have to specify steps per epoch in the fit call as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-xv7gufJRJW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585125,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# noinspection PyUnreachableCode\n",
    "if False:\n",
    "  history_x = model_x.fit(train_data,\n",
    "                          epochs=min(MAX_EPOCHS, 5),\n",
    "                          steps_per_epoch=len(train_data),\n",
    "                          validation_data=valid_data,\n",
    "                          validation_steps=len(valid_data),\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "mJr20vBUdUPX"
   },
   "source": [
    "### image_dataset_from_directory\n",
    "\n",
    "Typical Usage:\n",
    "1. Specify data directory\n",
    "1. Optional data augmentation layer\n",
    "1. Optional rescaling or pre-processing\n",
    "1. use dataset in model.fit()\n",
    "\n",
    "Notes:\n",
    "* Don't specify batch in .fit() when using image_dataset_from_directory\n",
    "  because the generator already has a batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Srw-0TFTdUPX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585125,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  # Specify training and testing for the binary classification directories\n",
    "  train_data_set = image_dataset_from_directory(dir_pizza_steak_train,\n",
    "                                                image_size=(224, 224),\n",
    "                                                batch_size=64)\n",
    "  test_data_set = image_dataset_from_directory(dir_pizza_steak_test,\n",
    "                                               image_size=(224, 224),\n",
    "                                               batch_size=64)\n",
    "  # Inspect the labels for the datasets to make sure they agree\n",
    "  print(f'Training set class names: {train_data_set.class_names}')\n",
    "  print(f'Test set class names    : {test_data_set.class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byUAE9-dJRJW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585125,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Example workflow for datasets\n",
    "if flags['datasets']:\n",
    "  train_data_set = image_dataset_from_directory(\n",
    "    dir_pizza_steak_train,\n",
    "    image_size=(180, 180),\n",
    "    shuffle=True,\n",
    "    batch_size=32)\n",
    "\n",
    "  data_augmentation = keras.Sequential(\n",
    "    [\n",
    "      layers.RandomFlip(\"horizontal\"),\n",
    "      layers.RandomRotation(0.1),\n",
    "      layers.RandomZoom(0.2),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  inputs_31 = keras.Input(shape=(180, 180, 3))\n",
    "  x = data_augmentation(inputs_31)\n",
    "  # Rescaling if you are not using a pre-processor\n",
    "  x = layers.Rescaling(1. / 255)(x)\n",
    "  # Pre-process if you are using a transfer learning model\n",
    "  # x = keras.applications.vgg16.preprocess_input(x)\n",
    "  # x = conv_base_16(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(256)(x)\n",
    "  outputs_31 = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_31 = keras.Model(inputs_31, outputs_31)\n",
    "\n",
    "  # Compile\n",
    "  # fit\n",
    "  # history_0 = model_0.fit(\n",
    "  #     train_dataset,\n",
    "  #     epochs=10,\n",
    "  #     validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "HLL8mG6edUPX"
   },
   "source": [
    "#### image_dataset_from_directory example fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Y0kthT_dUPX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585125,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# noinspection PyUnreachableCode\n",
    "if False:\n",
    "  history_x = model_x.fit(train_data_set,\n",
    "                          epochs=min(MAX_EPOCHS, 5),\n",
    "                          validation_data=test_data_set,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MkpHwaBtdUPX"
   },
   "source": [
    "#### Iterating a dataset to see its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9QgkiV9dUPX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585125,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  for i, (x, y) in enumerate(test_data_set):\n",
    "    if i == 0:\n",
    "      print(f'x type: {type(x)}\\ny type: {type(y)}')\n",
    "      print(f'batch, x.shape, y.shape')\n",
    "    print(f\"{i}, {x.shape}, {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "shT6dwk7dUPY"
   },
   "source": [
    "### Creating Dataset from Tensor Slices\n",
    "  * dset_1: dataset of 40 ints with no batching\n",
    "  * dset_1_batch_6: dset_1.batch(6)\n",
    "  * dset_1_batch_6_batch_3: dset_1_batch_6 batched again with .batch(3)\n",
    "  * imageset_batch_1: image_dataset_from_directory batched by 1\n",
    "  * imageset_batch_4: image_dataset_from_directory batched by 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98hcYKOtdUPY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585125,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  # Create dataset from list and loop through batches\n",
    "  values_1 = range(40)\n",
    "  dset_1 = tf.data.Dataset.from_tensor_slices(values_1)\n",
    "  print(f'dataset before batching')\n",
    "  print(f\"{type(dset_1) = }\\n{dset_1 = }\\n{len(dset_1) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "j2R7pPU4dUPY"
   },
   "source": [
    "### Dataset Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyz0kH6jdUPY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585126,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  dset_1_batch_6 = dset_1.batch(6)\n",
    "  print(f'dataset after batching(6)')\n",
    "  print(f\"{type(dset_1_batch_6) = }\\n{dset_1_batch_6 = }\\n{len(dset_1_batch_6) = }\")\n",
    "\n",
    "  # After batching, you can iterate through each batch\n",
    "  for (i, group) in enumerate(dset_1_batch_6):\n",
    "    if i == 0:\n",
    "      print(f'{type(group) = }')\n",
    "\n",
    "    print(f'Group {i}, shape = {group.shape}')\n",
    "    print(f'{group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rPN1sUtKdUPY"
   },
   "source": [
    "#### Calling batch on a dataset that has already been batched\n",
    "\n",
    "The new dataset has an additional dimension, so you are really creating dataset of a dataset (2-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPHYwUladUPY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585126,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  # Create batch of batch\n",
    "  dset_1_batch_6_batch_3 = dset_1_batch_6.batch(3)\n",
    "  print(f'dataset after batching again by 3')\n",
    "  print(f\"{type(dset_1_batch_6_batch_3) = }\\n{dset_1_batch_6_batch_3 = }\\n{len(dset_1_batch_6_batch_3) = }\")\n",
    "\n",
    "  for (i, group) in enumerate(dset_1_batch_6_batch_3):\n",
    "    if i == 0:\n",
    "      print(f'{type(group) = }')\n",
    "\n",
    "    print(f'Group {i}, shape = {group.shape}')\n",
    "    print(f'{group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Fh4NOPt5JRJX"
   },
   "source": [
    "#### Image Datasets and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "XeYNtJUbdUPY"
   },
   "source": [
    "Read single subdirectory into a dataset with desired image size and batch size\n",
    "\n",
    "*  The example below is from a dataset with one class of 26 letter images.\n",
    "*  shuffle=False is used so that results for diagnostics,\n",
    "but shuffle should be true for real analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EmUVBdydUPY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585126,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  imageset_batch_1 = image_dataset_from_directory(\n",
    "    tfu.paths['dir_images_local'],\n",
    "    image_size=(128, 128),\n",
    "    shuffle=False,\n",
    "    batch_size=1)\n",
    "\n",
    "  print('First 3 batches with batch size of 1')\n",
    "  for i, (x, y) in enumerate(imageset_batch_1):\n",
    "    if i >= 3:\n",
    "      break\n",
    "    print(f'loop {i}: x.shape: {x.shape}, y.shape: {y.shape}, y: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uw7PncGcdUPY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585126,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  imageset_batch_4 = image_dataset_from_directory(\n",
    "    tfu.paths['dir_images_local'],\n",
    "    image_size=(128, 128),\n",
    "    shuffle=False,\n",
    "    batch_size=4)\n",
    "\n",
    "  print('First 3 batches with batch size of 4')\n",
    "  for i, (x, y) in enumerate(imageset_batch_4):\n",
    "    if i >= 3:\n",
    "      break\n",
    "    print(f'loop {i}: x.shape: {x.shape}, y.shape: {y.shape}, y: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6mEpMegvdUPY"
   },
   "source": [
    "#### Example Batching & Visualizations of Letter Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "amcVablPdUPY"
   },
   "source": [
    "Visualize the first image in each of the first 3 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAquzuNYdUPY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585126,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  for i, (x, y) in enumerate(imageset_batch_4):\n",
    "    if i >= 3:\n",
    "      break\n",
    "    print(f'loop {i}: x.shape: {x.shape}, y.shape: {y.shape}, y: {y}')\n",
    "    # [0] because the first dimension is the batch sample #\n",
    "    tfu.plot_tensor(x[0], figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fSAI-GeqdUPY"
   },
   "source": [
    "Visualize all the images in the second image batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9-AY1jCdUPY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585126,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets']:\n",
    "  for i, (x, y) in enumerate(imageset_batch_4):\n",
    "    if i == 1:\n",
    "      print(f'loop {i}: x.shape: {x.shape}, y.shape: {y.shape}, y: {y}')\n",
    "      for image in x:\n",
    "        tfu.plot_tensor(image, figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dnMgSWmadUPY"
   },
   "source": [
    "### Convert an image Dataset into numpy array\n",
    "\n",
    "The numpy array can be subsequently saved to a single file\n",
    "\n",
    "You will likely have to squeeze out singleton dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdrK2_pwdUPZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585126,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets'] and True:\n",
    "  data_as_list = [x for (x, y) in imageset_batch_1.as_numpy_iterator()]\n",
    "  letters_numpy = np.array(data_as_list, dtype='uint8')\n",
    "  print(type(data_as_list), len(data_as_list))\n",
    "  print(type(letters_numpy), letters_numpy.shape)\n",
    "  letters_numpy = np.squeeze(letters_numpy)\n",
    "  print(type(letters_numpy), letters_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8uUSKz6ydUPZ"
   },
   "source": [
    "Save & Load numpy array to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmZw5PstdUPZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585127,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['datasets'] and True:\n",
    "  dir_images_local = tfu.paths['dir_images_local']\n",
    "  numpy_file_name = dir_images_local + 'letters.npy'\n",
    "  np.save(numpy_file_name, letters_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['datasets'] and True:\n",
    "  if run_mode == 'colab':\n",
    "    !ls -la \"$dir_images_local\"\n",
    "  else:\n",
    "    dir_wsl = dir_images_local.replace('D:', '/mnt/d')\n",
    "    !wsl ls -la \"$dir_wsl\""
   ],
   "metadata": {
    "id": "35wF-KJY3zLh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585127,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['datasets'] and True:\n",
    "  print(f'Loading numpy file: {numpy_file_name}')\n",
    "  letters_loaded = np.load(numpy_file_name)\n",
    "  print(type(letters_loaded), letters_loaded.shape)"
   ],
   "metadata": {
    "id": "QF2t2Qh-3zLh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585127,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "y6x8NOTJJRJY"
   },
   "source": [
    "# Image data augmentation\n",
    "\n",
    "Can use augmentation in two ways:\n",
    "* As a layer in the model\n",
    "  * Uses image_dataset_from_directory to create datasets for fitting\n",
    "* Using a pre-processor such as ImageDataGenerator before modeling\n",
    "  * Uses generator for fitting\n",
    "\n",
    "Visualizing augmented images for CNN\n",
    "  * tfu.plot_augmented_image(file_name, title, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NpAvSwx6JRJY"
   },
   "source": [
    "Kera augmentation layer types\n",
    "https://keras.io/api/layers/preprocessing_layers/image_augmentation/\n",
    "\n",
    "1. RandomCrop layer\n",
    "1. RandomFlip layer\n",
    "1. RandomTranslation layer\n",
    "1. RandomRotation layer\n",
    "1. RandomZoom layer\n",
    "1. RandomHeight layer\n",
    "1. RandomWidth layer\n",
    "1. RandomContrast layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "j_v3NoZLJRJY"
   },
   "source": [
    "## Image augmentation as a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "65jNMgGWJRJY"
   },
   "source": [
    "Define a data augmentation stage to add to an image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx5bJ9FBJRJY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585127,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(height_factor=0.25, width_factor=0.25),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    # layers.RandomContrast(0.2),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "jc0VKqNAJRJY"
   },
   "source": [
    "Example of including data_augmentation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ek8M5PfFJRJY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 801,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "inputs_30 = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs_30)  # Put the Augmentation before the hidden layers\n",
    "x = layers.Rescaling(1. / 255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs_30 = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_30 = keras.Model(inputs=inputs_30, outputs=outputs_30)\n",
    "\n",
    "model_30.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"rmsprop\",\n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "K843PvoCJRJY"
   },
   "source": [
    "## Image augmentation with ImageDataGenerator()\n",
    "\n",
    "Note that an ImageDataGenerator will create an infinite iterator, so use care if you are batch processing (you may have to put in a break in the loop)\n",
    "\n",
    "Resources/Tutorials:\n",
    "* [tf tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation)\n",
    "* [Great 3rd party tutorial](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBu2iym6JRJY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 25,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  # create data generator\n",
    "  from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "  datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pDg-OMjsJRJY"
   },
   "source": [
    "Sample image files for augmentation are at: dir_images_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "T7bthgYsJRJY"
   },
   "source": [
    "## Augment array of images that are already in numpy format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "edktYmUDJRJY"
   },
   "source": [
    "load the numpy array of 26 letter images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8yBvzKfJRJY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  # Load in numpy letters tensor\n",
    "  numpy_file_name = tfu.paths['dir_images_local'] + 'letters.npy'\n",
    "  np_letters = np.load(numpy_file_name)\n",
    "  print(f'np_letters: shape={np_letters.shape}, dtype={np_letters.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "uhDjBUGwJRJY"
   },
   "source": [
    "Augment with horizontal shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LZeaP2EJRJY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  # Select the number of pictures per batch\n",
    "  # And how many batches you wish to visualize\n",
    "  num_pics_per_batch = 4\n",
    "  num_batches_to_plot = 2\n",
    "\n",
    "  # prepare iterator\n",
    "  datagen = ImageDataGenerator(width_shift_range=[-20, 20])\n",
    "  it = datagen.flow(np_letters, batch_size=num_pics_per_batch, shuffle=False)\n",
    "\n",
    "  # generate samples and plot\n",
    "  for i in range(num_batches_to_plot):\n",
    "    # generate batch of images\n",
    "    batch = it.next()\n",
    "    print(f'Batch {i} shape: {batch.shape}')\n",
    "    for j, member in enumerate(batch):\n",
    "      # convert to unsigned integers for viewing\n",
    "      # even if the array used to be integer, it may switch to float after transformation\n",
    "      print(f'Member {j} of {len(batch) - 1}')\n",
    "      tfu.plot_tensor(member, figsize=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "-L_NYHgwJRJY"
   },
   "source": [
    "## Tutorial on image augmentation for a single image of a bird\n",
    "\n",
    "[How to Configure Image Data Augmentation in Keras](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)\n",
    "\n",
    "Note: it looks like vertical and horizontal are the opposite of what I expected, I think this may be a bug in the algorithm, but since I will generally be using flow_from_directory rather than Image Generator, I don't think it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxRieGCcJRJY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  # os.chdir(tfu.paths['dir_images_local'])\n",
    "  bird_fn = tfu.paths['dir_images_local'] + 'bird.jpg'\n",
    "\n",
    "  jpg = load_img(bird_fn)  # load the image\n",
    "  jpg_as_float = img_to_array(jpg)  # convert to numpy array\n",
    "  jpg_as_int = jpg_as_float.astype('uint8')  # imshow expects type of unsigned ints\n",
    "  samples = np.expand_dims(jpg_as_int, 0)  # Optionally expand dimension to one sample\n",
    "  plt.imshow(jpg_as_int)  # plot raw pixel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BOAHxe6yJRJY"
   },
   "source": [
    "Horizontal Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijRQakH7JRJY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  title = 'Horizontal Shift'\n",
    "  augments = {'width_shift_range': [-100, 100]}\n",
    "  tfu.plot_augmented_image(bird_fn, title, **augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "WB9lxq-LJRJY"
   },
   "source": [
    "Vertical Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPfPa48CJRJZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  title = 'Vertical Shift'\n",
    "  augments = {'height_shift_range': 0.5}\n",
    "  tfu.plot_augmented_image(bird_fn, title, **augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YCRfCyjZJRJZ"
   },
   "source": [
    "Horizontal AND Vertical shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unxfWGBoJRJZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 23,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  title = 'Both Shift'\n",
    "  augments = {'width_shift_range': [-200, 200], 'height_shift_range': 0.5}\n",
    "  tfu.plot_augmented_image(bird_fn, title, **augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sz20cZVeJRJZ"
   },
   "source": [
    "Horizontal Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyqKIDObJRJZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 23,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  title = 'Horiz Flip'\n",
    "  augments = {'horizontal_flip': True}\n",
    "  tfu.plot_augmented_image(bird_fn, title, **augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "telgsxKnJRJZ"
   },
   "source": [
    "Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUU_Hh0qJRJZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585921,
     "user_tz": 420,
     "elapsed": 23,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  title = 'Rotations'\n",
    "  augments = {'rotation_range': 90}\n",
    "  tfu.plot_augmented_image(bird_fn, title, **augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8XMo4H6qJRJZ"
   },
   "source": [
    "Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6z3devUMJRJZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  title = 'Brightness'\n",
    "  augments = {'brightness_range': [0.2, 1.0]}\n",
    "  tfu.plot_augmented_image(bird_fn, title, **augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IKBjbnejJRJZ"
   },
   "source": [
    "Random Zooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhhYVmyoJRJZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation']:\n",
    "  title = 'Rando Zoom'\n",
    "  augments = {'zoom_range': [0.5, 1.0]}\n",
    "  tfu.plot_augmented_image(bird_fn, title, **augments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "F8KhNKNRdUPZ"
   },
   "source": [
    "# Kera's Generic Modeling Workflow\n",
    "\n",
    "1. Design Model Layers\n",
    "2. Compile\n",
    "3. Fit\n",
    "4. Evaluate\n",
    "5. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Z-lh0-92dUPZ"
   },
   "source": [
    "## Improving Model Performance\n",
    "\n",
    "Consider the following changes:\n",
    "* Add features\n",
    "* Add more training data\n",
    "* Add more layers\n",
    "* Add more nodes per layer\n",
    "* Change activation functions\n",
    "* Change loss functions\n",
    "* Change optimizer\n",
    "* Change learning rate\n",
    "* Learn longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JTytNOWWdUPZ"
   },
   "source": [
    "## Early Stopping\n",
    "Discussed further in the 'Typical Fitting Callbacks' section, early stopping is a key callback to ensure that the model does not continue to train when over-fitting.\n",
    "\n",
    "Consider using in all fitting unless the fitting is explicitly designed\n",
    "to over fit for demonstration purposes.\n",
    "\n",
    "Usage:\n",
    "* model.fit( ..., callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adOZ2hDudUPZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Stop after val_accuracy does not improve for a certain number of epochs\n",
    "early_stopping = [keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                patience=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Rate Scheduler\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "LNvZg_tP3zLl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scheduler_func_1(epoch, lr):\n",
    "  \"\"\"Tapers the learning rate exponentially after 10 epochs\"\"\"\n",
    "  if epoch < 10:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.05)"
   ],
   "metadata": {
    "id": "ooEnjDh-3zLm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 24,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lrs_callback = tf.keras.callbacks.LearningRateScheduler(scheduler_func_1)"
   ],
   "metadata": {
    "id": "psFrg4mb3zLm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 23,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "W7VktysxdUPZ"
   },
   "source": [
    "## Preprocess & Normalize data\n",
    "\n",
    "For more details, see the Splitting Data Section."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "JDit8BtQ3zLm"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BoLb1YnHdUPZ"
   },
   "source": [
    "Data pre-processing for a simple regression model\n",
    "\n",
    "Note that this dataset and model are very sensitive and has trouble converging.\n",
    "* Sensitive to the range of x values (min, max)\n",
    "* does not work well with minmax standardization (use normalization instead)\n",
    "* sgd seems to work better than adam\n",
    "* may require more than 100 epochs (perhaps 500) to get good results\n",
    "* if the range of min max is small (say 1 to 10) normalizatino does not help much if at all\n",
    "* if there is a large range of data, only the normalized data will converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVHTCvn3dUPZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 23,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "a62fa993-af9a-4b35-91be-46dbb37b8619"
   },
   "outputs": [],
   "source": [
    "# Example Data for Regression Modeling for a straight line y = x + 17\n",
    "# X is shape (num_samples, 1)\n",
    "num_samples = 25\n",
    "X = np.linspace(-1, 25, num_samples, dtype=float)\n",
    "X = np.expand_dims(X, axis=-1)\n",
    "y = 50 + 10 * X\n",
    "test_fraction = 0.15\n",
    "val_fraction = 0.0\n",
    "\n",
    "# split into train, validation, and testing\n",
    "splits = tfu.split_data(X, y, test_size=test_fraction)\n",
    "X_train, X_test, _X_val, y_train, y_test, _y_val = splits\n",
    "\n",
    "# Use a column transformer to standardize input data\n",
    "# Values will range between -1 and 1 for the first non-sample column\n",
    "ct = make_column_transformer((StandardScaler(), [0]))\n",
    "\n",
    "# Fit column transformer on the training data only (doing so on test data would result in data leakage)\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform training and test data with normalization (MinMaxScalar)\n",
    "# These standardized data can be used as model.fit input\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "waHXQATKdUPZ"
   },
   "source": [
    "## Model Architecture Design\n",
    "Model's can be specified as:\n",
    "  * Sequential, or\n",
    "  * Functional API\n",
    "\n",
    "Input Layer:\n",
    "  * Optional, but input must be specified before model weights can be determined\n",
    "\n",
    "Output Layers:\n",
    "  * For regression:\n",
    "    - Last layer should have 1 node with no activation\n",
    "  * Binary Classification:\n",
    "    - Last layer should have 1 node with sigmoid activation\n",
    "    - loss should be binary_crossentropy\n",
    "  * Multiclass Classification:\n",
    "    - Last layer should have as many nodes as there are classes and softmax activation\n",
    "    - loss should be categorical_crossentropy if one-hot encoded or sparse_categorical_crossentropy if your target is a single integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0u8HeHN3dUPZ"
   },
   "source": [
    "### Layer Types Introduction\n",
    "Common layer types include:\n",
    "  * Input\n",
    "    * keras.Input(shape=(3,))\n",
    "  * Dense\n",
    "    * layers.Dense(64)\n",
    "  * Dropout\n",
    "    * layers.Dropout(0.5)\n",
    "\n",
    "Example Dense layer:\n",
    "\n",
    "```\n",
    "  layers.Dense(16,\n",
    "               activation=\"relu\",\n",
    "               kernel_regularizer=regularizers.l2(0.002),\n",
    "               name=\"my_layer\"),\n",
    "```\n",
    "Typical Dense layer options:\n",
    "  * Number of nodes\n",
    "  * activation type\n",
    "  * regularizers\n",
    "  * name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rEzYW_gUdUPZ"
   },
   "source": [
    "Basic Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "149uyocqdUPZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 14,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "model_10 = keras.Sequential([\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5LQqSuBqdUPZ"
   },
   "source": [
    "Incrementally building a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAMCxk9OdUPZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 14,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "model_11 = keras.Sequential(name=\"my_example_model\")\n",
    "model_11.add(keras.Input(shape=(3,)))  # optional input shape\n",
    "model_11.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n",
    "model_11.add(layers.Dense(10, activation=\"softmax\", name=\"my_second_layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gls8Au0pdUPa"
   },
   "source": [
    "Example Sequential Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmVd9W5OdUPa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 13,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "model_12 = models.Sequential([\n",
    "  layers.Input(shape=(3,)),  # optional specification of input shape\n",
    "  layers.Dense(32, activation=\"relu\"),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(16,\n",
    "               kernel_regularizer=regularizers.l2(0.002),\n",
    "               activation=\"relu\"),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "L-ZOgr8XdUPa"
   },
   "source": [
    "#### Building a model to initialize weights\n",
    "You must know the input shape to initialize the weights.\n",
    "If your model does not have the optional input_shape as a layer,\n",
    "then you must call .build() explicitly with the input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egiXWq5DdUPa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 13,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "d718f905-f004-489e-94ab-d436dfd44438"
   },
   "outputs": [],
   "source": [
    "model_10.build(input_shape=(None, 3))\n",
    "model_10.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2gqLd2AJdUPa"
   },
   "source": [
    "### Using the Functional API\n",
    "Using a model with two dense layers as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g21gVEICdUPa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 9,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "373ac002-8a12-4ac2-9a84-d17ea5076bbd"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model_13 = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(f'input shape: {inputs.shape}, output shape: {features.shape}\\n')\n",
    "model_13.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Fbhwo-H6dUPa"
   },
   "source": [
    "#### MNIST Functional API Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XI5tzmSxdUPa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276585922,
     "user_tz": 420,
     "elapsed": 3,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "inputs_32 = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs_32)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs_32 = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model_32 = keras.Model(inputs=inputs_32, outputs=outputs_32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qZwoCVp_dUPa"
   },
   "source": [
    "### Multi-input, multi-output models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vML9j6YqdUPa"
   },
   "source": [
    "Example input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6Al-KkPdUPa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586711,
     "user_tz": 420,
     "elapsed": 792,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Todo - come back here and figure this out :)\n",
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "num_samples = 1280\n",
    "\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
    "\n",
    "priority_data = np.random.random(size=(num_samples, 1))\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZ95x0ejdUPa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586712,
     "user_tz": 420,
     "elapsed": 51,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "2b5a8db8-6c19-46be-cd5f-a05ea1578d73"
   },
   "outputs": [],
   "source": [
    "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
    "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
    "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
    "\n",
    "features = layers.Concatenate()([title, text_body, tags])\n",
    "features = layers.Dense(64, activation=\"relu\")(features)\n",
    "\n",
    "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
    "department = layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(features)\n",
    "\n",
    "model_14 = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])\n",
    "tfu.plot_model_summary(model_14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "-fD3IV9UdUPa"
   },
   "source": [
    "Specifying A multi-input, multi-output Functional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "o-59fmPRdUPa"
   },
   "source": [
    "#### Training a multi-input, multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cjJE5mqldUPa"
   },
   "source": [
    "##### Training a model with lists of input & target arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDtRWkn6dUPa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586712,
     "user_tz": 420,
     "elapsed": 43,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['functional_api_example']:\n",
    "  model_14.compile(optimizer=\"rmsprop\",\n",
    "                   loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
    "                   metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
    "  model_14.fit([title_data, text_body_data, tags_data],\n",
    "               [priority_data, department_data],\n",
    "               epochs=min(MAX_EPOCHS, 10))\n",
    "  model_14.evaluate([title_data, text_body_data, tags_data],\n",
    "                    [priority_data, department_data])\n",
    "  priority_preds, department_preds = model_14.predict([title_data, text_body_data, tags_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_X7uMcmzdUPa"
   },
   "source": [
    "##### Training with dicts of input & target arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_xa4YUhdUPa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586712,
     "user_tz": 420,
     "elapsed": 43,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['functional_api_example']:\n",
    "  model_14.compile(optimizer=\"rmsprop\",\n",
    "                   loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n",
    "                   metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})\n",
    "  model_14.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "               {\"priority\": priority_data, \"department\": department_data},\n",
    "               epochs=min(MAX_EPOCHS, 10))\n",
    "  model_14.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "                    {\"priority\": priority_data, \"department\": department_data})\n",
    "  priority_preds, department_preds = model_14.predict(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1oC2VuD1dUPb"
   },
   "source": [
    "#### Inspecting Function model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyM3x28hdUPb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586712,
     "user_tz": 420,
     "elapsed": 43,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "cf3d61d5-bbbd-463c-9197-9155f2736762"
   },
   "outputs": [],
   "source": [
    "model_14.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2geVlL_dUPb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586712,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "fec8e93d-7a39-42dc-e68b-e6dce141a628"
   },
   "outputs": [],
   "source": [
    "model_14.layers[3].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_MakSdwdUPb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586712,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "a12ff704-6dc3-4f50-8b52-b758fcbe3d75"
   },
   "outputs": [],
   "source": [
    "model_14.layers[3].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QMwxFBd5dUPb"
   },
   "source": [
    "### Adding inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qa1TBVRrdUPb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586712,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new model by reusing intermediate layer outputs\n",
    "\n",
    "features = model_14.layers[4].output\n",
    "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
    "\n",
    "model_15 = keras.Model(\n",
    "  inputs=[title, text_body, tags],\n",
    "  outputs=[priority, department, difficulty])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NjWrrewsJRJb"
   },
   "source": [
    "## Layers - Most Common and Their Order\n",
    "1. Input\n",
    "2. Data Augmentation\n",
    "3. Scaling\n",
    "4. Conv2D(image classification)\n",
    "5. Filters should get larger deeper into the layer (powers of 2)\n",
    "6. Kernel size is typically 3 or 5\n",
    "1  Conv2DTranspose (Image Segmentation)\n",
    "7. MaxPooling2D (don't forget to pool after Conv2D)\n",
    "8. Flatten\n",
    "9. Dropout\n",
    "10. Dense Layers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFFJc6w4dUPb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586713,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Typical Modeling Order using Functional API\n",
    "inputs = keras.Input(shape=(28 * 28,))\n",
    "features = layers.Rescaling(1. / 255)(inputs)\n",
    "features = layers.Dense(512, activation=\"relu\")(features)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model_16 = keras.Model(inputs, outputs)\n",
    "\n",
    "# Image Processing Layers:\n",
    "# layers.Conv2D(filters, kernel_size)\n",
    "# layers.Conv2DTranspose(filters, kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "2Q51Nrv7dUPb"
   },
   "source": [
    "## Model Compilation\n",
    "Required to specify:\n",
    "1. Loss function\n",
    "2. Optimizer\n",
    "3. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kgqhjmH8dUPb"
   },
   "source": [
    "Model specification for 2 simple regression models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzgxwwXYdUPb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586713,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# model_20 uses non-normalized data\n",
    "# model_21 uses normalized data\n",
    "\n",
    "if flags['regression_example']:\n",
    "  # Set random seed\n",
    "  tf.random.set_seed(42)\n",
    "\n",
    "  # Non-normalized Model Geometry\n",
    "  model_20 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, name='layer1'),\n",
    "  ])\n",
    "\n",
    "  # Normalized Model Geometry\n",
    "  model_21 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, name='layer1'),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Me3NkOfXdUPb"
   },
   "source": [
    "Example regression model uses same compilation options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_3m99W2dUPb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586713,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['regression_example']:\n",
    "  # Compile the models\n",
    "  model_20.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                   optimizer='sgd',\n",
    "                   metrics=['mae', 'mse'])\n",
    "\n",
    "  model_21.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                   optimizer='sgd',\n",
    "                   metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "-xuiik2QdUPb"
   },
   "source": [
    "## Typical Fitting Callbacks\n",
    "\n",
    "Subclasses of keras.callbacks\n",
    "* **ModelCheckpoint**:  Saving the current state of the model at different points during training.\n",
    "* **EarlyStopping**: Interrupting training when the validation loss is no longer improving (and of course, saving the best model obtained during training).\n",
    "* **LearningRateScheduler**: Dynamically adjusting the value of certain parameters during training\n",
    "* **CSVLogger**: Logging training and validation metrics during training, or visualizing the representations learned by the model as they’re updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUv0ijbZdUPb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586713,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Early stopping, model checkpoints, csv logger, tensorboard\n",
    "model_callbacks_01 = [\n",
    "  # Stop after val_accuracy does not improve for patience epochs\n",
    "  keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "  ),\n",
    "  # Save model to filepath, only write over the last save if val_loss improved\n",
    "  keras.callbacks.ModelCheckpoint(\n",
    "    filepath=tfu.paths['dir_model_runs'] + \"model_20_checkpoint\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "  ),\n",
    "  # Save history\n",
    "  tf.keras.callbacks.CSVLogger(tfu.paths['dir_model_runs'] + \"model_20_logger\",\n",
    "                               separator=\",\",\n",
    "                               append=True),\n",
    "  # Save to tensorboard\n",
    "  keras.callbacks.TensorBoard(log_dir=tfu.paths['dir_tensor_board'] + \"model_20\"),\n",
    "]\n",
    "\n",
    "# Early stopping, model checkpoints, csv logger, tensorboard\n",
    "model_callbacks_02 = [\n",
    "  # Stop after val_accuracy does not improve for patience epochs\n",
    "  keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "  ),\n",
    "  # Save model to filepath, only write over the last save if val_loss improved\n",
    "  keras.callbacks.ModelCheckpoint(\n",
    "    filepath=tfu.paths['dir_model_runs'] + \"model_21_checkpoint\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "  ),\n",
    "  # Save history\n",
    "  tf.keras.callbacks.CSVLogger(tfu.paths['dir_model_runs'] + \"model_21_logger\",\n",
    "                               separator=\",\",\n",
    "                               append=True),\n",
    "  # Save to tensorboard\n",
    "  keras.callbacks.TensorBoard(log_dir=tfu.paths['dir_tensor_board'] + \"model_21\"),\n",
    "]\n",
    "\n",
    "# Early stopping, no checkpoints, revert to best fit\n",
    "model_callbacks_03 = [\n",
    "  # Stop after val_accuracy does not improve for patience epochs\n",
    "  keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "  ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JYeH67LMdUPc"
   },
   "source": [
    "## Run (fit) model\n",
    "Specify the following:\n",
    "1. Training data (X_train, y_train)\n",
    "2. Epochs\n",
    "3. Optional callbacks\n",
    "4. Validation data by split or x, y datasets\n",
    "5. validation_data=(X_test, y_test)\n",
    "6. validation_split=0.2\n",
    "7. Other Optional options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVTC9pdhdUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586713,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['regression_example']:\n",
    "  # Fit the models\n",
    "  history_20 = model_20.fit(X_train,\n",
    "                            y_train,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            epochs=min(MAX_EPOCHS, 100),  #orginally 50\n",
    "                            verbose=0,\n",
    "                            callbacks=model_callbacks_01)\n",
    "  history_21 = model_21.fit(X_train_normal,\n",
    "                            y_train,\n",
    "                            validation_data=(X_test_normal, y_test),\n",
    "                            epochs=min(MAX_EPOCHS, 100),  #orginally 50\n",
    "                            verbose=0,\n",
    "                            callbacks=model_callbacks_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "37tHOjxudUPc"
   },
   "source": [
    "## Plot Model History\n",
    "* tfu.plot_history_metric(history, metric='mae',title=None)\n",
    "* tfu.plot_all_history_metrics(history, title=None)\n",
    "* tfu.plot_loss_and_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xnj1pr3wdUPc"
   },
   "source": [
    "## Saving a model and its history\n",
    "Saves a model to a specified path:\n",
    "* model.save(filepath)\n",
    "\n",
    "History's need to be converted to a DataFrame and then saved as a .csv\n",
    "*  history_df = pd.DataFrame(history.history)\n",
    "*  history_df.to_csv(filename.csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAFbUCo3dUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586713,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['regression_example']:\n",
    "  # Save the models\n",
    "  model_20.save(tfu.paths['dir_model_runs'] + 'model_20')\n",
    "  model_21.save(tfu.paths['dir_model_runs'] + 'model_21')\n",
    "\n",
    "  # Save the histories\n",
    "  history_20_df = pd.DataFrame(history_20.history)\n",
    "  history_20_df.to_csv(tfu.paths['dir_model_runs'] + \"model_20_history.csv\", index=False)\n",
    "  history_21_df = pd.DataFrame(history_21.history)\n",
    "  history_21_df.to_csv(tfu.paths['dir_model_runs'] + \"model_21_history.csv\", index=False)\n",
    "\n",
    "  # Show that saved and loaded models give the same evaluations\n",
    "  model_20_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + 'model_20')\n",
    "\n",
    "  model_20.summary()\n",
    "  model_20_loaded.summary()\n",
    "  eval_20 = model_20.evaluate(X_test, y_test)\n",
    "  eval_20_loaded = model_20_loaded.evaluate(X_test, y_test)\n",
    "  print(f'Original model evaluation: {eval_20}')\n",
    "  print(f'Loaded   model evaluation: {eval_20_loaded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MxS0l0bndUPc"
   },
   "source": [
    "## Load a model and its history\n",
    "*  model_loaded = tf.keras.models.load_model(filepath)\n",
    "*  history_loaded = pd.read_csv(filename.csv)\n",
    "\n",
    "Notes:\n",
    "* load the model checkpoint because it saves only the best fit (not the most recent)\n",
    "* class names do not appear to be saved in the model or the history - see workaround in Binary Classification Example on how to save class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTn9wFFodUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586713,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['regression_example']:\n",
    "  model_20_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + 'model_20_checkpoint')\n",
    "  history_20_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_20_history.csv\")\n",
    "  model_21_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + 'model_21_checkpoint')\n",
    "  history_21_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_21_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DPbj-jv3dUPc"
   },
   "source": [
    "## Visualizing Model Architecture\n",
    "Two common ways to visualize model architecture are:\n",
    "* model.summary()\n",
    "  * Built-in text depiction of model\n",
    "* tfu.plot_model_summary(model)\n",
    "  * Graphical depiction of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X46MOzP3dUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['plot_regression_example']:\n",
    "  model_20_loaded.summary()\n",
    "  fig = tfu.plot_model_summary(model_20_loaded)\n",
    "  IPython.display.display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ax-dey-udUPc"
   },
   "source": [
    "## Baseline Performance by random shuffling of labels\n",
    "* tfu.random_guess_accuracy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwXbCNwqdUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Determine baseline accuracy associated with random shuffling of the y labels\n",
    "if flags['regression_example']:\n",
    "  shuffle_baseline = tfu.random_guess_accuracy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NBi88mU-dUPc"
   },
   "source": [
    "## Make Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4h4G7lTBdUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['plot_regression_example']:\n",
    "  # Make predictions based on model fit\n",
    "  y_preds_20 = model_20_loaded.predict(X_test)\n",
    "  y_preds_21 = model_21_loaded.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tBLh_ubZdUPc"
   },
   "source": [
    "## Evaluate Model Metrics\n",
    "* tfu.model_evaluate(model, X_test, y_test)\n",
    "* tfu.mae_mse_metrics(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kCAFOeAAdUPc"
   },
   "source": [
    "## Plot Model Predictions\n",
    "Key plotting routines are:\n",
    "* tfu.scatter_plot(y_actual, y_predicted, title=None)\n",
    "* tfu.plot_predictions(train_data, train_labels, test_data, test_labels, predictions)\n",
    "* tfu.pred_and_plot2(model, file_names, class_names_dict, img_shape, actual_class_names=None)\n",
    "* tfu.make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ghdnah0dUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['plot_regression_example']:\n",
    "  print('Evaluating model_20')\n",
    "  eval_20 = tfu.model_evaluate(model_20_loaded, X_test, y_test)\n",
    "  tfu.mae_mse_metrics(y_test, y_preds_20)\n",
    "\n",
    "  print('Evaluating model_21')\n",
    "  eval_21 = tfu.model_evaluate(model_21_loaded, X_test_normal, y_test)\n",
    "  tfu.mae_mse_metrics(y_test, y_preds_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I62L2OZidUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['plot_regression_example']:\n",
    "  tfu.plot_history_metric(history_20_loaded, title='No Normalization')\n",
    "  tfu.plot_all_history_metrics(history_20_loaded, title='No Normalization')\n",
    "  tfu.plot_all_history_metrics(history_21_loaded, title='With Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpJwaGVNdUPc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['plot_regression_example']:\n",
    "  tfu.plot_predictions(train_data=X_train,\n",
    "                       train_labels=y_train,\n",
    "                       test_data=X_test,\n",
    "                       test_labels=y_test,\n",
    "                       predictions=y_preds_20)\n",
    "  plt.title('No Normalization')\n",
    "  plt.figure()\n",
    "\n",
    "  tfu.plot_predictions(train_data=X_train_normal,\n",
    "                       train_labels=y_train,\n",
    "                       test_data=X_test_normal,\n",
    "                       test_labels=y_test,\n",
    "                       predictions=y_preds_21)\n",
    "  plt.title('With Normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression Example (one cell)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Mxgq1pc63zLt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['plot_regression_example'] and True:\n",
    "    model_20a = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(1, activation=None, name='layer1'),\n",
    "    ])\n",
    "\n",
    "    model_20a.compile(loss='mean_squared_error',\n",
    "                      optimizer='sgd',\n",
    "                      metrics=['mae', 'mse'])\n",
    "\n",
    "    model_callbacks_20a = [\n",
    "      # Stop after val_accuracy does not improve for patience epochs\n",
    "      keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "      ),\n",
    "    ]\n",
    "\n",
    "    # Fit the models\n",
    "    history_20a = model_20a.fit(X_train_normal,\n",
    "                              y_train,\n",
    "                              validation_data=(X_test_normal, y_test),\n",
    "                              epochs=500,  #orginally 50\n",
    "                              verbose=0,\n",
    "                              callbacks=model_callbacks_20a)\n",
    "\n",
    "    print('Evaluating model_20a')\n",
    "    y_preds_20a = model_20a.predict(X_test_normal)\n",
    "    eval_20 = tfu.model_evaluate(model_20a, X_test_normal, y_test)\n",
    "    tfu.mae_mse_metrics(y_test, y_preds_20a)\n",
    "    tfu.plot_history_metric(history_20a, title='Model 20a')\n",
    "\n",
    "    tfu.plot_predictions(train_data=X_train_normal,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test_normal,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=y_preds_20a)\n",
    "    plt.title('Model 20a, no model checkpoints')\n",
    "    plt.figure()"
   ],
   "metadata": {
    "id": "_X6E8raE3zLt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rDSEE4D-5yvv"
   },
   "source": [
    "## Retrain a Model\n",
    "* The default initial_epoch is 0\n",
    "* epoch parameter is the # of epochs in training\n",
    "  * the first pass will be indexed as 1 plus the starting epoch\n",
    "    * if epoch=4, you model 1/4, 2/4, 3/4, 4/4\n",
    "* If you retrain, the initial_epoch should be the previous end epoch\n",
    "  *  initial_epoch=4, epochs=7 you model 5/7, 6/7, 7/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex6JW57X5yvv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586714,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['retrain'] is True:\n",
    "  model_22 = keras.Sequential([\n",
    "    layers.Dense(12, activation=\"relu\"),\n",
    "    layers.Dense(12, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "  model_22.compile(loss=tf.keras.losses.mae,\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5MQUQ0d5yvv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['retrain'] is True:\n",
    "  histories = []\n",
    "  epochs = [0, 3, 7, 15]\n",
    "  for initial_epoch, final_epoch in zip(epochs, epochs[1:]):\n",
    "    print(f\"{initial_epoch=}, {final_epoch=}\")\n",
    "    history = model_22.fit(X_train,\n",
    "                           y_train,\n",
    "                           validation_data=(X_test, y_test),\n",
    "                           initial_epoch=initial_epoch,\n",
    "                           epochs=final_epoch, )\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewidvPeS5yvv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['retrain'] is True:\n",
    "  tfu.plot_history_list(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D9lIo9kGw3s"
   },
   "source": [
    "# Image Binary Classification\n",
    "#todo ImageDataGenerators use augmentation, but it appears only to use about 25% of the CPU and 25% of the GPU,\n",
    "perhaps using flow from directory and augmentation as layers would be faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr9XzgNEG9Tb"
   },
   "source": [
    "## Download & Inspect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc9Cm9XkUkYr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  # Load pizza steak images if necessary\n",
    "  tfu.url_to_local_dir(url_pizza_steak, dir_images_local)\n",
    "\n",
    "  # Find the labels and class counts for the test data\n",
    "  y_labels, counts = tfu.generator_labels(dir_pizza_steak_test, class_mode='binary')\n",
    "\n",
    "  # Find baseline accuracy using test labels (faster if you already have y_labels)\n",
    "  tfu.random_guess_accuracy(y_labels)\n",
    "\n",
    "  class_names_pizza_steak = tfu.dir_class_names(dir_pizza_steak_test)\n",
    "  print(f'Binary class names: {class_names_pizza_steak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JG-sDkthJRJc"
   },
   "source": [
    "List file counts in each directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yoa59T39JRJc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  tfu.walk_directory(dir_pizza_steak_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "j9csMF-0JRJc"
   },
   "source": [
    "Plot example files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scnY9p_AJRJc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  # Plot random examples of input files for each class\n",
    "  tfu.plot_all_classes(dir_pizza_steak_test, 2, figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyafRJmjKYFD"
   },
   "source": [
    "## Create ImageDataGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8w_6yWdcPGZH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  # Create ImageDataGenerator training instance without data augmentation\n",
    "  train_datagen_2 = ImageDataGenerator(rescale=1 / 255.)\n",
    "\n",
    "  # Create ImageDataGenerator training instance with data augmentation\n",
    "  train_datagen_augmented_2 = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                                 rotation_range=20,\n",
    "                                                 # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
    "                                                 shear_range=0.2,  # shear the image\n",
    "                                                 zoom_range=0.2,  # zoom into the image\n",
    "                                                 width_shift_range=0.2,  # shift the image width ways\n",
    "                                                 height_shift_range=0.2,  # shift the image height ways\n",
    "                                                 horizontal_flip=True)  # flip the image on the horizontal axis\n",
    "\n",
    "  # Create ImageDataGenerator test instance without data augmentation\n",
    "  test_datagen_2 = ImageDataGenerator(rescale=1 / 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2GawIT5KbNN"
   },
   "source": [
    "#### flow_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZXl-kEoPoEP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  # Create non-augmented data batches\n",
    "  print(\"Non-augmented training images:\")\n",
    "  train_data_2 = train_datagen_2.flow_from_directory(dir_pizza_steak_train,\n",
    "                                                     target_size=(224, 224),\n",
    "                                                     batch_size=32,\n",
    "                                                     class_mode='binary',\n",
    "                                                     shuffle=False)  # Don't shuffle for demonstration purposes\n",
    "\n",
    "  # Import data and augment it from training directory\n",
    "  print(\"Augmented training images with shuffle off:\")\n",
    "  train_data_augmented_2 = train_datagen_augmented_2.flow_from_directory(\n",
    "    dir_pizza_steak_train,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)  # Don't shuffle for demonstration purposes, usually a good thing to shuffle\n",
    "\n",
    "  # Import data and augment it from directories\n",
    "  print(\"Augmented training images with shuffle on:\")\n",
    "  train_data_augmented_shuffled_2 = train_datagen_augmented_2.flow_from_directory(\n",
    "    dir_pizza_steak_train,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=True)  # Shuffle data (default)\n",
    "\n",
    "  print(\"Unchanged test images:\")\n",
    "  test_data_2 = test_datagen_2.flow_from_directory(dir_pizza_steak_test,\n",
    "                                                   target_size=(224, 224),\n",
    "                                                   batch_size=32,\n",
    "                                                   class_mode='binary',\n",
    "                                                   shuffle=False)\n",
    "\n",
    "  # Optional: create a dictionary with index number as the key and label as the value\n",
    "  # class_names_dict = {value: key for key, value in test_data_2.class_indices.items()}\n",
    "  # class_names_df = pd.DataFrame.from_dict(class_names_dict, orient='index')\n",
    "  # class_names_df.to_csv(model_runs + \"model_38_class_names.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IazsMpf07Eq-"
   },
   "source": [
    "## Define Model Structure\n",
    "Note, 38a and 38b don't always converge (at least not on pycharm)\n",
    "38b is a clone of 38a - so it is weird that one would converge and not the other\n",
    "I think it is because of the initial random generation of parameters, or a learning rate issue\n",
    "\n",
    "When I put in a lr scheduler that used slow learning for the 1st 3 epochs, it appeared to\n",
    "converge 3 times (but then did not converge on the 4th)\n",
    "\n",
    "tried to shuffle the weights, but that did not seem to help (https://gist.github.com/jkleint/eb6dc49c861a1c21b612b568dd188668)\n",
    "\n",
    "The only solution I could find was to re-define, re-compile, then re-fit the model's ... seemingly at random, this appears to 'fix' the convergence problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scheduler_func_2(epoch, lr):\n",
    "  \"\"\"Slow learning rate to start with, then fast\"\"\"\n",
    "  if epoch < 3:\n",
    "    return 0.0005\n",
    "  else:\n",
    "    return 0.001\n",
    "\n",
    "lrs_callback = tf.keras.callbacks.LearningRateScheduler(scheduler_func_2)\n",
    "stopping_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)"
   ],
   "metadata": {
    "id": "4aCBj_tz3zLv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586715,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callbacks_stopping_only = [stopping_callback]\n",
    "callbacks_stop_and_lrs = [stopping_callback, stopping_callback]"
   ],
   "metadata": {
    "id": "RxHbTo5D3zLv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6I5ayvQSP5j",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  # Original model 8 from tutorial\n",
    "  model_38 = Sequential([\n",
    "    Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),  # same input shape as our images\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  # Enhanced model 8 trying to get better performance\n",
    "  model_38a = Sequential([\n",
    "    Conv2D(16, 3, activation='relu', input_shape=(224, 224, 3)),  # same input shape as our images\n",
    "    Conv2D(16, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(32, 3, activation='relu'),\n",
    "    Conv2D(32, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  # Using clone to copy the layer structure of another model\n",
    "  # Note you need to re-compile the cloned model before fitting\n",
    "  model_38b = tf.keras.models.clone_model(model_38a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEIQTAj87LH2"
   },
   "source": [
    "## Compile the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odxVJfB07JuN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  model_38.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "  model_38a.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    # optimizer=\"rmsprop\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "  model_38b.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    # optimizer=\"rmsprop\",\n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JfgrgrxfdUPf"
   },
   "source": [
    "## Fit & Save Model\n",
    "\n",
    "Note that if you are using an ImageDataGenerator, you will have to specify:\n",
    "* **steps_per_epoch**=len(train_data),\n",
    "* **validation_steps**=len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_ADjYnS6f0f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  history_38 = model_38.fit(train_data_augmented_shuffled_2,\n",
    "                            callbacks=early_stopping,\n",
    "                            epochs=min(MAX_EPOCHS, 35),  # I think it originally was 5, use 35\n",
    "                            steps_per_epoch=len(train_data_augmented_shuffled_2),\n",
    "                            validation_data=test_data_2,\n",
    "                            validation_steps=len(test_data_2))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezodk5JSdUPf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  model_38.save(tfu.paths['dir_model_runs'] + \"model_38_saved\")\n",
    "  history_38_df = pd.DataFrame(history_38.history)\n",
    "  history_38_df.to_csv(tfu.paths['dir_model_runs'] + \"model_38_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hack fix to reset all the parameters on a model if it is failing to converge\n",
    "# If the model is not converging, rerun this section before re-fitting\n",
    "\n",
    "# I've only noticed the lack of convergence on pycharm, not sure if it happens on colab too\n",
    "# But I don't remember it being an issue on colab\n",
    "\n",
    "if flags['binary_example'] and True:\n",
    "  model_38a = tf.keras.models.clone_model(model_38a)\n",
    "  model_38a.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    # optimizer=\"rmsprop\",\n",
    "                    metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "id": "gfsBKKXg3zLw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0fJaG-d6mhQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  history_38a = model_38a.fit(train_data_augmented_shuffled_2,\n",
    "                              epochs=min(MAX_EPOCHS, 35),  # seeing if it converges\n",
    "                              callbacks=callbacks_stop_and_lrs,\n",
    "                              steps_per_epoch=len(train_data_augmented_shuffled_2),\n",
    "                              validation_data=test_data_2,\n",
    "                              validation_steps=len(test_data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1r1wc7AXdUPf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586716,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  model_38a.save(tfu.paths['dir_model_runs'] + \"model_38a_saved\")\n",
    "  history_38a_df = pd.DataFrame(history_38a.history)\n",
    "  history_38a_df.to_csv(tfu.paths['dir_model_runs'] + \"model_38a_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['binary_example'] and True:\n",
    "  # hack to reset parameters if you don't converge\n",
    "  model_38b = tf.keras.models.clone_model(model_38b)\n",
    "  model_38b.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    # optimizer=\"rmsprop\",\n",
    "                    metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "id": "SO3k_DJO3zLw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586717,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lOZknw96oMb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586717,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  history_38b = model_38b.fit(train_data_augmented_shuffled_2,\n",
    "                              epochs=min(MAX_EPOCHS, 35),  # Good results on my model at 25, use 100?\n",
    "                              callbacks=callbacks_stop_and_lrs,\n",
    "                              steps_per_epoch=len(train_data_augmented_shuffled_2),\n",
    "                              validation_data=test_data_2,\n",
    "                              validation_steps=len(test_data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez-gjRWqdUPg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586717,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  model_38b.save(tfu.paths['dir_model_runs'] + \"model_38b_saved\")\n",
    "  history_38b_df = pd.DataFrame(history_38b.history)\n",
    "  history_38b_df.to_csv(tfu.paths['dir_model_runs'] + \"model_38b_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8nF_6SdNJez"
   },
   "source": [
    "## Load Model Simulations\n",
    "Note class names do not appear to be saved in the model or the history, so you will have to save/load that separately as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4n5MlFJOSZr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586717,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  model_38_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_38_saved\")\n",
    "  model_38a_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_38a_saved\")\n",
    "  model_38b_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_38b_saved\")\n",
    "  history_38_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_38_history.csv\")\n",
    "  history_38a_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_38a_history.csv\")\n",
    "  history_38b_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_38b_history.csv\")\n",
    "\n",
    "  class_names_2_loaded = tfu.dir_class_names(dir_pizza_steak_train).tolist()\n",
    "  print(f'The pizza/steak class names are: \\n\\t{class_names_2_loaded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPriekXFMwAR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586717,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  # Check model_38 architecture\n",
    "  print(f'Class Names: {class_names_2_loaded}')\n",
    "  model_38_loaded.summary()\n",
    "  model_38a_loaded.summary()\n",
    "  model_38b_loaded.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx6bon9BZmwX"
   },
   "source": [
    "## Visualize Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dUbjz3sYUTO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586717,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  tfu.plot_loss_and_accuracy(history_38_loaded)\n",
    "  tfu.plot_loss_and_accuracy(history_38a_loaded)\n",
    "  tfu.plot_loss_and_accuracy(history_38b_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5i_Cq5LAs-Y"
   },
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwzuTreTWVzc"
   },
   "source": [
    "Find Actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vH4dTkzMAvdC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586718,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  print(f'model_38 performance')\n",
    "  model_38_labels, model_38_counts = tfu.generator_labels(dir_pizza_steak_test, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqkWYMX5IKjw"
   },
   "source": [
    "Predict Labels with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20Q7Kqg6WelR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586718,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  # predictions will be probabilities\n",
    "  model_38_probabilities = model_38_loaded.predict(test_data_2)\n",
    "  # change probability to class 0 or 1\n",
    "  model_38_predictions = model_38_probabilities.round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qyDdIAlXvc-"
   },
   "source": [
    "Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LffSMs31X0Pp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586718,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example']:\n",
    "  tfu.make_confusion_matrix(model_38_labels,\n",
    "                            model_38_predictions,\n",
    "                            classes=class_names_2_loaded,\n",
    "                            figsize=(10, 10),\n",
    "                            text_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R91GIaX2poLb"
   },
   "source": [
    "## Baseline (random labels) and actual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyV8sAq0poLe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586718,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example'] and True:\n",
    "  # Determine baseline accuracy associated with random shuffling of the y labels\n",
    "  shuffle_baseline_2 = tfu.random_guess_accuracy(model_38_labels)\n",
    "  print(f'Actual Model Performance')\n",
    "  model_38_loaded.evaluate(test_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6I4QuKZKJTY"
   },
   "source": [
    "## Prediction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjoiY49fPv7a"
   },
   "source": [
    "Plot random images from the testing data for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diIUP3L-dUPg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586718,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example'] and True:\n",
    "  random_images = tfu.random_sample_directory(dir_pizza_steak_pizza_only, 1)\n",
    "  print(random_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ow1sndGldUPg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586718,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example'] and True:\n",
    "  file_names, classes = tfu.random_sample_all_classes(dir_pizza_steak_test, 2)\n",
    "  print(file_names, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw1AXSP_MbHF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586718,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['binary_example'] and True:\n",
    "  random_images = tfu.random_sample_directory(dir_pizza_steak_steak_only, 1)\n",
    "  tfu.pred_and_plot2(model_38_loaded, random_images, class_names_2_loaded,\n",
    "                     img_shape=224, actual_class_names='steak')\n",
    "\n",
    "  random_images = tfu.random_sample_directory(dir_pizza_steak_pizza_only, 1)\n",
    "  tfu.pred_and_plot2(model_38_loaded, random_images, class_names_2_loaded,\n",
    "                     img_shape=224, actual_class_names='pizza')\n",
    "\n",
    "  file_names, classes = tfu.random_sample_all_classes(dir_pizza_steak_test, 2)\n",
    "  tfu.pred_and_plot2(model_38_loaded, file_names, class_names_2_loaded,\n",
    "                     img_shape=224, actual_class_names=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgdnUDLmHPcw"
   },
   "source": [
    "# Image Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHQOQIXIeRxp"
   },
   "source": [
    "## Download & Inspect Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SViyhQ8JgWD2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586719,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # _10 means 10 class dataset (10 food types)\n",
    "  tfu.url_to_local_dir(url_food_10_class_all, dir_images_local)\n",
    "\n",
    "  # Find the labels and class counts for the test data\n",
    "  y_labels, _counts = tfu.generator_labels(dir_food_10_class_all_test, class_mode='categorical')\n",
    "\n",
    "  # Find baseline accuracy using test labels (faster if you already have y_labels)\n",
    "  tfu.random_guess_accuracy(y_labels)\n",
    "\n",
    "  class_names_food_10 = tfu.dir_class_names(dir_food_10_class_all_test)\n",
    "  print(f'Multiclass classification class names: {class_names_food_10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwrRcpMGgL0b"
   },
   "source": [
    "List file counts in each directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIOJzSNuJRJe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586719,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  tfu.walk_directory(dir_food_10_class_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yihtxaODgR5F"
   },
   "source": [
    "Plot random images associated with each classification type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tw0WgkasJRJe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586719,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # Plot random examples of input files for each class\n",
    "  tfu.plot_all_classes(dir_food_10_class_all_test, number_files=2, figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx2qHzVJJsSE"
   },
   "source": [
    "## Create ImageDataGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTxsdxE5JsSL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586719,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # Create ImageDataGenerator training instance without data augmentation\n",
    "  train_datagen_10 = ImageDataGenerator(rescale=1 / 255.)\n",
    "\n",
    "  # Create ImageDataGenerator training instance with data augmentation\n",
    "  train_datagen_augmented_10 = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                                  rotation_range=20,\n",
    "                                                  # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
    "                                                  shear_range=0.2,  # shear the image\n",
    "                                                  zoom_range=0.2,  # zoom into the image\n",
    "                                                  width_shift_range=0.2,  # shift the image width ways\n",
    "                                                  height_shift_range=0.2,  # shift the image height ways\n",
    "                                                  horizontal_flip=True)  # flip the image on the horizontal axis\n",
    "\n",
    "  # Create ImageDataGenerator test instance without data augmentation\n",
    "  test_datagen_10 = ImageDataGenerator(rescale=1 / 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwQQ52-hJsSM"
   },
   "source": [
    "## flow_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUC7wAESJsSM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586719,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # Import data and augment it from directories\n",
    "  print(\"training images (no-augmentation) with shuffle on:\")\n",
    "  train_data_shuffled_10 = train_datagen_10.flow_from_directory(dir_food_10_class_all_train,\n",
    "                                                                target_size=(224, 224),\n",
    "                                                                batch_size=32,\n",
    "                                                                class_mode='categorical',\n",
    "                                                                shuffle=True)  # Shuffle data (default)\n",
    "\n",
    "  print(\"Augmented training images with shuffle on:\")\n",
    "  train_data_augmented_shuffled_10 = train_datagen_augmented_10.flow_from_directory(dir_food_10_class_all_train,\n",
    "                                                                                    target_size=(224, 224),\n",
    "                                                                                    batch_size=32,\n",
    "                                                                                    class_mode='categorical',\n",
    "                                                                                    shuffle=True)  # Shuffle data (default)\n",
    "  # Don't shuffle the test images to allow for easy y_label extraction\n",
    "  print(\"Unchanged test images:\")\n",
    "  test_img_gen_10 = test_datagen_10.flow_from_directory(dir_food_10_class_all_test,\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        shuffle=False)\n",
    "\n",
    "  # Optional: create a dictionary with index number as the key and label as the value\n",
    "  # class_names_11_dict = {value: key for key, value in test_data_10.class_indices.items()}\n",
    "  # class_names_11_df = pd.DataFrame.from_dict(class_names_11_dict, orient='index')\n",
    "  # class_names_11_df.to_csv(model_runs + \"model_11_class_names.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-bWXZn9jgtW"
   },
   "source": [
    "Find number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yArslGN2jjdu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586719,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  print(test_img_gen_10.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcfGd8-K_wVX"
   },
   "source": [
    "## Define and Fit Model\n",
    "model_40a does not always converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Also consider this model geometry\n",
    "if False:\n",
    "  model_name_01 = \"simple_conv2d\"\n",
    "  inputs = layers.Input(shape=(32, 32, 3))  # inputs are 1-dimensional strings\n",
    "\n",
    "  x = layers.Conv2D(32, (3, 3), activation='relu') (inputs)\n",
    "  x = layers.MaxPooling2D((2, 2))(x)\n",
    "  x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "  x = layers.MaxPooling2D((2, 2))(x)\n",
    "  x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "  outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "\n",
    "  model_01 = keras.Model(inputs, outputs, name=model_name_01)\n",
    "  model_01.compile(optimizer=\"adam\",\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "  model_01.summary()"
   ],
   "metadata": {
    "id": "aw6NSDjZCT6b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586719,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd1PbjVfKY-e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586721,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  model_40 = Sequential([\n",
    "    Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    Conv2D(10, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')  # changed to have 10 neurons (same as number of classes) and 'softmax' activation\n",
    "  ])\n",
    "\n",
    "  # Enhanced model 40 trying to get better performance\n",
    "  model_40a = Sequential([\n",
    "    Conv2D(16, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "    Conv2D(16, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(16, 3, activation='relu'),\n",
    "    Conv2D(16, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    Conv2D(32, 3, activation='relu'),\n",
    "    Conv2D(32, 3, activation='relu'),\n",
    "    MaxPool2D(),\n",
    "    layers.Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # changed to have 10 neurons (same as number of classes) and 'softmax' activation\n",
    "  ])\n",
    "\n",
    "  # Compile the model\n",
    "  model_40.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "  # Compile the model\n",
    "  model_40a.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxH6p8EgEUI5"
   },
   "source": [
    "Fit the models\n",
    "Note: I had some weird convergence problems that went away when I cleared the model runs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "early_stopping_02 = [keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                patience=7)]"
   ],
   "metadata": {
    "id": "y27JklxF3zME",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586721,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMFuwbXtEPxJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586721,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example'] and True:\n",
    "  history_40 = model_40.fit(train_data_augmented_shuffled_10,\n",
    "                            epochs=min(MAX_EPOCHS, 50),  # I think it originally was 5\n",
    "                            callbacks=early_stopping_02,\n",
    "                            validation_data=test_img_gen_10,\n",
    "                            # steps_per_epoch=len(train_data_augmented_shuffled_10),\n",
    "                            # validation_steps=len(test_data_10),\n",
    "                            )\n",
    "  model_40.save(tfu.paths['dir_model_runs'] + \"model_40_saved\")\n",
    "  history_40_df = pd.DataFrame(history_40.history)\n",
    "  history_40_df.to_csv(tfu.paths['dir_model_runs'] + \"model_40_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBZ1I6s6EV4J",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586721,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example'] and True:\n",
    "  history_40a = model_40a.fit(train_data_augmented_shuffled_10,\n",
    "                              epochs=min(MAX_EPOCHS, 50),  # Good results on my model at 25, use 35 or 100\n",
    "                              callbacks=early_stopping_02,\n",
    "                              steps_per_epoch=len(train_data_augmented_shuffled_10),\n",
    "                              validation_data=test_img_gen_10,\n",
    "                              validation_steps=len(test_img_gen_10))\n",
    "  model_40a.save(tfu.paths['dir_model_runs'] + \"model_40a_saved\")\n",
    "  history_40a_df = pd.DataFrame(history_40a.history)\n",
    "  history_40a_df.to_csv(tfu.paths['dir_model_runs'] + \"model_40a_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jCvGuCJKY-l"
   },
   "source": [
    "## Load Model Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7BU8NrQKY-m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586721,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  model_40_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_40_saved\")\n",
    "  model_40a_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_40a_saved\")\n",
    "  history_40_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_40_history.csv\")\n",
    "  history_40a_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_40a_history.csv\")\n",
    "\n",
    "  # with open(tfu.paths['dir_model_runs'] + \"class_names_10.json\", \"r\") as read_file:\n",
    "  #   class_names_10_loaded = json.load(read_file)\n",
    "\n",
    "  class_names_10_loaded = tfu.dir_class_names(dir_food_10_class_all_test).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLOMUsskKY-m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586721,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # Check model_40 architecture\n",
    "  print(f'Class Names: {class_names_10_loaded}')\n",
    "  model_40_loaded.summary()\n",
    "  model_40a_loaded.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2bnln4HKY-m"
   },
   "source": [
    "## Visualize Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jYQWEK5KY-m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586722,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  tfu.plot_loss_and_accuracy(history_40_loaded)\n",
    "  tfu.plot_loss_and_accuracy(history_40a_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAEUxkXbe210"
   },
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg7mt_73e211"
   },
   "source": [
    "Find Actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDhiZLEFe211",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586722,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  print(f'model_40 performance')\n",
    "  model_40_labels, model_40_counts = tfu.generator_labels(dir_food_10_class_all_test,\n",
    "                                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w7S9wUke211"
   },
   "source": [
    "Predict Labels with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPVx9LoJe212",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586722,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # predictions will be probabilities\n",
    "  model_40_predictions_one_hot = model_40_loaded.predict(test_img_gen_10)\n",
    "  # change probability to index predictions\n",
    "  model_40_predictions = model_40_predictions_one_hot.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGMA1ogUe212"
   },
   "source": [
    "Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccev2AM_e212",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586722,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  tfu.make_confusion_matrix(model_40_labels, model_40_predictions, classes=class_names_10_loaded, figsize=(15, 15), text_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJj7ryzcreNb"
   },
   "source": [
    "## Baseline (random labels) and actual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kuisLy5reNb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586722,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # Determine baseline accuracy associated with random shuffling of the y labels\n",
    "  shuffle_baseline_2 = tfu.random_guess_accuracy(model_40_labels)\n",
    "  print(f'Actual Model Performance')\n",
    "  model_40_loaded.evaluate(test_img_gen_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWWlFp4FsF7x"
   },
   "source": [
    "## Prediction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1ismi_CsF7x"
   },
   "source": [
    "Plot random images from the testing data for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvk3zXA5AAjW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586722,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  # Random sample of images to spot check performance\n",
    "  file_names, classes = tfu.random_sample_all_classes(dir_food_10_class_all_test, 2)\n",
    "  tfu.pred_and_plot2(model_40_loaded, file_names, class_names_10_loaded,\n",
    "                     img_shape=224, actual_class_names=classes)\n",
    "\n",
    "  # Udemy image examples\n",
    "  example_files = [pizza_dad_03, steak_03, hamburger_03, sushi_03]\n",
    "  example_classes = ['pizza', 'steak', 'hamburger', 'sushi']\n",
    "  tfu.pred_and_plot2(model_40_loaded, example_files, class_names_10_loaded,\n",
    "                     img_shape=224, actual_class_names=example_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "SmjOFsXo3zMF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586722,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Redo using datasets\n",
    "\n",
    "Use just the training data and split off 20% for testing"
   ],
   "metadata": {
    "collapsed": false,
    "id": "F5T1zm-j3zMF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "  batch_size = 32\n",
    "  img_height = 224\n",
    "  img_width = 224\n",
    "  rescale = 255.\n",
    "  validation_split = 0.2\n",
    "\n",
    "  ds_train = image_dataset_from_directory(\n",
    "    dir_food_10_class_all_train,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    subset='training',\n",
    "    seed=42,\n",
    "    label_mode='categorical'\n",
    "  )\n",
    "\n",
    "  ds_test = image_dataset_from_directory(\n",
    "    dir_food_10_class_all_train,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation',\n",
    "    seed=42,\n",
    "    label_mode='categorical'\n",
    "  )\n",
    "\n",
    "  num_classes = len(ds_train.class_names)\n",
    "\n",
    "  data_augmentation = keras.Sequential(\n",
    "    [\n",
    "      layers.RandomFlip(\"horizontal\"),\n",
    "      layers.RandomTranslation(height_factor=0.25, width_factor=0.25),\n",
    "      layers.RandomRotation(0.2),\n",
    "      layers.RandomZoom(0.2),\n",
    "      # layers.RandomContrast(0.2),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  inputs_01 = keras.Input(shape=(img_height, img_width, 3))\n",
    "  x = data_augmentation(inputs_01)  # Put the Augmentation before the hidden layers\n",
    "  x = layers.Rescaling(1. / rescale)(x)\n",
    "  x = layers.Conv2D(filters=16, kernel_size=3, activation=\"relu\")(x)\n",
    "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "  x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "  x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dropout(0.2)(x)\n",
    "  x = layers.Dense(128, activation=\"relu\")(x)\n",
    "  outputs_01 = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "  model_01 = keras.Model(inputs_01, outputs_01)\n",
    "\n",
    "  model_01.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "id": "YE12Tj7S3zMF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['multiclass_example']:\n",
    "    history_01 = model_01.fit(ds_train,\n",
    "                              validation_data=ds_test,\n",
    "                              epochs=min(MAX_EPOCHS, 50))\n",
    "    tfu.plot_loss_and_accuracy(history_01)"
   ],
   "metadata": {
    "id": "i7fyFmEm3zMF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "loxj2MZDJRJf"
   },
   "source": [
    "# Famous CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "49AT92zDJRJf"
   },
   "source": [
    "## Layer Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZSuWqNHJRJf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "vgg_Conv2D = partial(tf.keras.layers.Conv2D,\n",
    "                     filters=10,\n",
    "                     kernel_size=3,\n",
    "                     activation=\"relu\",\n",
    "                     padding=\"same\",\n",
    "                     )\n",
    "vgg_MaxPool2D = partial(tf.keras.layers.MaxPool2D,\n",
    "                        pool_size=2,\n",
    "                        strides=2,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9wIDGoUOJRJf"
   },
   "source": [
    "## Tiny VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "66fpOyzjJRJf"
   },
   "source": [
    "#### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2L6ozBHJRJf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  image_size = 224\n",
    "  batch_size = 32\n",
    "\n",
    "  model_tiny_vgg = tf.keras.models.Sequential([\n",
    "    vgg_Conv2D(),\n",
    "    vgg_Conv2D(),\n",
    "    vgg_MaxPool2D(strides=1),\n",
    "    vgg_Conv2D(),\n",
    "    vgg_Conv2D(),\n",
    "    vgg_MaxPool2D(strides=1),\n",
    "    tf.keras.layers.Flatten(),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBjnpd2lJRJf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  # with tf.device('/CPU:0'):   # required by mac m1 bare-metal to work properly\n",
    "  augmentation_60 = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomTranslation(height_factor=0.15, width_factor=0.15),\n",
    "    layers.RandomContrast(0.15),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-MFo7s9JRJf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  # image_dataset_from_directory - no augmentation\n",
    "  model_60 = Sequential([\n",
    "    keras.Input(shape=(image_size, image_size, 3)),\n",
    "    layers.Rescaling(1 / 255.),\n",
    "    tf.keras.models.clone_model(model_tiny_vgg),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),  # binary activation output\n",
    "  ])\n",
    "\n",
    "  # image_dataset_from_directory - with augmentation\n",
    "  model_61 = Sequential([\n",
    "    keras.Input(shape=(image_size, image_size, 3)),\n",
    "    augmentation_60,\n",
    "    layers.Rescaling(1 / 255.),\n",
    "    tf.keras.models.clone_model(model_tiny_vgg),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),  # binary activation output\n",
    "  ])\n",
    "\n",
    "  # ImageDataGenerator - no augmentation\n",
    "  model_62 = Sequential([\n",
    "    keras.Input(shape=(image_size, image_size, 3)),\n",
    "    tf.keras.models.clone_model(model_tiny_vgg),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),  # binary activation output\n",
    "  ])\n",
    "\n",
    "  # ImageDataGenerator - with augmentation\n",
    "  model_63 = Sequential([\n",
    "    keras.Input(shape=(image_size, image_size, 3)),\n",
    "    tf.keras.models.clone_model(model_tiny_vgg),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),  # binary activation output\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NWj_UfBRJRJf"
   },
   "source": [
    "Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Swf5ymzuJRJf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  model_60.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "  model_61.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "  model_62.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "  model_63.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FfaxlAjnJRJf"
   },
   "source": [
    "#### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8PjNQfoJRJf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  # Specify training and testing for the binary classification directories\n",
    "  train_data_set = image_dataset_from_directory(dir_pizza_steak_train,\n",
    "                                                image_size=(image_size, image_size),\n",
    "                                                batch_size=batch_size)\n",
    "  test_data_set = image_dataset_from_directory(dir_pizza_steak_test,\n",
    "                                               image_size=(image_size, image_size),\n",
    "                                               batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "s-HBBcE5eeUJ"
   },
   "source": [
    "#### Create ImageDataGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9WEgwwkeeUJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586723,
     "user_tz": 420,
     "elapsed": 30,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  # Create ImageDataGenerator training instance without data augmentation\n",
    "  train_image_gen = ImageDataGenerator(rescale=1 / 255.)\n",
    "\n",
    "  # Create ImageDataGenerator training instance with data augmentation\n",
    "  train_image_gen_aug = ImageDataGenerator(rescale=1 / 255.,\n",
    "                                           rotation_range=20,\n",
    "                                           # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
    "                                           shear_range=0.1,  # shear the image\n",
    "                                           zoom_range=0.1,  # zoom into the image\n",
    "                                           width_shift_range=0.15,  # shift the image width ways\n",
    "                                           height_shift_range=0.15,  # shift the image height ways\n",
    "                                           horizontal_flip=True)  # flip the image on the horizontal axis\n",
    "\n",
    "  # Create ImageDataGenerator test instance without data augmentation\n",
    "  test_image_gen = ImageDataGenerator(rescale=1 / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSCTcnGOeeUJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  print(\"Non-augmented training images:\")\n",
    "  train_set = train_image_gen.flow_from_directory(dir_pizza_steak_train,\n",
    "                                                  target_size=(image_size, image_size),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='binary',\n",
    "                                                  )\n",
    "\n",
    "  print(\"Augmented training images:\")\n",
    "  train_set_aug = train_image_gen_aug.flow_from_directory(dir_pizza_steak_train,\n",
    "                                                          target_size=(image_size, image_size),\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          class_mode='binary',\n",
    "                                                          )\n",
    "\n",
    "  print(\"Training images with shuffle off:\")\n",
    "  test_set = test_image_gen.flow_from_directory(dir_pizza_steak_test,\n",
    "                                                target_size=(image_size, image_size),\n",
    "                                                batch_size=batch_size,\n",
    "                                                class_mode='binary',\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au8EZeA6eeUJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  history_60 = model_60.fit(train_data_set,\n",
    "                            epochs=min(MAX_EPOCHS, 10),\n",
    "                            validation_data=test_data_set,\n",
    "                            )\n",
    "  tfu.plot_loss_and_accuracy(history_60, 'Dataset - No Augment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkxvMGvUeeUJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  history_61 = model_61.fit(train_data_set,\n",
    "                            epochs=min(MAX_EPOCHS, 10),\n",
    "                            validation_data=test_data_set,\n",
    "                            )\n",
    "  tfu.plot_loss_and_accuracy(history_61, 'Dataset - With Augment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZLHbkzseeUJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  history_62 = model_62.fit(train_set,\n",
    "                            epochs=min(MAX_EPOCHS, 10),\n",
    "                            validation_data=test_set,\n",
    "                            )\n",
    "  tfu.plot_loss_and_accuracy(history_62, 'ImageGen - No Augment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISeKjmYMeeUJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  history_63 = model_63.fit(train_set_aug,\n",
    "                            epochs=min(MAX_EPOCHS, 10),\n",
    "                            validation_data=test_set,\n",
    "                            )\n",
    "  tfu.plot_loss_and_accuracy(history_63, 'ImageGen - With Augment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "AlmFU-L6JRJf"
   },
   "source": [
    "## VGG\n",
    "Define the VGG16 model as sequential model:\n",
    "*  Input Shape (224, 224, 3)\n",
    "*  All Conv layers have a 3x3 kernel and same padding\n",
    "*  All MaxPool layers are 2x2 pool size and stride 2x2\n",
    "\n",
    "*  2 x convolution layers with 64 filters\n",
    "*  1 x maxpool layer\n",
    "*  2 x convolution layers with 128 filters\n",
    "*  1 x maxpool layer\n",
    "*  3 x convolution layers with 256 filters\n",
    "*  1 x maxpool layer\n",
    "*  3 x convolution layers with 512 filters\n",
    "*  1 x maxpool layer\n",
    "*  3 x convolution layers with 512 filters\n",
    "*  1 x maxpool layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiMHjSCHJRJf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 30,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tiny_vgg']:\n",
    "  model_vgg = tf.keras.models.Sequential([\n",
    "    vgg_Conv2D(filters=64, input_shape=(224, 224, 3)),\n",
    "    vgg_Conv2D(filters=64, ),\n",
    "    vgg_MaxPool2D(),\n",
    "    vgg_Conv2D(filters=128, ),\n",
    "    vgg_Conv2D(filters=128, ),\n",
    "    vgg_MaxPool2D(),\n",
    "    vgg_Conv2D(filters=256, ),\n",
    "    vgg_Conv2D(filters=256, ),\n",
    "    vgg_Conv2D(filters=256, ),\n",
    "    vgg_MaxPool2D(),\n",
    "    vgg_Conv2D(filters=512, ),\n",
    "    vgg_Conv2D(filters=512, ),\n",
    "    vgg_Conv2D(filters=512, ),\n",
    "    vgg_MaxPool2D(),\n",
    "    vgg_Conv2D(filters=512, ),\n",
    "    vgg_Conv2D(filters=512, ),\n",
    "    vgg_Conv2D(filters=512, ),\n",
    "    vgg_MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),  # binary activation output\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),  # binary activation output\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")  # binary activation output\n",
    "  ])\n",
    "\n",
    "  # Compile the model\n",
    "  model_vgg.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbixC1mqfXDh"
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "See google tutorial vision 03 at bottom for modeling from scratch\n",
    "-------------------------------------------------------------------\n",
    "*\n",
    "* You can keep the whole conv base if the training data looks like your data, or you can just keep a few of the first layers that are the most generic.\n",
    "* Either method requires your own trainable classifier on the end.\n",
    "* Two common ways to use a pre-trained model:\n",
    "  * hub.KerasLayer (https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer)\n",
    "    * Use TensorFlow Hub to find a suitable model and save its url\n",
    "  * keras.applications.<model name>\n",
    "    * Example: keras.applications.vgg16.VGG16(...)\n",
    "    * You must use the conv_base's preprocessing unit to get images in the right shape\n",
    "\n",
    "**Feature Extraction**\n",
    "  * All layers in the convolutional base are un-trainable (conv_base.trainable = False)\n",
    "\n",
    "**Fine-tuning**\n",
    "* Last few layers of convolutional base are un-trainable (conv_base.trainable = True)\n",
    "* Likely want to use a small training rate to avoid corrupting trainable section of the conv_base\n",
    "\n",
    "Order of Layers\n",
    "* Input with Shape\n",
    "* Augmentation\n",
    "* conv_base preprocessing (in lieu of your own scaling)\n",
    "* conv_base\n",
    "  * Make sure to invoke with training=False to avoid batch normalization issues\n",
    "* Flatten\n",
    "* Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnPq7WO7KY3X"
   },
   "source": [
    "## Training Dataset\n",
    "Train on 10% of available training data and all testing data\n",
    "\n",
    "Can use either a Imagedata generator or a dataset\n",
    "\n",
    "Notes:\n",
    "* Put the rescale in the model rather than the imagedata so that you can use either datasource type (datasets don't have a scaling parameter)\n",
    "* Wrapper around tensorflow hub layer has a slightly different calling pattern in a functional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm07B9laJKPK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 30,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['transfer']:\n",
    "  tfu.url_to_local_dir(url_food_10_class_10, dir_images_local)\n",
    "  print(f\"Training directory 100% train 100% test: {dir_food_10_class_all_train}\")\n",
    "  print(f\"Testing directory  100% train 100% test: {dir_food_10_class_all_test}\")\n",
    "  print(f\"Training directory  10% train 100% test: {dir_food_10_class_10_train}\")\n",
    "  print(f\"Testing directory   10% train 100% test: {dir_food_10_class_10_test}\")\n",
    "  print(f\"Training directory   1% train 100% test: {dir_food_10_class_1_train}\")\n",
    "  print(f\"Testing directory    1% train 100% test: {dir_food_10_class_1_test}\")\n",
    "  print(f\"Model Save Directory: {tfu.paths['dir_model_runs']}\")\n",
    "  print(f\"TensorBoard Directory: {tfu.paths['dir_tensor_board']}\")\n",
    "  IMAGE_SIZE = 224\n",
    "  BATCH_SIZE = 32\n",
    "\n",
    "  class_names_10 = tfu.dir_class_names(dir_food_10_class_10_train).tolist()\n",
    "  num_classes = len(class_names_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_4J69Zn35yvz"
   },
   "source": [
    "ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51BygrXb5yvz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586724,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['transfer']:\n",
    "  # Train on 1% or 10% of available training data and all training data\n",
    "  # Use a layer to rescale rather than the ImageDataGenerator, so you can use the same model for a dataset too\n",
    "  train_1_percent = ImageDataGenerator(rotation_range=20,\n",
    "                                       # rotate the image slightly between 0 and 20 degrees\n",
    "                                       # (note: this is an int not a float)\n",
    "                                       zoom_range=0.2,  # zoom into the image\n",
    "                                       width_shift_range=0.2,  # shift the image width ways\n",
    "                                       height_shift_range=0.2,  # shift the image height ways\n",
    "                                       horizontal_flip=True,  # flip the image on the horizontal axis\n",
    "                                       )\n",
    "  train_10_percent = ImageDataGenerator(rotation_range=20,\n",
    "                                        # rotate the image slightly between 0 and 20 degrees\n",
    "                                        # (note: this is an int not a float)\n",
    "                                        zoom_range=0.2,  # zoom into the image\n",
    "                                        width_shift_range=0.2,  # shift the image width ways\n",
    "                                        height_shift_range=0.2,  # shift the image height ways\n",
    "                                        horizontal_flip=True,  # flip the image on the horizontal axis\n",
    "                                        )\n",
    "  train_100_percent = ImageDataGenerator(rotation_range=20,\n",
    "                                         # rotate the image slightly between 0 and 20 degrees\n",
    "                                         # (note: this is an int not a float)\n",
    "                                         zoom_range=0.2,  # zoom into the image\n",
    "                                         width_shift_range=0.2,  # shift the image width ways\n",
    "                                         height_shift_range=0.2,  # shift the image height ways\n",
    "                                         horizontal_flip=True,  # flip the image on the horizontal axis\n",
    "                                         )\n",
    "\n",
    "  test_1_percent = ImageDataGenerator()\n",
    "  test_10_percent = ImageDataGenerator()\n",
    "  test_100_percent = ImageDataGenerator()\n",
    "\n",
    "  train_img_gen_1 = train_1_percent.flow_from_directory(dir_food_10_class_1_train,\n",
    "                                                        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        shuffle=True,\n",
    "                                                        )\n",
    "  train_img_gen_10 = train_10_percent.flow_from_directory(dir_food_10_class_10_train,\n",
    "                                                          target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                          batch_size=BATCH_SIZE,\n",
    "                                                          class_mode='categorical',\n",
    "                                                          shuffle=True,\n",
    "                                                          )\n",
    "  train_img_gen_100 = train_10_percent.flow_from_directory(dir_food_10_class_all_train,\n",
    "                                                           target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                           batch_size=BATCH_SIZE,\n",
    "                                                           class_mode='categorical',\n",
    "                                                           shuffle=True,\n",
    "                                                           )\n",
    "\n",
    "  test_img_gen_1 = test_1_percent.flow_from_directory(dir_food_10_class_1_test,\n",
    "                                                      target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                      batch_size=BATCH_SIZE,\n",
    "                                                      class_mode='categorical',\n",
    "                                                      shuffle=False)  # Allows for easier creation of conf matrix\n",
    "\n",
    "  test_img_gen_10 = test_10_percent.flow_from_directory(dir_food_10_class_10_test,\n",
    "                                                        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        shuffle=False)  # Allows for easier creation of conf matrix\n",
    "  test_img_gen_100 = test_10_percent.flow_from_directory(dir_food_10_class_all_test,\n",
    "                                                         target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode='categorical',\n",
    "                                                         shuffle=False)  # Allows for easier creation of conf matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4iqMXjYt5yv0"
   },
   "source": [
    "Datasets using image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fDkItjD5yv0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 30,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['transfer']:\n",
    "  train_data_set_1 = image_dataset_from_directory(\n",
    "    dir_food_10_class_1_train,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "  )\n",
    "\n",
    "  train_data_set_10 = image_dataset_from_directory(\n",
    "    dir_food_10_class_10_train,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "  )\n",
    "\n",
    "  train_data_set_100 = image_dataset_from_directory(\n",
    "    dir_food_10_class_all_train,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "  )\n",
    "\n",
    "  test_data_set_1 = image_dataset_from_directory(\n",
    "    dir_food_10_class_1_test,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "  )\n",
    "  test_data_set_10 = image_dataset_from_directory(\n",
    "    dir_food_10_class_10_test,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "  )\n",
    "  test_data_set_100 = image_dataset_from_directory(\n",
    "    dir_food_10_class_all_test,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fbO0xpOO5yv0"
   },
   "source": [
    "## TensorFlow Hub\n",
    "\n",
    "May require install:\n",
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YscbAwdxdUPk"
   },
   "source": [
    "### Download Base from Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ],
   "metadata": {
    "id": "-zZRHgNl3zMH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNsTAWrVAJzM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  # Original: EfficientNetB0 feature vector (version 1)\n",
    "  efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
    "  resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "  IMAGE_SIZE = 224\n",
    "  BATCH_SIZE = 32\n",
    "\n",
    "  # Download the pretrained model and save it as a Keras layer\n",
    "  efficient_net = hub.KerasLayer(efficientnet_url,\n",
    "                                 trainable=False,  # freeze the underlying patterns\n",
    "                                 name='feature_extraction_layer',\n",
    "                                 input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))  # define the input image shape\n",
    "\n",
    "  res_net = hub.KerasLayer(resnet_url,\n",
    "                           trainable=False,  # freeze the underlying patterns\n",
    "                           name='feature_extraction_layer',\n",
    "                           input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))  # define the input image shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wphOLVKSdUPk"
   },
   "source": [
    "### Model Design\n",
    "Model 52 is efficient_net and 53 is res_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSAH_pibdUPk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  class_names_10 = tfu.dir_class_names(dir_food_10_class_10_train).tolist()\n",
    "  num_classes = len(class_names_10)\n",
    "  inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "  x = layers.Rescaling(1 / 255.)(inputs)\n",
    "  x = efficient_net(x, training=False)  # Note the call to the prebuilt does not have the same (x) ending structure\n",
    "  x = layers.Dense(32)(x)\n",
    "  outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(x)\n",
    "  model_52 = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fwHzZKi5yv0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub'] and False:\n",
    "  # Use datasets instead of imagegen\n",
    "  # model_52a = tf.keras.models.clone_model(model_52)\n",
    "  inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "  x = layers.Rescaling(1 / 255.)(inputs)\n",
    "  x = efficient_net(x, training=False)  # Note the call to the prebuilt does not have the same (x) ending structure\n",
    "  x = layers.Dense(32)(x)\n",
    "  outputs = layers.Dense(num_classes, activation='softmax', name='output_layer')(x)\n",
    "  model_52a = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  model_52a.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "  model_52a_callbacks = [\n",
    "    # Stop after val_accuracy does not improve for patience epochs\n",
    "    keras.callbacks.EarlyStopping(\n",
    "      monitor=\"val_accuracy\",\n",
    "      patience=4,\n",
    "    ),\n",
    "    # Save to tensorboard\n",
    "    keras.callbacks.TensorBoard(log_dir=tfu.paths['dir_tensor_board'] + \"model_52a\"),\n",
    "  ]\n",
    "\n",
    "  history_52a = model_52a.fit(train_data_set_10,\n",
    "                              epochs=min(MAX_EPOCHS, 15),  # 15 is good for proof of concept\n",
    "                              validation_data=test_data_set_10,\n",
    "                              callbacks=model_52a_callbacks,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Gz_rSfB5yv0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  inputs2 = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "  x = layers.Rescaling(1 / 255.)(inputs2)\n",
    "  x = res_net(x, training=False)\n",
    "  x = layers.Dense(32)(x)\n",
    "  outputs2 = layers.Dense(num_classes, activation='softmax', name='output_layer')(x)\n",
    "  model_53 = keras.Model(inputs=inputs2, outputs=outputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CCqTPLFUdUPk"
   },
   "source": [
    "### Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2UhE3rVdUPk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  model_52.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=['accuracy'])\n",
    "  model_53.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=tf.keras.optimizers.Adam(),\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "a9WFwT8odUPk"
   },
   "source": [
    "### Model Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PP888KR0L7TN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586725,
     "user_tz": 420,
     "elapsed": 28,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  model_52_callbacks = [\n",
    "    # Stop after val_accuracy does not improve for patience epochs\n",
    "    keras.callbacks.EarlyStopping(\n",
    "      monitor=\"val_accuracy\",\n",
    "      patience=4,\n",
    "    ),\n",
    "    # Save model to filepath, only write over the last save if val_loss improved\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + \"model_52_checkpoint\",\n",
    "      monitor=\"val_loss\",\n",
    "      save_best_only=True,\n",
    "    ),\n",
    "    # Save to tensorboard\n",
    "    keras.callbacks.TensorBoard(log_dir=tfu.paths['dir_tensor_board'] + \"model_52\"),\n",
    "  ]\n",
    "\n",
    "  model_53_callbacks = [\n",
    "    # Stop after val_accuracy does not improve for patience epochs\n",
    "    keras.callbacks.EarlyStopping(\n",
    "      monitor=\"val_accuracy\",\n",
    "      patience=4,\n",
    "    ),\n",
    "    # Save model to filepath, only write over the last save if val_loss improved\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + \"model_53_checkpoint\",\n",
    "      monitor=\"val_loss\",\n",
    "      save_best_only=True,\n",
    "    ),\n",
    "    # Save to tensorboard\n",
    "    keras.callbacks.TensorBoard(log_dir=tfu.paths['dir_tensor_board'] + \"model_53\"),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MoknVn4qdUPk"
   },
   "source": [
    "### Fitting\n",
    "\n",
    "can use the datasets (train_data_set_10, test_data_set_10) or the\n",
    "imagegen (train_img_gen, test_img_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0wdDlJI5yv0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586726,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "if flags['tensorflow_hub']:\n",
    "  history_52 = model_52.fit(train_img_gen_10,\n",
    "                            epochs=min(MAX_EPOCHS, 15),  # 15 is good for proof of concept\n",
    "                            validation_data=test_img_gen_10,\n",
    "                            callbacks=model_52_callbacks,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMAXS75W5yv0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586726,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "if flags['tensorflow_hub']:\n",
    "  history_52 = model_52.fit(train_data_set_10,\n",
    "                            epochs=min(MAX_EPOCHS, 15),  # 15 is good for proof of concept\n",
    "                            validation_data=test_data_set_10,\n",
    "                            callbacks=model_52_callbacks,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lx-LdkNjKCqK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276586726,
     "user_tz": 420,
     "elapsed": 29,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "if flags['tensorflow_hub']:\n",
    "  history_53 = model_53.fit(train_img_gen_10,\n",
    "                            epochs=min(MAX_EPOCHS, 15),  # 15 is good for proof of concept\n",
    "                            validation_data=test_img_gen_10,\n",
    "                            callbacks=model_53_callbacks,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OwMlp9pFdUPl"
   },
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSWLokmPdUPl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587077,
     "user_tz": 420,
     "elapsed": 380,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  model_52.save(tfu.paths['dir_model_runs'] + \"model_52_saved\")\n",
    "  history_52_df = pd.DataFrame(history_52.history)\n",
    "  history_52_df.to_csv(tfu.paths['dir_model_runs'] + \"model_52_history.csv\", index=False)\n",
    "  # Save class names to json files\n",
    "  with open(tfu.paths['dir_model_runs'] + \"class_names_10_percent.json\", \"w\") as write_file:\n",
    "    json.dump(class_names_10, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYYVoQA9euwO"
   },
   "source": [
    "### Load Model Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjDhGvRh5yv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587077,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  model_52_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_52_saved\")\n",
    "  history_52_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_52_history.csv\")\n",
    "  with open(tfu.paths['dir_model_runs'] + \"class_names_10_percent.json\", \"r\") as read_file:\n",
    "    class_names_10 = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "axP9xY125yv1"
   },
   "source": [
    "### Visualize Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GBmUIOt5yv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587077,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  tfu.plot_loss_and_accuracy(history_52_loaded, title='EfficientNet')\n",
    "  tfu.plot_loss_and_accuracy(history_53, title='ResNet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ieL1sPV-bQx"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rne4l0ey-bQx"
   },
   "source": [
    "Find Actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9xK2cGQ-bQy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587077,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  print(f'model 52 performance')\n",
    "  model_52_labels, model_52_counts = tfu.generator_labels(dir_food_10_class_10_test,\n",
    "                                                          class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v0k_m7_-bQy"
   },
   "source": [
    "Predict Labels with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_n2GOiI7-bQy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587077,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  # predictions will be probabilities\n",
    "  model_52_predictions_one_hot = model_52_loaded.predict(test_img_gen_10)\n",
    "  # change probability to index predictions\n",
    "  model_52_predictions = model_52_predictions_one_hot.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrHuqPqF-bQy"
   },
   "source": [
    "Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Adn9hjAC-bQy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587077,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  tfu.make_confusion_matrix(model_52_labels,\n",
    "                            model_52_predictions,\n",
    "                            classes=class_names_10,\n",
    "                            figsize=(15, 15),\n",
    "                            text_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QQHb850-bQz"
   },
   "source": [
    "### Baseline (random labels) and actual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4N4yBsF-bQz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  # Determine baseline accuracy associated with random shuffling of the y labels\n",
    "  shuffle_baseline = tfu.random_guess_accuracy(model_52_labels)\n",
    "  sklearn_accuracy = accuracy_score(model_52_labels, model_52_predictions)\n",
    "  print(f'accuracy_score: {sklearn_accuracy}')\n",
    "\n",
    "  print(f'Model 52 Performance')\n",
    "  model_52_loaded.evaluate(test_img_gen_10)\n",
    "\n",
    "  print(f'Model 53 Performance')\n",
    "  model_53.evaluate(test_img_gen_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo8iTmVP-bQz"
   },
   "source": [
    "### Prediction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPyEX_8o-bQz"
   },
   "source": [
    "Plot random images from the testing data for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-HHjFJP-bQz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['tensorflow_hub']:\n",
    "  # The model\n",
    "  file_names, classes = tfu.random_sample_all_classes(dir_food_10_class_10_test,\n",
    "                                                      number_files=2)\n",
    "  tfu.pred_and_plot2(model_52_loaded,\n",
    "                     file_names,\n",
    "                     class_names_10,\n",
    "                     img_shape=IMAGE_SIZE,\n",
    "                     actual_class_names=classes,\n",
    "                     rescale=1.,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KC5gJPT85yv1"
   },
   "source": [
    "## Feature Extraction Example\n",
    "* When using Feature Extraction, the entire CNN base is frozen\n",
    "* When using Fine-Tuning, certain CNN base layers\n",
    "  are allowed to train (with a small learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aLLRvM1d5yv1"
   },
   "source": [
    "### Instantiating CNN Base\n",
    "* pre-trained VGG16 convolutional base is used\n",
    "* include_top = False to remove the classifier after then CONV base\n",
    "* input_shape is the shape of the data you are feeding into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5Y-Ylhu5yv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# conv_base_16 is used for feature extraction\n",
    "if flags['feature_extraction']:\n",
    "  conv_base_16 = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "  )\n",
    "\n",
    "  # Freeze the whole base\n",
    "  conv_base_16.trainable = False\n",
    "\n",
    "  if True:\n",
    "    conv_base_16.summary()\n",
    "    plot_model(conv_base_16, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gZ8BGC1P5yv1"
   },
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOlQ7vCR5yv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['feature_extraction']:\n",
    "  data_augmentation = keras.Sequential(\n",
    "    [\n",
    "      layers.RandomFlip(\"horizontal\"),\n",
    "      layers.RandomRotation(0.1),\n",
    "      layers.RandomZoom(0.2),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  # data augmentation on mac fails, so i'm skipping it\n",
    "  # x = data_augmentation(inputs)\n",
    "  # x = keras.applications.vgg16.preprocess_input(x)\n",
    "\n",
    "  inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "  # x = layers.Rescaling(1/255.) (inputs)   <--- can't rescale because it won't learn\n",
    "  x = keras.applications.vgg16.preprocess_input(inputs)\n",
    "  x = conv_base_16(x, training=False)  # training=False avoids batch norm issues\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  # x = layers.Flatten()(x)\n",
    "  x = layers.Dense(256)(x)\n",
    "  # x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "  model_50 = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tVeuAi4j5yv1"
   },
   "source": [
    "### Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Knw1OzvD5yv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['feature_extraction']: \\\n",
    "        model_50.compile(loss=\"categorical_crossentropy\",\n",
    "                         optimizer=\"rmsprop\",\n",
    "                         metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "PmKw7jru5yv1"
   },
   "source": [
    "### Model Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDILzrRG5yv1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['feature_extraction']: \\\n",
    "        model_50_callbacks = [\n",
    "          # Stop after val_accuracy does not improve for patience epochs\n",
    "          keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=4,\n",
    "          ),\n",
    "          # Save model to filepath, only write over the last save if val_loss improved\n",
    "          # keras.callbacks.ModelCheckpoint(\n",
    "          #   filepath=tfu.paths['dir_model_runs'] + \"model_50_checkpoint\",\n",
    "          #   monitor=\"val_loss\",\n",
    "          #   save_best_only=True,\n",
    "          # ),\n",
    "          # Save history\n",
    "          # tf.keras.callbacks.CSVLogger(tfu.paths['dir_model_runs'] + \"model_50_logger\",\n",
    "          #                              separator=\",\",\n",
    "          #                              append=True),\n",
    "          # Save to tensorboard\n",
    "          keras.callbacks.TensorBoard(log_dir=tfu.paths['dir_tensor_board'] + \"model_50\"),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "udq1x1E15yv1"
   },
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xlhn8Mff5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['feature_extraction'] & True:\n",
    "  history_50 = model_50.fit(\n",
    "    train_img_gen_10,\n",
    "    epochs=min(MAX_EPOCHS, 15),  # changed from 10 to 3\n",
    "    validation_data=test_img_gen_10,\n",
    "    callbacks=model_50_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "57UNbXTD5yv2"
   },
   "source": [
    "### Save Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X2_W6oE5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587078,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['feature_extraction'] & True:\n",
    "  model_50.save(tfu.paths['dir_model_runs'] + \"model_50_saved\")\n",
    "  history_50_df = pd.DataFrame(history_50.history)\n",
    "  history_50_df.to_csv(tfu.paths['dir_model_runs'] + \"model_50_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "iGuV-_9Y5yv2"
   },
   "source": [
    "### Load Model Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pflpPdEp5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['feature_extraction']:\n",
    "  model_50_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_50_saved\")\n",
    "  history_50_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_50_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NYlLQT9y5yv2"
   },
   "source": [
    "### Visualize Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ak3ER-P5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['feature_extraction']:\n",
    "  tfu.plot_loss_and_accuracy(history_50_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "G46pg-IF5yv2"
   },
   "source": [
    "## Fine-Tuning Example\n",
    "* Using the 10 classes with 10% of training data image database\n",
    "* First train with entire base frozen to get an initial classifier\n",
    "* Next unfreeze the topmost layers\n",
    "* reduce learning rate and train the top of the base\n",
    "  and the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "B4gNQzUI5yv2"
   },
   "source": [
    "### Instantiating CNN Base\n",
    "* pre-trained VGG16 convolutional base is used\n",
    "* include_top = False to remove the classifier after then CONV base\n",
    "* input_shape is the shape of the data you are feeding into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZUIV9Rs5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# cnn_base_70 is used for fine-tuning\n",
    "if flags['fine_tuned']:\n",
    "  conv_base_70 = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "  conv_base_70.trainable = False\n",
    "  # Diagnostics\n",
    "  conv_base_70.summary()\n",
    "  plot_model(conv_base_70, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ikza5qIG5yv2"
   },
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnBk7EBx5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "  # Can't rescale here because it will conflict with preprocessing\n",
    "  x = keras.applications.vgg16.preprocess_input(inputs)\n",
    "  x = conv_base_70(x, training=False)\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  x = layers.Dense(32)(x)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  x = layers.Dense(32)(x)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "  model_70 = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "EeCWT3HU5yv2"
   },
   "source": [
    "### Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBRigKIZ5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  model_70.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer=keras.optimizers.Adam(),  # Use default learning rate\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BdT6EaXv5yv2"
   },
   "source": [
    "### Model Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPW7Au035yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  model_70_callbacks = [\n",
    "    # Stop after val_accuracy does not improve for patience epochs\n",
    "    keras.callbacks.EarlyStopping(\n",
    "      monitor=\"val_accuracy\",\n",
    "      patience=4,\n",
    "    ),\n",
    "    # Save model to filepath, only write over the last save if val_loss improved\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + \"model_70_checkpoint\",\n",
    "      monitor=\"val_loss\",\n",
    "      save_best_only=True,\n",
    "    ),\n",
    "    # Save to tensorboard\n",
    "    keras.callbacks.TensorBoard(log_dir=tfu.paths['dir_tensor_board'] + \"model_70\"),\n",
    "\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qFTdeYvn5yv2"
   },
   "source": [
    "\n",
    "### Fit model (with entire base frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMPUodAW5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned'] & True:\n",
    "  initial_epoch = 0\n",
    "  final_epoch = min(MAX_EPOCHS, 10)\n",
    "  history_70a = model_70.fit(train_img_gen_100,\n",
    "                             initial_epoch=initial_epoch,\n",
    "                             epochs=final_epoch,  # 5 is still improving ... change to 10?\n",
    "                             validation_data=test_img_gen_100,\n",
    "                             callbacks=model_70_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7a3hI-X_5yv2"
   },
   "source": [
    "### Unfreeze the topmost layers of the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kMtKTBU5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587079,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  conv_base_70.trainable = True\n",
    "  for layer in conv_base_70.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "  # Check to see the layers follow your desired train-ability\n",
    "  for i, layer in enumerate(conv_base_70.layers):\n",
    "    print(f'Layer {i}: {layer.name}, trainable = {layer.trainable}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6wxEu1r75yv2"
   },
   "source": [
    "### Re-compile unfrozen model\n",
    "* Must re-compile to run partially unfrozen model.\n",
    "* Use lower learning rate on fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJfevql75yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  model_70.compile(loss=\"categorical_crossentropy\",\n",
    "                   optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NKoIBqpb5yv2"
   },
   "source": [
    "### Re-Fit Model\n",
    "* Make sure to decrease the learning rate and specify and initial epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6miujljQ5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  initial_epoch = final_epoch\n",
    "  final_epoch = min(MAX_EPOCHS+2, 15)\n",
    "\n",
    "  history_70c = model_70.fit(\n",
    "    train_img_gen_100,\n",
    "    initial_epoch=initial_epoch,\n",
    "    epochs=final_epoch,\n",
    "    validation_data=test_img_gen_100,\n",
    "    callbacks=model_70_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "C80PfNho5yv2"
   },
   "source": [
    "### Save Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON9T6ezo5yv2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned'] & True:\n",
    "  # Commented below because callback already saves best model\n",
    "  # model_70a.save(tfu.paths['dir_model_runs'] + \"model_70a_saved\")\n",
    "  history_70a_df = pd.DataFrame(history_70a.history)\n",
    "  history_70a_df.to_csv(tfu.paths['dir_model_runs'] + \"model_70a_history.csv\", index=False)\n",
    "  history_70c_df = pd.DataFrame(history_70c.history)\n",
    "  history_70c_df.to_csv(tfu.paths['dir_model_runs'] + \"model_70c_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "T1IjvgDb5yv3"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "chy3GP4n5yv3"
   },
   "source": [
    "### Load Model Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqxTBqoi5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  model_70_loaded = tf.keras.models.load_model(tfu.paths['dir_model_runs'] + \"model_70_checkpoint\")\n",
    "  history_70a_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_70a_history.csv\")\n",
    "  history_70c_loaded = pd.read_csv(tfu.paths['dir_model_runs'] + \"model_70c_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yhl4932x5yv3"
   },
   "source": [
    "### Visualize Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6dN576H5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['fine_tuned']:\n",
    "  tfu.plot_loss_and_accuracy(history_70a_loaded, 'Base Frozen')\n",
    "  tfu.plot_loss_and_accuracy(history_70c_loaded, 'Fine Tuning')\n",
    "  tfu.plot_history_list([history_70a, history_70c], 'accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "9qCpMEim5yv3"
   },
   "source": [
    "# Comparing models using TensorBoard\n",
    "\n",
    "The experiment tracking tool [TensorBoard](https://www.tensorflow.org/tensorboard) is preinstalled in Google Colab.\n",
    "\n",
    "To visualize tensorboard callbacks we upload the results to [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "\n",
    "### Uploading experiments to TensorBoard\n",
    "\n",
    "To upload a series of TensorFlow logs to TensorBoard, use the following command:\n",
    "\n",
    "```\n",
    "Upload TensorBoard dev records\n",
    "\n",
    "!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n",
    "  --name \"EfficientNetB0 vs. ResNet50V2\" \\\n",
    "  --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\\n",
    "  --one_shot\n",
    "```\n",
    "\n",
    "Where:\n",
    "* `--logdir` is the target upload directory\n",
    "* `--name` is the name of the experiment\n",
    "* `--description` is a brief description of the experiment\n",
    "* `--one_shot` exits the TensorBoard uploader once uploading is finished\n",
    "\n",
    "Running the `tensorboard dev upload` command will first ask you to authorize the upload to TensorBoard.dev. After you've authorized the upload, your log files will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh73aA835yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Upload TensorBoard dev records\n",
    "# Having trouble with the prompting in interactive mode, so use this shell command instead\n",
    "if flags['tensorboard']:\n",
    "  tensor_board = tfu.paths['dir_tensor_board']\n",
    "  shell_cmd = f'tensorboard dev upload --logdir \"{tensor_board}\" ' \\\n",
    "              f'--name \"Tutorial Runs\" ' \\\n",
    "              f'--description \"Simulations From Deep Learning and Udemy Courses\" ' \\\n",
    "              f'--one_shot'\n",
    "  print(shell_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1izATAe5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Upload TensorBoard dev records\n",
    "# Pycharm prompt can't be answered\n",
    "if flags['tensorboard'] and False:\n",
    "  !tensorboard dev upload --logdir \"$tensor_board\" \\\n",
    "          --name \"Tutorial Runs\" \\\n",
    "          --description \"Simulations From Deep Learning and Udemy Courses\" \\\n",
    "          --one_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "HrNH5iCG5yv3"
   },
   "source": [
    "Every time you upload something to TensorBoard.dev you'll get a new experiment ID. The experiment ID will look something like this: https://tensorboard.dev/experiment/73taSKxXQeGPQsNBcVvY3g/ (this is the actual experiment from this notebook).\n",
    "\n",
    "If you upload the same directory again, you'll get a new experiment ID to go along with it.\n",
    "\n",
    "This means to track your experiments, you may want to look into how you name your uploads. That way when you find them on TensorBoard.dev you can tell what happened during each experiment (e.g. \"efficientnet0_10_percent_data\").\n",
    "\n",
    "### Listing experiments you've saved to TensorBoard\n",
    "\n",
    "To see all experiments you've uploaded you can use the command:\n",
    "\n",
    "```tensorboard dev list```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anrLspUh5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Check out experiments\n",
    "if flags['tensorboard'] and False:\n",
    "  !tensorboard dev list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nhxq4lRK5yv3"
   },
   "source": [
    "### Deleting experiments from TensorBoard\n",
    "\n",
    "Remember, all uploads to TensorBoard.dev are public, so to delete an experiment you can use the command:\n",
    "\n",
    "`tensorboard dev delete --experiment_id [INSERT_EXPERIMENT_ID]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OCXmp8U5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587080,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Delete an experiment\n",
    "if flags['tensorboard'] and False:\n",
    "  !tensorboard dev delete --experiment_id wEdYAPzWSdWE4mR5jy3wqQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFLt6J6u5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Check to see if experiments still exist\n",
    "if flags['tensorboard'] and False:\n",
    "  !tensorboard dev list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GxoQ71oA5yv3"
   },
   "source": [
    "# Top to Bottom Efficient Net\n",
    "Model various transfer simulations from the ground up efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "saEx4UdI5yv3"
   },
   "source": [
    "## Augmentation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbZuHrB_5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation_visualization']:\n",
    "  data_augmentation = keras.Sequential(\n",
    "    [\n",
    "      layers.RandomFlip(\"horizontal\"),\n",
    "      layers.RandomTranslation(height_factor=0.25, width_factor=0.25),\n",
    "      layers.RandomRotation(0.2),\n",
    "      layers.RandomZoom(0.2),\n",
    "      layers.RandomContrast(0.2),\n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UU0DTaL5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation_visualization']:\n",
    "  file_names, file_class = tfu.random_sample_all_classes(dir_food_10_class_1_test, number_files=1)\n",
    "  print(file_names, file_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_jG3EC-5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation_visualization']:\n",
    "  image_tensor = tfu.image_files_to_tensor(file_names, IMAGE_SIZE, channels=3, rescale=1.)\n",
    "  augmented_tensor = data_augmentation(image_tensor, training=True)\n",
    "  print(image_tensor.shape, augmented_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tp69gkTz5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation_visualization']:\n",
    "  np.all(np.isclose(np.array(image_tensor), np.array(augmented_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FY7XLIFr5yv3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['augmentation_visualization']:\n",
    "  num_images = 3\n",
    "  for image, image_aug, image_class in zip(image_tensor[:num_images],\n",
    "                                           augmented_tensor[:num_images],\n",
    "                                           file_class[:num_images]):\n",
    "    tfu.plot_tensor(image, title=image_class, include_shape=True, figsize=(3, 3))\n",
    "    tfu.plot_tensor(image_aug, title=\"augmented\", include_shape=True, figsize=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVbSz3sK5yv5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "def transfer_model(base_model,\n",
    "                   description=\"\",\n",
    "                   pooling=False,\n",
    "                   rescale=False,\n",
    "                   add_dense=False,\n",
    "                   patience=3,\n",
    "                   epochs=20,\n",
    "                   image_augmentation=False,\n",
    "                   percent_of_data=10,\n",
    "                   ):\n",
    "  \"\"\"\n",
    "  Create a transfer model from pre-trained base model\n",
    "  Args:\n",
    "    base_model: transfer model from tf.keras.applications or tensor_hub\n",
    "    description (str): Caption for tensorboard\n",
    "    pooling (bool): True if global pooling required after base mode\n",
    "    rescale (bool): True if inputs need to be rescaled by dividing by 255\n",
    "    add_dense (bool): True if you wish to add a dense 32 layer before the classifier\n",
    "    epochs (int): # of epochs to fit\n",
    "    patience (int): # early stopping patience\n",
    "    percent_of_data: 1 or 10 representing the number of data to use in training\n",
    "    image_augmentation:\n",
    "\n",
    "  Returns:\n",
    "\n",
    "  Notes:\n",
    "    1) tensor hub base models are likely feature vectors that do not require global pooling,\n",
    "       models likely require scaling\n",
    "    2) tf.keras.applications is likely a pretrained model with the classifier removed,\n",
    "       it will likely require global averaging (or flattening),\n",
    "       it may or may not require scaling (it depends on the model)\n",
    "    3) Very similar performance between app and hub base models\n",
    "       This is as expected, after all the bugs were worked out.\n",
    "\n",
    "  \"\"\"\n",
    "  epochs = min(MAX_EPOCHS, epochs)\n",
    "\n",
    "  # 1. Freeze the base model (so the pre-learned patterns remain)\n",
    "  base_model.trainable = False\n",
    "\n",
    "  # 2. Create inputs into the base model\n",
    "  inputs = tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_layer\")\n",
    "\n",
    "  # 3. Rescale if needed\n",
    "  #    Note: using tf.keras.applications, EfficientNet inputs don't have to be normalized)\n",
    "  if rescale is True:\n",
    "    x = layers.Rescaling(1. / 255)(inputs)\n",
    "  else:\n",
    "    x = layers.Rescaling(1.)(inputs)\n",
    "\n",
    "  # 4. Create Optional data augmentation layer\n",
    "  data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomTranslation(height_factor=0.25, width_factor=0.25),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "  ])\n",
    "  if image_augmentation is True:\n",
    "    x = data_augmentation(x)\n",
    "\n",
    "  # 5. Pass the inputs to the base_model\n",
    "  x = base_model(x, training=False)\n",
    "  # Check data shape after passing it to base_model\n",
    "  print(f\"Shape after base_model: {x.shape}\")\n",
    "\n",
    "  # 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\n",
    "  if pooling is True:\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "    print(f\"After GlobalAveragePooling2D(): {x.shape}\")\n",
    "\n",
    "  if add_dense is True:\n",
    "    x = layers.Dense(32)(x)\n",
    "\n",
    "  # 7. Create the output activation layer\n",
    "  outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "  # 8. Combine the inputs with the outputs into a model\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  # 9. Compile the model\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "  # 10. Select training data\n",
    "  print(f\"Using {percent_of_data}% of training data for fitting\")\n",
    "  if percent_of_data == 1:\n",
    "    train_data = train_data_set_1\n",
    "    test_data = test_data_set_1\n",
    "  elif percent_of_data == 10:\n",
    "    train_data = train_data_set_10\n",
    "    test_data = test_data_set_10\n",
    "  else:\n",
    "    raise ValueError(f\"percent_of_data can be only 1 or 10, you selected: {percent_of_data}\")\n",
    "\n",
    "  # 11. Fit the model (we use fewer steps for validation, so it's faster)\n",
    "  history = model.fit(train_data,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=test_data,\n",
    "                      # Go through less of the validation data so epochs are faster (we want faster experiments!)\n",
    "                      validation_steps=int(0.25 * len(test_data_set_10)),\n",
    "                      # Track our model's training logs for visualization later\n",
    "                      callbacks=[tfu.create_tensorboard_callback(\n",
    "                        tfu.paths['dir_tensor_board'], description),\n",
    "                        keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                      patience=patience), ],\n",
    "                      )\n",
    "  return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwDVfLYI5yv5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587081,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "# flags['efficient_net']=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5hmCldyP5yv5"
   },
   "source": [
    "## Download the EfficientNetB0 base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aL9RLjYx5yv5"
   },
   "source": [
    "### Tensorflow Hub Base Layer\n",
    "* Does not require Pooling\n",
    "* Require rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_E0PZXC5yv5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub']:\n",
    "  efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
    "\n",
    "  # Download the pretrained model and save it as a Keras layer\n",
    "  eff_net_hub = hub.KerasLayer(efficientnet_url,\n",
    "                               trainable=False,  # freeze the underlying patterns\n",
    "                               name='feature_extraction_layer',\n",
    "                               input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))  # define the input image shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "WZXtu0CK5yv5"
   },
   "source": [
    "### tf.keras.applications Base Layer\n",
    "* Requires Pooling\n",
    "* Does not require rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsMPMQay5yv5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub']:\n",
    "  eff_net_app = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "  eff_net_app.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "IfdclGCV5yv5"
   },
   "source": [
    "### Efficient Net app vs TF Hub Analysis\n",
    "* If the application and TF are set up with the correct pooling and rescaling, there is little difference between them\n",
    "* Going forward just use tf.keras.applications.EfficientNetB0() because it is faster to load\n",
    "* Adding a Dense model to the 10% training data had the best results\n",
    "* Nice to get both the resnet types working, but there is no real use in running the next set of sims again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIi7WVE15yv6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "compare_configs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZcEPeiWC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  model, history = transfer_model(eff_net_app,\n",
    "                                  'eff_app no aug',\n",
    "                                  pooling=True,\n",
    "                                  rescale=False,\n",
    "                                  add_dense=False,\n",
    "                                  epochs=min(MAX_EPOCHS, 5), )\n",
    "  print(f\"\\nEfficiency Net App with 10% of training data, no Augmentation Accuracy\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_4vo3xdC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  model, history = transfer_model(eff_net_app,\n",
    "                                  'eff_app w aug',\n",
    "                                  pooling=True,\n",
    "                                  rescale=False,\n",
    "                                  add_dense=False,\n",
    "                                  epochs=min(MAX_EPOCHS, 3),\n",
    "                                  image_augmentation=True)\n",
    "  print(f\"\\nEfficiency Net App with 10% of training data, W Augmentation Accuracy\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcIXCjB3C1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  model, history = transfer_model(eff_net_app,\n",
    "                                  'eff_app with dense',\n",
    "                                  pooling=True,\n",
    "                                  rescale=False,\n",
    "                                  add_dense=True,\n",
    "                                  epochs=min(MAX_EPOCHS, 3))\n",
    "  print(f\"\\nEfficiency Net App with 10% of training data, no augmentation, with dense, Accuracy:\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXPXQmifC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  model, history = transfer_model(eff_net_hub,\n",
    "                                  'eff_net_hub',\n",
    "                                  pooling=False,\n",
    "                                  rescale=True,\n",
    "                                  add_dense=False,\n",
    "                                  epochs=min(MAX_EPOCHS, 3))\n",
    "  print(f\"\\nEfficiency Net Hub with 10% of training data, no augmentation, no dense, Accuracy:\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLBjSPLTC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587082,
     "user_tz": 420,
     "elapsed": 32,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  model, history = transfer_model(eff_net_hub,\n",
    "                                  'eff_net_hub with dense',\n",
    "                                  pooling=False,\n",
    "                                  rescale=True,\n",
    "                                  add_dense=True)\n",
    "  print(f\"\\nEfficiency Net Hub with 10% of training data, no augmentation, with dense, Accuracy:\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzTM1iziC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  model, history = transfer_model(eff_net_hub,\n",
    "                                  'eff_net_hub 1%',\n",
    "                                  pooling=False,\n",
    "                                  rescale=True,\n",
    "                                  add_dense=False,\n",
    "                                  image_augmentation=True,\n",
    "                                  percent_of_data=1)\n",
    "  print(f\"\\nEfficiency Net Hub with 10% of training data, no augmentation, with dense, Accuracy:\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ulcC_MfeC1hv"
   },
   "source": [
    "### Compare Efficiency Net with Limited Data, Augments, and Dense Layer\n",
    "\n",
    "1. 1%  With Dense Layer and W/ Augment Val Accuracy of 0.71 after 14 epochs\n",
    "2. 1%  No Dense and W/ Augment Val Accuracy of 0.67 after 14 epochs\n",
    "3. 10% With Dense Layer and No Augment reached 0.85 after 9 epochs\n",
    "4. 10% No Dense Layer and No Augment reached 0.84 after 14 epochs\n",
    "\n",
    "Takeaways:\n",
    "* 10% with Dense was fast and the best\n",
    "* 1% Benefited from augment\n",
    "* For this data, use a Dense and Augment only if you have scarce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6tIqBsjC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '10p Data No Dense No Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=False,\n",
    "                                   add_dense=False,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=10,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O9epGY8C1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '1p Data No Dense No Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=False,\n",
    "                                   add_dense=False,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=1,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFHMwFL1C1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '10p Data W Dense No Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=False,\n",
    "                                   add_dense=True,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=10,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvGECPltC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '1p Data W Dense No Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=False,\n",
    "                                   add_dense=True,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=1,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2gnJqtXC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 30,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '10p Data No Dense W Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=True,\n",
    "                                   add_dense=False,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=10,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzSbg-l3C1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 30,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '1p Data No Dense W Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=True,\n",
    "                                   add_dense=False,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=1,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ku5HrrUFC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587083,
     "user_tz": 420,
     "elapsed": 30,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '10p Data W Dense W Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=True,\n",
    "                                   add_dense=True,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=10,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHDn7KpwC1hv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587084,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['efficient_net'] and flags['tensorflow_hub'] and compare_configs:\n",
    "  description = '1p Data W Dense W Aug'\n",
    "  model, _history = transfer_model(eff_net_app,\n",
    "                                   pooling=True,\n",
    "                                   rescale=False,\n",
    "                                   image_augmentation=True,\n",
    "                                   add_dense=True,\n",
    "                                   epochs=min(MAX_EPOCHS, 15),\n",
    "                                   percent_of_data=1,\n",
    "                                   description=description)\n",
    "  print(f\"\\n{description}\")\n",
    "  model.evaluate(test_data_set_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSC0mDjGC1hv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587084,
     "user_tz": 420,
     "elapsed": 31,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "1415b04a-2fe9-4e6f-b5da-eb6e0974e030"
   },
   "outputs": [],
   "source": [
    "tensor_board = tfu.paths['dir_tensor_board']\n",
    "shell_cmd = f'tensorboard dev upload --logdir \"{tensor_board}\" ' \\\n",
    "            f'--name \"Tutorial Runs\" ' \\\n",
    "            f'--description \"Simulations From Deep Learning and Udemy Courses\" ' \\\n",
    "            f'--one_shot'\n",
    "print(shell_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ScXK6ccOC1hw"
   },
   "source": [
    "Previous Experiment Locations\n",
    "* https://tensorboard.dev/experiment/Lj5kGLkgR1OQcnOey3Mu6g/\n",
    "* https://tensorboard.dev/experiment/Cg49c165TbqpGI474MRRpA/ compares app ves hub\n",
    "* https://tensorboard.dev/experiment/1dWUje85S4eJUP0NknlX5g/ looks at % data, dense, and augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tDdf5aeWC1hw"
   },
   "source": [
    "# TimeSeries\n",
    "\n",
    "Note it is very important to specify the input shape for a time series model\n",
    "Some model architectures require the data to be flattened first (such as a dense model)\n",
    "and some do not such as Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8P27LbIPC1hw"
   },
   "source": [
    "## Create simulated timeseries from integer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iN-jJEYC1hw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587084,
     "user_tz": 420,
     "elapsed": 19,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "00a5e8ac-89f1-4948-e175-33df02bd791d"
   },
   "outputs": [],
   "source": [
    "if flags['time_series']:\n",
    "  # Create input and targets from a simulated timeseries\n",
    "  num_points = 21  # num points in simulated data\n",
    "  int_sequence1 = np.arange(num_points, dtype=np.float64)\n",
    "  # tfu.TimeSeries expects a 2-d feature matrix\n",
    "  int_sequence2 = np.expand_dims(int_sequence1, axis=1)\n",
    "  print(f\"{int_sequence1.shape, int_sequence2.shape}\")\n",
    "\n",
    "  ts1 = tfu.TimeSeries(\n",
    "    features=int_sequence2,\n",
    "    target_column=0,\n",
    "    fraction_training=1.0,\n",
    "    fraction_validation=0.0,\n",
    "    fraction_testing=0.0,\n",
    "    window=4,\n",
    "    shuffle=False,\n",
    "    batch_size=3,\n",
    "    standardize=True,\n",
    "  )\n",
    "  ts1.calc_all_naive_abs_errors_1()\n",
    "  ts1.diagnostics(verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "uwobhS7bC1hw"
   },
   "source": [
    "## Create timeseries from meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRPq-yZBC1hw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587854,
     "user_tz": 420,
     "elapsed": 787,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "0a14455e-7c8a-435b-aa17-e2ff9c270c92"
   },
   "outputs": [],
   "source": [
    "if flags['time_series']:\n",
    "  fn_climate_data = \"jena_climate_2009_2016.csv\"\n",
    "  url_climate_data = \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "  tfu.url_to_local_dir(url_climate_data, decompress=True)\n",
    "  climate_df = pd.read_csv(fn_climate_data)\n",
    "  features = climate_df.drop(['Date Time'], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYxj2hxYC1hw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276587855,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "08ca188d-235a-4d14-f176-54584be2c1a1"
   },
   "outputs": [],
   "source": [
    "if flags['time_series']:\n",
    "  display(climate_df)\n",
    "  print(f\"\\ncolumns: {climate_df.columns}\")\n",
    "  print(f\"\\nshape: {climate_df.shape}\")\n",
    "  print(f\"{features.shape =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6htXNKLC1hw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276588175,
     "user_tz": 420,
     "elapsed": 326,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "4aea733b-97b9-4f91-b19c-2c9511cc5e3e"
   },
   "outputs": [],
   "source": [
    "if flags['time_series']:\n",
    "  ts_weather2 = tfu.TimeSeries(\n",
    "    features=features.copy(),  # Use a copy to avoid corrupting original\n",
    "    target_column=1,  # temperature data in index 1\n",
    "    fraction_training=0.5,\n",
    "    fraction_validation=0.25,\n",
    "    fraction_testing=0.25,\n",
    "    sampling_unit=\"hours\",\n",
    "    sampling_rate=6,  # 6 samples per hour\n",
    "    window=5 * 24,  # five-day window of input\n",
    "    horizon=24,  # predict 24 hours into the future\n",
    "    shuffle=True,\n",
    "    batch_size=256,\n",
    "    standardize=True\n",
    "  )\n",
    "  ts_weather2.diagnostics(verbose=0)\n",
    "  naive_metrics = ts_weather2.naive_metrics()\n",
    "  print(f\"{naive_metrics = } \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82AyOwxwC1hw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276605162,
     "user_tz": 420,
     "elapsed": 16990,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "5bdc0c9d-a235-495d-829d-5621c7f16f3d"
   },
   "outputs": [],
   "source": [
    "# calc_all_naive_abs_errors_1 is slow, but agrees with Deep Learning book\n",
    "if flags['time_series']:\n",
    "  ts_weather2.calc_all_naive_abs_errors_1()\n",
    "  ts_weather2.diagnostics(verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TX1z6yjFC1hw"
   },
   "source": [
    "## Modeling timeseries datasets\n",
    "Use the training and the val dataset for model fitting\n",
    "and use the testing dataset for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "gdQsBJpyC1hw"
   },
   "source": [
    "### Simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdn6bjTn5yv7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276746267,
     "user_tz": 420,
     "elapsed": 141109,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "b6b31a4e-9fbf-4134-f0ec-e72f540090f5"
   },
   "outputs": [],
   "source": [
    "if flags['time_series']:\n",
    "  inputs = keras.Input(shape=ts_weather2.input_shape)\n",
    "  x = layers.Flatten()(inputs)  # Note, you need flatten 2-d data before dense!\n",
    "  x = layers.Dense(16, activation=\"relu\")(x)\n",
    "  outputs = layers.Dense(1)(x)\n",
    "  model_82 = keras.Model(inputs, outputs)\n",
    "  model_file_name_82 = \"weather_dense_82\"\n",
    "\n",
    "\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_file_name_82,\n",
    "      save_best_only=True,\n",
    "      )\n",
    "  ]\n",
    "  model_82.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# next line now gives problem on colab\n",
    "# if I commned out the callbacks, it will work\n",
    "if flags['time_series'] and True:\n",
    "  history_82 = model_82.fit(ts_weather2.train_dataset,\n",
    "                            epochs=min(MAX_EPOCHS, 10),\n",
    "                            validation_data=ts_weather2.val_dataset,\n",
    "                            callbacks=callbacks,\n",
    "                            )\n",
    "\n",
    "  model_82 = keras.models.load_model(tfu.paths['dir_model_runs'] + model_file_name_82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VScef9xb5yv7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276750862,
     "user_tz": 420,
     "elapsed": 4604,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "cbc878c2-14d8-4f89-b8e3-45a56f44d0bd"
   },
   "outputs": [],
   "source": [
    "if flags['time_series'] and True:\n",
    "  results_82 = model_82.evaluate(ts_weather2.test_dataset)\n",
    "  print(f\"model_82 Results: {results_82}\")\n",
    "  print(f\"model_82 Test MAE: {results_82[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8ifYcb15yv7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276755221,
     "user_tz": 420,
     "elapsed": 4371,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "52370040-2617-4dfa-be4e-d92d5a336463"
   },
   "outputs": [],
   "source": [
    "if flags['time_series'] and True:\n",
    "  preds_82 = model_82.predict(ts_weather2.test_dataset)\n",
    "  metrics_82 = ts_weather2.evaluate_metrics(ts_weather2.y_true_test,\n",
    "                                            preds_82,\n",
    "                                            scaling='testing')\n",
    "  print(f\"{metrics_82=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['time_series'] and True:\n",
    "  ts_weather2.plot_test_vs_real(model_82)"
   ],
   "metadata": {
    "id": "zJ5gA_Eq3zMS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276765357,
     "user_tz": 420,
     "elapsed": 10147,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "515c7b51-44ee-4b14-ce8d-22d440d06d6f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(tfu)"
   ],
   "metadata": {
    "id": "GW1qzEZ_3zMS",
    "outputId": "dd762c0d-15ae-4e6b-ff9f-d3b6f6542be1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276766727,
     "user_tz": 420,
     "elapsed": 1376,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "84LiHbKM5yv7"
   },
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfbeWlet5yv7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276957787,
     "user_tz": 420,
     "elapsed": 181345,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "cf5ddb47-014d-4973-d604-0ff83cb8b2f0"
   },
   "outputs": [],
   "source": [
    "if flags['time_series']:\n",
    "  inputs = keras.Input(shape=ts_weather2.input_shape)\n",
    "  x = layers.LSTM(16)(inputs)\n",
    "  outputs = layers.Dense(1)(x)\n",
    "  model_83 = keras.Model(inputs, outputs)\n",
    "  model_file_name_83 = \"weather_dense_83\"\n",
    "\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_file_name_83,\n",
    "      save_best_only=True,\n",
    "      )\n",
    "  ]\n",
    "  model_83.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "if flags['time_series'] and True:\n",
    "  history_83 = model_83.fit(ts_weather2.train_dataset,\n",
    "                            epochs=min(MAX_EPOCHS, 10),\n",
    "                            validation_data=ts_weather2.val_dataset,\n",
    "                            callbacks=callbacks)\n",
    "\n",
    "  model_83 = keras.models.load_model(tfu.paths['dir_model_runs'] + model_file_name_83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['time_series'] and True:\n",
    "  results_83 = model_83.evaluate(ts_weather2.test_dataset)\n",
    "  print(f\"model_83 Results: {results_83}\")\n",
    "  print(f\"model_83 Test MAE: {results_83[1]:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['time_series'] and True:\n",
    "  preds_83 = model_83.predict(ts_weather2.test_dataset)\n",
    "  metrics_83 = ts_weather2.evaluate_metrics(ts_weather2.y_true_test,\n",
    "                                            preds_83,\n",
    "                                            scaling='testing')\n",
    "  print(f\"{metrics_83=}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Ensemble prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4nJ6HjI5yv7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276966977,
     "user_tz": 420,
     "elapsed": 9203,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "e660be3e-d8a0-4369-d28a-11340d98a071"
   },
   "outputs": [],
   "source": [
    "if flags['time_series'] and True:\n",
    "  model_names = ['model_82', 'model_83']\n",
    "  models_dict = {'model_82': model_82, 'model_83': model_83}\n",
    "  preds_dict = tfu.create_preds_dict(model_names, models_dict, ts_weather2.test_dataset)\n",
    "  preds_array = tfu.create_preds_array(model_names, preds_dict)\n",
    "  ensemble = tfu.ensemble_preds(preds_array, method='mean')\n",
    "  metrics_84 = ts_weather2.evaluate_metrics(ts_weather2.y_true_test,\n",
    "                                            ensemble,\n",
    "                                            scaling='testing')\n",
    "  print(f\"{metrics_84=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kKVDthAs5yv7"
   },
   "source": [
    "### Compare multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNdWMNkq5yv7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967386,
     "user_tz": 420,
     "elapsed": 424,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "outputId": "30a37e27-06f4-42f3-bc02-c4561d682ca9"
   },
   "outputs": [],
   "source": [
    "if flags['time_series'] and True:\n",
    "  mae_dict = {'naive': naive_metrics['mae'],\n",
    "              'dense': metrics_82['mae'],\n",
    "              'LSTM': metrics_83['mae'],\n",
    "              'ensemble': metrics_84['mae'],\n",
    "              }\n",
    "  df_mae = pd.DataFrame(mae_dict, index=['mae'])\n",
    "  display(df_mae)\n",
    "  df_mae.plot(figsize=(6, 6), kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "-FJ2HVMY5yv7"
   },
   "source": [
    "# NLP\n",
    "\n",
    "Steps to NLP\n",
    "1. Create Datasets from file structure\n",
    "2. Create vocab\n",
    "3. Pre-process pre-trained embedding\n",
    "4. Create embedding matrix\n",
    "5. Create embedding layer (with or without pre-trained embedding)\n",
    "\n",
    "Two approaches to modeling using Chollet's approach are tried below.\n",
    "1. A scalable solution that uses datasets (original book approach)\n",
    "2. A non-scalable solution where the entire file system is loaded into memory\n",
    "\n",
    "I thought that loading the entire dataset into memory first would be much faster\n",
    "but it turns out that the dataset scalable soln was slightly faster ..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Key Parameters"
   ],
   "metadata": {
    "collapsed": false,
    "id": "hhBFlOx73zMS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  MAX_TOKENS = 20000  # Max words in corpus vocab\n",
    "  OUTPUT_DIMENSIONS = 256  # Num of dims when embedding vector learns off data\n",
    "  PRE_TRAINED_DIM = 100  # Max dimension in embedding vector using glove pre-trained data\n",
    "  MAX_LENGTH = 600  # Max words in feature input string\n",
    "  NGRAMS = 1"
   ],
   "metadata": {
    "id": "JKKmr3rj3zMS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967386,
     "user_tz": 420,
     "elapsed": 9,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "bjwAy8IR3zMS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLP Scalable Datasets\n",
    "\n",
    "Note that the nlp deep learning book sequence modeling initially fails on windows, but requires the deletion of a modeling directory first\n",
    "*\tThe problem with the windows run was that there was an 'unsupported' directory in the training\n",
    "\tthat was not deleted by the  !rm -r aclImdb/train/unsup\n",
    "*\tThat lead to total crashing and negative loss because there were different number of catagories in the various folders\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "MImIB3B13zMS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  print(f\"Ensuring IMDB file system is present\")\n",
    "  base_dir = \"aclImdb\"\n",
    "  file_name = Path(\"aclImdb_v1.tar.gz\")\n",
    "  url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "  if file_name.exists():\n",
    "    print(f\"{file_name} exists and will not be re-downloaded\")\n",
    "  else:\n",
    "    print(f\"{file_name}: downloading, expanding, and splitting\")\n",
    "\n",
    "    imdb_path = tfu.url_to_local_dir(url, decompress=False)  # I have not implemented tar.gz protocol\n",
    "    !tar -xf aclImdb_v1.tar.gz\n",
    "    # Warning!!! If you don't successfully remove 'unsup', your model will freak out and\n",
    "    # give garbage results (with weird things like negative losses)\n",
    "    if run_mode == 'colab':\n",
    "      !rm -r aclImdb/train/unsup\n",
    "    else:\n",
    "      !wsl rm -r aclImdb/train/unsup\n",
    "\n",
    "    # Split training dataset to create valuation dataset\n",
    "    tfu.split_directory(base_dir=base_dir,\n",
    "                        old_dir=\"train\",\n",
    "                        new_dir=\"val\",\n",
    "                        fraction=0.2,\n",
    "                        )"
   ],
   "metadata": {
    "id": "Jti9d6bo3zMS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967386,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning Methodology (Scalable)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4cIPGSYz3zMS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Datasets\n",
    "Make sure to cache and pre-fetch for increased speed\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "--wCcl5q3zMS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  AUTOTUNE = tf.data.AUTOTUNE\n",
    "  BATCH_SIZE = 32\n",
    "  train_ds = keras.utils.text_dataset_from_directory(\n",
    "    base_dir + \"/train\", batch_size=BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  val_ds = keras.utils.text_dataset_from_directory(\n",
    "    base_dir + \"/val\", batch_size=BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "  test_ds = keras.utils.text_dataset_from_directory(\n",
    "    base_dir + \"/test\", batch_size=BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "id": "I_r1eT4C3zMS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967386,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create TextVectorization\n",
    "output_mode=\"int\" will result in the first two index being:\n",
    "0 - no word here and is a blank string \"\".\n",
    "1 - is used for OOV - out of vocabulary or Unknown '[UNK]'"
   ],
   "metadata": {
    "collapsed": false,
    "id": "kLJEndgCXkCJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  text_vectorization_1 = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    "    ngrams=NGRAMS,\n",
    "  )\n",
    "  text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "  text_vectorization_1.adapt(text_only_train_ds)\n",
    "\n",
    "  vocab = text_vectorization_1.get_vocabulary()\n",
    "  print(f'Vocab length = {len(vocab)}')\n",
    "  print(f'First 10 words are: {vocab[:10]}')\n",
    "  print(f'Last  10 words are: {vocab[-10:]}')"
   ],
   "metadata": {
    "id": "5_4LisTyXkCK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967387,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "XE6eG6Gu5yv8"
   },
   "source": [
    "### Create Embedding Layers (trainable and pre-trained)\n",
    "\n",
    "The stanford glove embedding has 400000 words in 100d format\n",
    "Downloaded glove data from:\n",
    "https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt\n",
    "This creates glove.6B.100d.txt.zip\n",
    "run 'unzip glove.6B.100d.txt.zip' to create glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  print(f\"Ensuring Glove embedding files are present\")\n",
    "  file_name = Path(\"glove.6B.zip\")\n",
    "  url_glove = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "  if file_name.exists():\n",
    "    print(f\"{file_name} exists and will not be re-downloaded\")\n",
    "  else:\n",
    "    print(f\"{file_name}: downloading, expanding\")\n",
    "    tfu.url_to_local_dir(url_glove, decompress=True)"
   ],
   "metadata": {
    "id": "NqHwLn5Fbk45",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967387,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuKrov365yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967387,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  # download/process as above to create glove_embedding\n",
    "  glove_embedding = \"glove.6B.100d.txt\"\n",
    "  embedding_dict_1 = tfu.read_embeddings_file(glove_embedding)\n",
    "  # Create embedding matrix for the pre-trained embedding\n",
    "  embedding_matrix_1 = tfu.get_embedding_matrix(MAX_TOKENS,\n",
    "                                              PRE_TRAINED_DIM,\n",
    "                                              text_vectorization_1,\n",
    "                                              embedding_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjJasldj5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967387,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  name = 'embedding_trainable_1'\n",
    "  embedding_trainable_1 = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=OUTPUT_DIMENSIONS,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=True,\n",
    "    mask_zero=True,  # True the input value 0 is a \"padding\" to be masked out.\n",
    "    embeddings_initializer=\"uniform\",  # default, initialize randomly\n",
    "    name=name)\n",
    "\n",
    "  name = 'embedding_pre_trained'\n",
    "  # Create embedding layer\n",
    "  embedding_pre_trained_1 = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=PRE_TRAINED_DIM,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=False,\n",
    "    mask_zero=True,  # True the input value 0 is a \"padding\" to be masked out.\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix_1),\n",
    "    name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Google NLP Tutorial Methodology"
   ],
   "metadata": {
    "id": "Xb8mtn6Irgg-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# See google tutorial load 07 text for details.\n",
    "# A conv1d model may be very effective and fast\n",
    "if False:\n",
    "  model_name_03 = \"conv1d_model\"\n",
    "  # double stacked bidirectional\n",
    "\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")  # inputs are 1-dimensional strings\n",
    "  x = text_vec_int(inputs)  # turn the input text into vector\n",
    "\n",
    "  embedding_layer = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=64,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    mask_zero = True,\n",
    "    trainable=True,\n",
    "  )\n",
    "  x = embedding_layer(x)\n",
    "\n",
    "  x = layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2)(x)\n",
    "  x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "  outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "  model_03 = keras.Model(inputs, outputs, name=model_name_03)\n",
    "  model_03.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "  model_03.summary()"
   ],
   "metadata": {
    "id": "VdnQuDF5ryFF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967387,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JZcEh5Rb5yv8"
   },
   "source": [
    "### Bag of words modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fduO2kcF5yv8"
   },
   "source": [
    "#### Create simple dense model with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOBZ_Bqz5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967388,
     "user_tz": 420,
     "elapsed": 8,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  def run_model(model_name,\n",
    "                train_ds,\n",
    "                val_ds,\n",
    "                test_ds,\n",
    "                max_tokens=20000,\n",
    "                hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=model_name)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    callbacks = [\n",
    "      keras.callbacks.ModelCheckpoint(\n",
    "        filepath=tfu.paths['dir_model_runs'] + model_name,\n",
    "        save_best_only=True)\n",
    "    ]\n",
    "    model.summary()\n",
    "    model.fit(train_ds.cache(),\n",
    "              validation_data=val_ds.cache(),\n",
    "              epochs=min(MAX_EPOCHS, 10),  # normally 10, 5 for debugging\n",
    "              callbacks=callbacks)\n",
    "    model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name)\n",
    "    print(f\"{model_name} Test acc: {model.evaluate(test_ds)[1]:.3f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Wco_wC_d5yv8"
   },
   "source": [
    "#### Multi-hot and tf_idf TextVectorization\n",
    "Seemed to get the best performance on the example sentences\n",
    "for count with ngrams of 2\n",
    "\n",
    "Multi-hot, ngrams=1 --> test accuracy = 0.889\n",
    "\n",
    "Multi-hot, ngrams=2 --> test accuracy = 0.896\n",
    "\n",
    "Multi-hot, ngrams=5 --> test accuracy = 0.893\n",
    "\n",
    "tf_idf, ngrams=1    --> test accuracy = 0.884\n",
    "\n",
    "tf_idf, ngrams=2    --> test accuracy = 0.857\n",
    "\n",
    "count, ngrams=1    --> test accuracy = 0.889\n",
    "\n",
    "count, ngrams=2    --> test accuracy = 0.885\n",
    "\n",
    "count, ngrams=5    --> test accuracy = 0.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXnjsJ8b5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967388,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  # Select output mode and ngrams\n",
    "  output_mode = \"tf_idf\"  # \"multi_hot\", \"tf_idf\", or \"count\"\n",
    "  ngrams = 2  # 1,2, or 5 were tried\n",
    "  model_name_90 = f\"{output_mode}_ngrams_{ngrams}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0ksIedh5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967388,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  text_vec_1 = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=output_mode,\n",
    "    ngrams=ngrams,\n",
    "  )\n",
    "  text_vec_1.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJF9LrUA5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967388,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  # Encode the text by modifying the datasets (not by including an embedding layer)\n",
    "  train_ds_encoded = train_ds.map(\n",
    "    lambda x, y: (text_vec_1(x), y),\n",
    "    num_parallel_calls=4)\n",
    "  val_ds_encoded = val_ds.map(\n",
    "    lambda x, y: (text_vec_1(x), y),\n",
    "    num_parallel_calls=4)\n",
    "  test_ds_encoded = test_ds.map(\n",
    "    lambda x, y: (text_vec_1(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLcgv4fa5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967388,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp'] and True:\n",
    "  model_90 = run_model(model_name_90,\n",
    "                       train_ds_encoded,\n",
    "                       val_ds_encoded,\n",
    "                       test_ds_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fBTR-5JL5yv8"
   },
   "source": [
    "#### Make predictions of sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17EuruZx5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967388,
     "user_tz": 420,
     "elapsed": 6,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp'] and True:\n",
    "  review_01 = tf.convert_to_tensor([\n",
    "    \"That was an excellent movie, I loved it.\",\n",
    "  ])\n",
    "  review_01_encoded = text_vec_1(review_01)\n",
    "  review_01_pred = model_90.predict(review_01_encoded)\n",
    "  print(f\"Review: {review_01}\")\n",
    "  print(f'Estimated probability the review was positive: {review_01_pred[0][0] * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWHPzirJ5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967389,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp'] and True:\n",
    "  review_02 = tf.convert_to_tensor([\n",
    "    \"That movie was terrible, I hated every scene.\",\n",
    "  ])\n",
    "  review_02_encoded = text_vec_1(review_02)\n",
    "  review_02_pred = model_90.predict(review_02_encoded)\n",
    "  print(f\"Review: {review_02}\")\n",
    "  print(f'Estimated probability the review was positive: {review_02_pred[0][0] * 100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eljEzhVi5yv8"
   },
   "source": [
    "### Sequence Modeling\n",
    "1. Use text_vectorization to change sentences to array of ints\n",
    "2. Can use various approaches to creating the embedding layer\n",
    "  * one-hot encode array of ints into 2-d matrix with shape (MAX_LENGTH, MAX_TOKENS)\n",
    "where MAX_LENGTH is the max words in feature input string and\n",
    "MAX_TOKENS is the max words in corpus vocab.  This is too slow to be practical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "v70IXczD5yv8"
   },
   "source": [
    "#### Adjust datasets to output ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38UKbhxy5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967389,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  # text_vectorization from 'Create TextVectorization' section\n",
    "  train_ds_int = train_ds.map(\n",
    "    lambda x, y: (text_vectorization_1(x), y),\n",
    "    # lambda x, y: (text_vectorization_1(x), tf.cast(y, dtype=tf.float64)),\n",
    "    num_parallel_calls=4,\n",
    "  )\n",
    "  val_ds_int = val_ds.map(\n",
    "    lambda x, y: (text_vectorization_1(x), y),\n",
    "    # lambda x, y: (text_vectorization_1(x), tf.cast(y, dtype=tf.float64)),\n",
    "    num_parallel_calls=4,\n",
    "  )\n",
    "  test_ds_int = test_ds.map(\n",
    "    lambda x, y: (text_vectorization_1(x), y),\n",
    "    # lambda x, y: (text_vectorization_1(x), tf.cast(y, dtype=tf.float64)),\n",
    "    num_parallel_calls=4,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  for x in train_ds_int:\n",
    "    print(x)\n",
    "    break"
   ],
   "metadata": {
    "id": "J3CyWGwzXkCK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967389,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "AYAxgdcZ5yv8"
   },
   "source": [
    "#### sequential models with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycacDctM5yv8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967389,
     "user_tz": 420,
     "elapsed": 7,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  model_name_91 = \"lstm_untrained_embeddings_1\"\n",
    "  # inputs = keras.Input(shape=(None,), dtype=\"int64\")  # Book formulation does not specify shape\n",
    "  inputs = keras.Input(shape=(MAX_LENGTH,), dtype=\"int64\")  # using MAX_LENGTH\n",
    "  # inputs = keras.Input(shape=(MAX_LENGTH,))  # using MAX_LENGTH\n",
    "  embedding_layer = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=OUTPUT_DIMENSIONS,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=True,\n",
    "  )\n",
    "  embedded = embedding_layer(inputs)\n",
    "  x = layers.Bidirectional(layers.LSTM(32))(embedded)  # this does not converge on macOS with my interpreter\n",
    "  # x = layers.LSTM(32)(embedded)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_91 = keras.Model(inputs, outputs, name=model_name_91)\n",
    "  model_91.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    # optimizer=\"adam\",\n",
    "    # optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.001),\n",
    "    # optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.001),\n",
    "    # optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.0001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "  model_91.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1upOyYfxC1hy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967875,
     "user_tz": 420,
     "elapsed": 38,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_91,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_91.fit(train_ds_int,\n",
    "               validation_data=val_ds_int,\n",
    "               epochs=min(MAX_EPOCHS, 10),\n",
    "               callbacks=callbacks)\n",
    "  model_91 = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_91)\n",
    "  print(f\"Test acc: {model_91.evaluate(test_ds_int)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  model_name_92 = \"lstm_trained_embeddings_1\"\n",
    "  # inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "  inputs = keras.Input(shape=(MAX_LENGTH,), dtype=\"int64\")  # using MAX_LENGTH\n",
    "  # inputs = keras.Input(shape=(MAX_LENGTH,))  # using MAX_LENGTH\n",
    "  x = embedding_pre_trained_1(inputs)  # See embedding layer section for details.\n",
    "  x = layers.Bidirectional(layers.LSTM(32))(x)  # this does not converge on macOS with my interpreter\n",
    "  # x = layers.LSTM(32)(embedded)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_92 = keras.Model(inputs, outputs, name=model_name_92)\n",
    "  model_92.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "  model_92.summary()"
   ],
   "metadata": {
    "id": "64dsj_Uabk46",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967876,
     "user_tz": 420,
     "elapsed": 38,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWOZvMkr5yv9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967876,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_92,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_92.fit(train_ds_int,\n",
    "               validation_data=val_ds_int,\n",
    "               epochs=min(MAX_EPOCHS, 10),  # originally 10\n",
    "               callbacks=callbacks)\n",
    "  model_92 = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_92)\n",
    "  print(f\"Test acc: {model_92.evaluate(test_ds_int)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8n_GqOcbrgG_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967876,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modified Deep Learning Methodology (Not-Scalable)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "aWiVat-E3zMU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Datasets"
   ],
   "metadata": {
    "collapsed": false,
    "id": "mmzq_wXM3zMU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo - consider redoing below using the as_numpy_iterator() to avoid hard coding max_files\n",
    "# *\tReturns an iterator which converts all elements of the dataset to numpy.\n",
    "# *\tdata_as_list = [x for (x, y) in train_dataset.as_numpy_iterator()]"
   ],
   "metadata": {
    "id": "m-E4zS7D3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967876,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_entire_dataset_from_directory(base_dir, name, max_num_files):\n",
    "  \"\"\"Read an NLP file structure into single x and y variables\"\"\"\n",
    "  path = base_dir + \"/\" + name\n",
    "  print(f\"loading data_set: {name} from: {path}\")\n",
    "  data_set = keras.utils.text_dataset_from_directory(path, batch_size=max_num_files)\n",
    "  for x, y in data_set:\n",
    "    if len(x) > max_num_files:\n",
    "      raise ValueError(f\"{len(x)} > {max_num_files}.  Increase max_num_files\")\n",
    "    return x, y"
   ],
   "metadata": {
    "id": "LBIBaAM03zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967876,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  max_files = 1_000_000\n",
    "  X_train, y_train = load_entire_dataset_from_directory(base_dir,\n",
    "                                                        \"train\",\n",
    "                                                        max_files)\n",
    "  X_val, y_val = load_entire_dataset_from_directory(base_dir,\n",
    "                                                        \"val\",\n",
    "                                                        max_files)\n",
    "  X_test, y_test = load_entire_dataset_from_directory(base_dir,\n",
    "                                                        \"test\",\n",
    "                                                        max_files)\n",
    "  print(X_train.shape, X_val.shape, X_test.shape)"
   ],
   "metadata": {
    "id": "BPhwa_7m3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967876,
     "user_tz": 420,
     "elapsed": 36,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create TextVectorization\n",
    "output_mode=\"int\" will result in the first two index being:\n",
    "0 - no word here and is a blank string \"\".\n",
    "1 - is used for OOV - out of vocabulary or Unknown '[UNK]'"
   ],
   "metadata": {
    "collapsed": false,
    "id": "j4iH5Prl3zMU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  text_vectorization_2 = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    "    ngrams=NGRAMS,\n",
    "  )\n",
    "  text_vectorization_2.adapt(X_train)\n",
    "\n",
    "  vocab = text_vectorization_2.get_vocabulary()\n",
    "  print(f'Vocab length = {len(vocab)}')\n",
    "  print(f'First 10 words are: {vocab[:10]}')\n",
    "  print(f'Last  10 words are: {vocab[-10:]}')"
   ],
   "metadata": {
    "id": "pjVOMtU33zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967877,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Embedding Layers (trainable and pre-trained)\n",
    "\n",
    "The stanford glove embedding has 400000 words in 100d format\n",
    "Downloaded glove data from:\n",
    "https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt\n",
    "This creates glove.6B.100d.txt.zip\n",
    "run 'unzip glove.6B.100d.txt.zip' to create glove.6B.100d.txt"
   ],
   "metadata": {
    "collapsed": false,
    "id": "t5BLbzVu3zMU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  print(f\"Ensuring Glove embedding files are present\")\n",
    "  file_name = Path(\"glove.6B.zip\")\n",
    "  url_glove = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "  if file_name.exists():\n",
    "    print(f\"{file_name} exists and will not be re-downloaded\")\n",
    "  else:\n",
    "    print(f\"{file_name}: downloading, expanding\")\n",
    "    tfu.url_to_local_dir(url_glove, decompress=True)"
   ],
   "metadata": {
    "id": "1SZhkdKn3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967877,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  # download/process as above to create glove_embedding\n",
    "  glove_embedding = \"glove.6B.100d.txt\"\n",
    "  embedding_dict_2 = tfu.read_embeddings_file(glove_embedding)\n",
    "  # Create embedding matrix for the pre-trained embedding\n",
    "  embedding_matrix_2 = tfu.get_embedding_matrix(MAX_TOKENS,\n",
    "                                              PRE_TRAINED_DIM,\n",
    "                                              text_vectorization_2,\n",
    "                                              embedding_dict_2)"
   ],
   "metadata": {
    "id": "-tVnNXCe3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967877,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  name = 'embedding_trainable'\n",
    "  embedding_trainable_2 = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=OUTPUT_DIMENSIONS,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=True,\n",
    "    mask_zero=True,  # True the input value 0 is a \"padding\" to be masked out.\n",
    "    embeddings_initializer=\"uniform\",  # default, initialize randomly\n",
    "    name=name)\n",
    "\n",
    "  name = 'embedding_pre_trained'\n",
    "  # Create embedding layer\n",
    "  embedding_pre_trained_2 = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=PRE_TRAINED_DIM,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=False,\n",
    "    mask_zero=True,  # True the input value 0 is a \"padding\" to be masked out.\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix_2),\n",
    "    name=name)"
   ],
   "metadata": {
    "id": "Iyi80z9G3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967877,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bag of words modeling\n",
    "\n",
    "Note consider using 'binary' as the output for text vectorization for bag of words modeling"
   ],
   "metadata": {
    "collapsed": false,
    "id": "bJ0DzFvI3zMU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create simple dense model with dropout"
   ],
   "metadata": {
    "collapsed": false,
    "id": "V7G-nVFo3zMU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  def run_model_2(model_name,\n",
    "                  X_train,\n",
    "                  y_train,\n",
    "                  X_val,\n",
    "                  y_val,\n",
    "                  X_test,\n",
    "                  y_test,\n",
    "                  text_vec,\n",
    "                  hidden_dim=16):\n",
    "    inputs = layers.Input(shape=(1,), dtype=\"string\")  # inputs are 1-dimensional strings\n",
    "    x = text_vec(inputs)  # turn the input text into vector\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=model_name)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    callbacks = [\n",
    "      keras.callbacks.ModelCheckpoint(\n",
    "        filepath=tfu.paths['dir_model_runs'] + model_name,\n",
    "        save_best_only=True)\n",
    "    ]\n",
    "    model.summary()\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=min(MAX_EPOCHS, 10),  # normally 10, 5 for debugging\n",
    "              callbacks=callbacks)\n",
    "    model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name)\n",
    "    print(f\"{model_name} Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")\n",
    "    return model"
   ],
   "metadata": {
    "id": "OY-RnZpw3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967877,
     "user_tz": 420,
     "elapsed": 36,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Multi-hot and tf_idf TextVectorization\n",
    "Seemed to get the best performance on the example sentences\n",
    "for count with ngrams of 2\n",
    "\n",
    "Multi-hot, ngrams=1 --> test accuracy = 0.889\n",
    "\n",
    "Multi-hot, ngrams=2 --> test accuracy = 0.896\n",
    "\n",
    "Multi-hot, ngrams=5 --> test accuracy = 0.893\n",
    "\n",
    "tf_idf, ngrams=1    --> test accuracy = 0.884\n",
    "\n",
    "tf_idf, ngrams=2    --> test accuracy = 0.857\n",
    "\n",
    "count, ngrams=1    --> test accuracy = 0.889\n",
    "\n",
    "count, ngrams=2    --> test accuracy = 0.885\n",
    "\n",
    "count, ngrams=5    --> test accuracy = 0.500"
   ],
   "metadata": {
    "collapsed": false,
    "id": "R7yf6yeJ3zMU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  # Select output mode and ngrams\n",
    "  output_mode = \"tf_idf\"  # \"multi_hot\", \"tf_idf\", or \"count\"\n",
    "  ngrams = 2  # 1,2, or 5 were tried\n",
    "  model_name_93 = f\"{output_mode}_ngrams_{ngrams}\""
   ],
   "metadata": {
    "id": "YagrCyzj3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967878,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  text_vec_2 = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=output_mode,\n",
    "    ngrams=ngrams,\n",
    "  )\n",
    "  text_vec_2.adapt(X_train)"
   ],
   "metadata": {
    "id": "t-5aF2NW3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967878,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp'] and True:\n",
    "  model_93 = run_model_2(model_name_93,\n",
    "                         X_train,\n",
    "                         y_train,\n",
    "                         X_val,\n",
    "                         y_val,\n",
    "                         X_test,\n",
    "                         y_test,\n",
    "                         text_vec_2)"
   ],
   "metadata": {
    "id": "0tZmOKpW3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967879,
     "user_tz": 420,
     "elapsed": 38,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Make predictions of sample reviews"
   ],
   "metadata": {
    "collapsed": false,
    "id": "dTKNxHsG3zMU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp'] and True:\n",
    "  review_01 = tf.convert_to_tensor([\n",
    "    \"That was an excellent movie, I loved it.\",\n",
    "  ])\n",
    "  review_01_pred = model_93.predict(review_01)\n",
    "  print(f\"Review: {review_01}\")\n",
    "  print(f'Estimated probability the review was positive: {review_01_pred[0][0] * 100:.2f}')"
   ],
   "metadata": {
    "id": "I7hjhbLv3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967879,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp'] and True:\n",
    "  review_02 = tf.convert_to_tensor([\n",
    "    \"That movie was terrible, I hated every scene.\",\n",
    "  ])\n",
    "  review_02_pred = model_93.predict(review_02)\n",
    "  print(f\"Review: {review_02}\")\n",
    "  print(f'Estimated probability the review was positive: {review_02_pred[0][0] * 100:.2f}')"
   ],
   "metadata": {
    "id": "upIClN7K3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967879,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sequence Modeling\n",
    "1. Use text_vectorization to change sentences to array of ints\n",
    "2. Can use various approaches to creating the embedding layer\n",
    "  * one-hot encode array of ints into 2-d matrix with shape (MAX_LENGTH, MAX_TOKENS)\n",
    "where MAX_LENGTH is the max words in feature input string and\n",
    "MAX_TOKENS is the max words in corpus vocab.  This is too slow to be practical"
   ],
   "metadata": {
    "collapsed": false,
    "id": "NQt9DHoc3zMU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### sequential models with embedding"
   ],
   "metadata": {
    "collapsed": false,
    "id": "b2yactR63zMU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  model_name_94 = \"lstm_untrained_embeddings_1\"\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")  # inputs are 1-dimensional strings\n",
    "  x = text_vectorization_2(inputs) # turn the input text into vector\n",
    "  embedding_layer = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=OUTPUT_DIMENSIONS,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=True,\n",
    "  )\n",
    "  x = embedding_layer(x)\n",
    "  x = layers.Bidirectional(layers.LSTM(32))(x)  # this does not converge on macOS with my interpreter\n",
    "  # x = layers.LSTM(32)(embedded)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_94 = keras.Model(inputs, outputs, name=model_name_94)\n",
    "  model_94.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    # optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "  model_94.summary()"
   ],
   "metadata": {
    "id": "HkfJix3S3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967879,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_94,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_94.fit(X_train,\n",
    "               y_train,\n",
    "               validation_data=(X_val, y_val),\n",
    "               epochs=min(MAX_EPOCHS, 10),\n",
    "               callbacks=callbacks)\n",
    "  model_94 = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_94)\n",
    "  print(f\"Test acc: {model_94.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "A65nTVhK3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967879,
     "user_tz": 420,
     "elapsed": 37,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  model_name_95 = \"lstm_trained_embeddings_2\"\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")  # inputs are 1-dimensional strings\n",
    "  x = text_vectorization_2(inputs) # turn the input text into vector\n",
    "  x = embedding_pre_trained_2(x)  # See embedding layer section for details.\n",
    "  x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "  # x = layers.LSTM(32)(embedded)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_95 = keras.Model(inputs, outputs, name=model_name_95)\n",
    "  model_95.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "  model_95.summary()"
   ],
   "metadata": {
    "id": "hIGJGmQ83zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967879,
     "user_tz": 420,
     "elapsed": 36,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp_sequence']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_95,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_95.fit(X_train,\n",
    "               y_train,\n",
    "               validation_data=(X_val, y_val),\n",
    "               epochs=min(MAX_EPOCHS, 10),\n",
    "               callbacks=callbacks)\n",
    "  model_95 = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_95)\n",
    "  print(f\"Test acc: {model_94.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "tYLWCDE_3zMU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967880,
     "user_tz": 420,
     "elapsed": 36,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Udemy Examples\n",
    "These use pandas to create X and y without using datasets\n",
    "\n",
    "Bag of words was one of the fastest and best with 80% accuracy, stacked bidirectional was similar, but took longer to train"
   ],
   "metadata": {
    "collapsed": false,
    "id": "izudLtwTXkCL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if run_mode == \"pycharm\":  # for some reason reload will cause colab to freeze\n",
    "  pass\n",
    "  # importlib.reload(tfu)"
   ],
   "metadata": {
    "id": "6Gfd4WDoxljo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967880,
     "user_tz": 420,
     "elapsed": 35,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set key hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "id": "D55kRfhmXkCL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  MAX_TOKENS = 15000  # Max words in corpus vocab (udemy picked 10,000)\n",
    "  OUTPUT_DIMENSIONS = 128  # Num of dims when embedding vector learns off data\n",
    "  PRE_TRAINED_DIM = 300  # Max dimension in embedding vector using glove pre-trained data (300 for this example, not 100)\n",
    "  MAX_LENGTH = 15  # Max words in feature input string\n",
    "  # Udemy selected 15 which is 50%, there are other reasonable choices\n",
    "  NGRAMS = 1"
   ],
   "metadata": {
    "id": "P38vkeskXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967880,
     "user_tz": 420,
     "elapsed": 35,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "KrlPihGMXkCL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  print(f\"Ensuring nlp_getting_started.zip is present\")\n",
    "  file_name = Path(\"nlp_getting_started.zip\")\n",
    "  url_nlp = \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "  nlp_test_file = 'test.csv'\n",
    "  nlp_train_file = 'train.csv'\n",
    "\n",
    "  if file_name.exists():\n",
    "    print(f\"{file_name} exists and will not be re-downloaded\")\n",
    "  else:\n",
    "    print(f\"{file_name}: downloading, expanding, and splitting\")\n",
    "    nlp_data = tfu.url_to_local_dir(url_nlp, decompress=True)"
   ],
   "metadata": {
    "id": "e9xIMuFqXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967880,
     "user_tz": 420,
     "elapsed": 35,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  nlp_all = pd.read_csv(nlp_train_file)\n",
    "  X = nlp_all['text'].to_numpy()\n",
    "  y = nlp_all['target'].to_numpy()"
   ],
   "metadata": {
    "id": "2cCjK3CgXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967880,
     "user_tz": 420,
     "elapsed": 35,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                      y,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=42)\n",
    "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "  tfu.random_guess_accuracy(y_test);"
   ],
   "metadata": {
    "id": "l7vJuBifXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967880,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find the percentiles of samples shorter than a given length"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YMhYPh5wXkCL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  word_count = [len(line.split()) for line in X]\n",
    "  # find percentile length of strings\n",
    "  cuts = range(10, 110, 10)\n",
    "  percentiles = [np.percentile(word_count, cut) for cut in cuts]\n",
    "  for cut, percentile in zip(cuts, percentiles):\n",
    "    print(f\"{cut} % -> length = {percentile}\")"
   ],
   "metadata": {
    "id": "3SVWwO3QXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967880,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Alternative methodology for datasets\n",
    "if False:\n",
    "  line_lens = []\n",
    "  for x in ds_train_input_only:\n",
    "    for line in x.numpy():\n",
    "      line_lens.append(len(line))\n",
    "  cuts = range(10, 110, 10)\n",
    "  percentiles = [np.percentile(line_lens, cut) for cut in cuts]\n",
    "  for cut, percentile in zip(cuts, percentiles):\n",
    "    print(f\"{cut} % -> length = {percentile}\")"
   ],
   "metadata": {
    "id": "MHRqQO7Gdxbo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967881,
     "user_tz": 420,
     "elapsed": 35,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determine TextVectorization"
   ],
   "metadata": {
    "collapsed": false,
    "id": "CpEEqDM7XkCL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  # text_vec_int - use for sequence models\n",
    "  text_vec_int = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    "    ngrams=NGRAMS,\n",
    "  )\n",
    "\n",
    "  # text_vec_tf_idf - use for bag of words models\n",
    "  text_vec_tf_idf = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    # output_mode=\"tf_idf\",\n",
    "    output_mode=\"binary\", # undocumented, binary seems to work well for bag of words\n",
    "    ngrams=NGRAMS,\n",
    "  )\n",
    "  text_vec_int.adapt(X_train)\n",
    "  text_vec_tf_idf.adapt(X_train)\n",
    "  vocab = text_vec_int.get_vocabulary()\n",
    "  print(f'Vocab length = {len(vocab)}')\n",
    "  print(f'First 10 words are: {vocab[:10]}')\n",
    "  print(f'Last  10 words are: {vocab[-10:]}')"
   ],
   "metadata": {
    "id": "WojQ-pWFXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967881,
     "user_tz": 420,
     "elapsed": 35,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determine embedding layers"
   ],
   "metadata": {
    "collapsed": false,
    "id": "n8UBux3xXkCL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  name = 'embedding_pre_trained'\n",
    "\n",
    "  # download/process as above to create glove_embedding\n",
    "  glove_embedding = \"glove.6B.300d.txt\"\n",
    "  embedding_dict = tfu.read_embeddings_file(glove_embedding)\n",
    "  # Create embedding matrix for the pre-trained embedding\n",
    "  embedding_matrix = tfu.get_embedding_matrix(MAX_TOKENS,\n",
    "                                              PRE_TRAINED_DIM,\n",
    "                                              text_vec_int,\n",
    "                                              embedding_dict)\n",
    "  # Create embedding layer\n",
    "  embedding_pre_trained = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=PRE_TRAINED_DIM,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=False,\n",
    "    mask_zero=True,  # True the input value 0 is a \"padding\" to be masked out.\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    name=name)"
   ],
   "metadata": {
    "id": "TGd8n7dGNB1v",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967881,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Baseline from TF-IDF Multinomial Naive Bayes\n",
    "Note, the X data should be strings (no vectorization or embedding)"
   ],
   "metadata": {
    "id": "_dfTlWxCS95p"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pwd"
   ],
   "metadata": {
    "id": "seXh3xtn3zMV",
    "outputId": "01d6fe62-20b3-40c6-fb7c-f58134f3b24c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967881,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  # Create a pipeline\n",
    "  model_0 = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "  ])\n",
    "\n",
    "  # Fit the pipeline to the training data\n",
    "  model_0.fit(X=X_train,\n",
    "              y=y_train);"
   ],
   "metadata": {
    "id": "5trzPBwITGcO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967881,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  # Evaluate baseline on validation dataset\n",
    "  baseline_score = model_0.score(X=X_test, y=y_test)\n",
    "  print(f\"Fast naive_bayes baseline accuracy score to beat: {baseline_score:.4f}\")"
   ],
   "metadata": {
    "id": "RO7i1x1KTjNy",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967881,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model as a bag of words"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ob0FIYDeXkCL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  model_name_100 = \"tweet_bag_of_words\"\n",
    "  hidden_dim = 16\n",
    "\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")  # inputs are 1-dimensional strings\n",
    "  x = text_vec_tf_idf(inputs)  # turn the input text into vector\n",
    "  x = layers.Dense(hidden_dim, activation=\"relu\")(x)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "  model_100 = keras.Model(inputs, outputs, name=model_name_100)\n",
    "  model_100.compile(optimizer=\"rmsprop\",\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                    metrics=[\"accuracy\"])\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_100,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_100.summary()"
   ],
   "metadata": {
    "id": "39OHoR9WXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967881,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  model_100.fit(X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=min(MAX_EPOCHS, 10),\n",
    "                callbacks=callbacks)"
   ],
   "metadata": {
    "id": "UT0vEUYDXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967882,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_100)\n",
    "  print(f\"{model_name_100} Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "MZibfQvsXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967883,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model as a sequences"
   ],
   "metadata": {
    "id": "YAKH-_DXvqBv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bidirectional with learned embedding"
   ],
   "metadata": {
    "id": "OY4-ejtoUlHa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if flags['nlp']:\n",
    "  model_name_101 = \"bidirectional_learned_embeddings_udemy\"\n",
    "  # double stacked bidirectional\n",
    "\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")  # inputs are 1-dimensional strings\n",
    "  x = text_vec_int(inputs)  # turn the input text into vector\n",
    "\n",
    "  embedding_layer = layers.Embedding(\n",
    "    input_dim=MAX_TOKENS,  # max tokens in vocab\n",
    "    output_dim=OUTPUT_DIMENSIONS,  # length of embedding vector\n",
    "    input_length=MAX_LENGTH,  # max tokens in a single input\n",
    "    trainable=True,\n",
    "    mask_zero=True,\n",
    "  )\n",
    "  x = embedding_layer(x)\n",
    "  x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(\n",
    "    x)  # stacking RNN layers requires return_sequences=True\n",
    "  x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_101 = keras.Model(inputs, outputs, name=model_name_101)\n",
    "  model_101.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "  model_101.summary()"
   ],
   "metadata": {
    "id": "t-aqAtgaXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967883,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  # if flags['nlp'] and run_mode == 'colab':\n",
    "  if flags['nlp']:\n",
    "    callbacks = [\n",
    "      keras.callbacks.ModelCheckpoint(\n",
    "        filepath=tfu.paths['dir_model_runs'] + model_name_101,\n",
    "        save_best_only=True)\n",
    "    ]\n",
    "    model_101.fit(X_train,\n",
    "                  y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  epochs=min(MAX_EPOCHS, 10),\n",
    "                  callbacks=callbacks)"
   ],
   "metadata": {
    "id": "rWVOv3CowZp8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967883,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_101)\n",
    "  print(f\"{model_name_101} Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "lvfPSHx3w52l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967883,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LSTM with learned embeddings"
   ],
   "metadata": {
    "id": "fJ4MZ5HNUujo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model_name_102 = \"lstm_learned_embeddings_udemy\"\n",
    "  # Use double stack of LSTM with learned embeddings\n",
    "\n",
    "  tf.random.set_seed(42)\n",
    "  model_102_embedding = layers.Embedding(input_dim=MAX_TOKENS,\n",
    "                                         output_dim=OUTPUT_DIMENSIONS,\n",
    "                                         embeddings_initializer=\"uniform\",\n",
    "                                         input_length=MAX_LENGTH,\n",
    "                                         name=\"embedding_102\")\n",
    "\n",
    "  # Create LSTM model\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "  x = text_vec_int(inputs)\n",
    "  x = model_102_embedding(x)\n",
    "  print(x.shape)\n",
    "  x = layers.LSTM(64, return_sequences=True)(\n",
    "    x)  # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "  x = layers.LSTM(64)(x)  # return vector for whole sequence\n",
    "  print(x.shape)\n",
    "  # x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_102 = tf.keras.Model(inputs, outputs, name=model_name_102)\n",
    "\n",
    "  model_102.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "  model_102.summary()"
   ],
   "metadata": {
    "id": "YSPR7SNPG6d-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967883,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_102,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_102.fit(X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=min(MAX_EPOCHS, 10),\n",
    "                callbacks=callbacks)"
   ],
   "metadata": {
    "id": "halDJteLHocE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967883,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_102)\n",
    "  print(f\"{model_name_102} Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "ufViiBuVIMJ6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967884,
     "user_tz": 420,
     "elapsed": 34,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LSTM with pre-trained embeddings"
   ],
   "metadata": {
    "id": "SAhF-LtnU1gp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model_name_103 = \"lstm_pretrained_embeddings_udemy\"\n",
    "\n",
    "  tf.random.set_seed(42)\n",
    "\n",
    "  # Create LSTM model\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "  x = text_vec_int(inputs)\n",
    "  x = embedding_pre_trained(x)\n",
    "  print(x.shape)\n",
    "  # x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "  x = layers.LSTM(64)(x)  # return vector for whole sequence\n",
    "  print(x.shape)\n",
    "  # x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_103 = tf.keras.Model(inputs, outputs, name=model_name_103)\n",
    "\n",
    "  model_103.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "  model_103.summary()"
   ],
   "metadata": {
    "id": "gdlG2eTgVRPH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967884,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_103,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_103.fit(X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=min(MAX_EPOCHS, 10),\n",
    "                callbacks=callbacks)"
   ],
   "metadata": {
    "id": "9Hu4CXnXWslM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967884,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_103)\n",
    "  print(f\"{model_name_103} Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "5eFjHUiBWyBp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967884,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GRU with Dense Layer"
   ],
   "metadata": {
    "id": "kUFVglTaWsiC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model_name_104 = \"gru_learned_embeddings_udemy\"\n",
    "\n",
    "  tf.random.set_seed(42)\n",
    "  model_104_embedding = layers.Embedding(input_dim=MAX_TOKENS,\n",
    "                                         output_dim=OUTPUT_DIMENSIONS,\n",
    "                                         embeddings_initializer=\"uniform\",\n",
    "                                         input_length=MAX_LENGTH,\n",
    "                                         name=\"embedding_104\")\n",
    "\n",
    "  # Create LSTM model\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "  x = text_vec_int(inputs)\n",
    "  x = model_104_embedding(x)\n",
    "  x = layers.GRU(64)(x)  # return vector for whole sequence\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)  # optional dense layer\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_104 = tf.keras.Model(inputs, outputs, name=model_name_104)\n",
    "\n",
    "  model_104.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "  model_104.summary()"
   ],
   "metadata": {
    "id": "ZKbOgKJgeSyu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967884,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_104,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_104.fit(X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=min(MAX_EPOCHS, 10),\n",
    "                callbacks=callbacks)"
   ],
   "metadata": {
    "id": "VW-QrxgeekW8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967884,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_104)\n",
    "  print(f\"{model_name_104} Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "1Z0orzPsepXV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967885,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1-d Conv"
   ],
   "metadata": {
    "id": "xSIe841q8enC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model_name_105 = \"conv1d_learned_embeddings_udemy\"\n",
    "\n",
    "  tf.random.set_seed(42)\n",
    "  model_105_embedding = layers.Embedding(input_dim=MAX_TOKENS,\n",
    "                                         output_dim=OUTPUT_DIMENSIONS,\n",
    "                                         embeddings_initializer=\"uniform\",\n",
    "                                         input_length=MAX_LENGTH,\n",
    "                                         name=\"embedding_104\")\n",
    "\n",
    "  # Create model\n",
    "  inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "  x = text_vec_int(inputs)\n",
    "  x = model_105_embedding(x)\n",
    "  x = layers.Conv1D(128, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "  x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
    "  x = layers.GlobalAveragePooling1D()(x)  # condense the output of our feature vector\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  x = layers.Dense(64, activation=\"relu\")(x)  # optional dense layer\n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  x = layers.Dense(32, activation=\"relu\")(x)  # optional dense layer\n",
    "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "  model_105 = tf.keras.Model(inputs, outputs, name=model_name_105)\n",
    "\n",
    "  model_105.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "  model_105.summary()"
   ],
   "metadata": {
    "id": "2rbs6zvV8enD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967885,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=tfu.paths['dir_model_runs'] + model_name_105,\n",
    "      save_best_only=True)\n",
    "  ]\n",
    "  model_105.fit(X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=min(MAX_EPOCHS, 25),\n",
    "                callbacks=callbacks)"
   ],
   "metadata": {
    "id": "KRZCOYyF8enD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967885,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if flags['nlp']:\n",
    "  model = keras.models.load_model(tfu.paths['dir_model_runs'] + model_name_105)\n",
    "  print(f\"{model_name_105} Test acc: {model.evaluate(X_test, y_test)[1]:.3f}\")"
   ],
   "metadata": {
    "id": "-N_5GbGL8enD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967885,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook Timer Diagnostics"
   ],
   "metadata": {
    "collapsed": false,
    "id": "86GWjwuxXkCL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfu.Timer.lap('notebook complete')  # Input Data Preprocessing"
   ],
   "metadata": {
    "id": "Sdne9aeVXkCL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1695276967885,
     "user_tz": 420,
     "elapsed": 33,
     "user": {
      "displayName": "Tony Held",
      "userId": "06589665641780981463"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c648c368-4717-4bdc-ac2d-adfa505539d6"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1fM3EOv5KhQUjaTIGYY8BvdVZFRUXzJtn",
     "timestamp": 1682997806181
    },
    {
     "file_id": "1dJAcpkZr7_IOfLLTIXyta7J4ApjCzhGA",
     "timestamp": 1682556518090
    },
    {
     "file_id": "1RCF0-bPeV9HxBGNd0es2nFToFx-29gP7",
     "timestamp": 1682386590110
    }
   ],
   "toc_visible": true,
   "gpuType": "V100",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "512px",
    "width": "393px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "198px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
